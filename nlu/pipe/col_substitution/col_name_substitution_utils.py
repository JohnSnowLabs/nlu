"""Utils for making final output columns generated by pythonify nicer
Col naming schema follows
<type>.<nlu_ref_identifier>.<field>
IF there is only 1 component of <type> in the pipe, the <type> will/can be ommitted.

- we remove all _<field> suffixex
- replace all '@' with '_'
"""

from sparknlp.annotator import *
from nlu.pipe.viz.vis_utils_OS import VizUtilsOS
from nlu.pipe.col_substitution import substitution_map_OS
from nlu.pipe.col_substitution import col_substitution_OS
import logging
logger = logging.getLogger('nlu')

""" NAMING SCHEMAS after pythonify procedure : 
### NAMING RESULT SCHEMA: 


results         = { configs.output_col_prefix+'_results'  : list(map(unpack_result,row))} if configs.get_result else {}
beginnings      = { configs.output_col_prefix+'_beginnings' : list(map(unpack_begin,row))} if configs.get_begin or configs.get_positions else {}
endings         = { configs.output_col_prefix+'_endings'    : next(map(unpack_end,row))} if configs.get_end or configs.get_positions else {}
embeddings      = { configs.output_col_prefix+'_embeddings' : next(map(unpack_embeddings,row))} if configs.get_embeds else {}


### METADATA NAMING SCHEMA

result = dict(zip(list(map(lambda x : 'meta_'+ configs.output_col_prefix + '_' + x, keys_in_metadata)),meta_values_list))




"""
class ColSubstitutionUtils():
    """Utils for substituting col names in Pythonify to short and meaningful names.
    Uses custom rename methods for either PySpark or Pandas
    """
    @staticmethod
    def substitute_col_names(df,anno_2_ex,pipe,drop_debug_cols=True):
        """
        Some truly irrelevant cols might be dropped, regardless of anno Extractor config
        Some truly irrelevant cols might be dropped, regardless of anno Extractor config
        0. Get list of annotator classes that are duplicates. Check inside the NLU Component Embelishment
        1. Get list of cols derived by component
        2. Substitute list of cols in DF with custom logic
        """
        substitution_fn = 'TODO'
        new_cols = {}
        if pipe.has_licensed_components :
            from nlu.pipe.col_substitution import col_substitution_HC
            from nlu.pipe.col_substitution import substitution_map_HC

        for c in pipe.components :
            is_unique = True # TODO infer this properly
            cols_to_substitute = ColSubstitutionUtils.get_final_output_cols_of_component(c,df,anno_2_ex)

            if type(c.model) in substitution_map_OS.OS_anno2substitution_fn.keys():
                substitution_fn = substitution_map_OS.OS_anno2substitution_fn[type(c.model)]['default']
            if pipe.has_licensed_components and substitution_fn != 'TODO':
                if type(c.model) in substitution_map_HC.HC_anno2substitution_fn.keys():
                    substitution_fn = substitution_map_HC.HC_anno2substitution_fn[type(c.model)]['default']
            if substitution_fn =='TODO':
                logger.info(f"Could not find substitution function for c={c}, leaving col names untouched")
                new_cols.update(dict(zip(cols_to_substitute,cols_to_substitute)))
                continue
            # dic, key=old_col, value=new_col. Some cols may be omitted and missing from the dic which are deemed irrelevant. Behaivour can be disabled by setting drop_debug_cols=False
            new_cols = {**new_cols, **(substitution_fn(c,cols_to_substitute,is_unique))}

        return df.rename(columns = new_cols)[new_cols.values] if drop_debug_cols else df.rename(columns = new_cols)


    @staticmethod
    def get_final_output_cols_of_component(c,df,anno_2_ex):
        # get_final_output_cols_of_component(self.components[1], pretty_df, anno_2_ex_config)
        """Get's a list of all columns that have been derived in the pythonify procedure from the component c in dataframe df for anno_2_ex configs """
        og_output_col = c.info.spark_output_column_names[0]
        configs       = anno_2_ex[og_output_col]
        result_cols   = []
        if configs.get_annotator_type                 : result_cols.append(configs.output_col_prefix+'_types')
        if configs.get_result                         : result_cols.append(configs.output_col_prefix+'_results')
        if configs.get_begin or configs.get_positions : result_cols.append(configs.output_col_prefix+'_beginnings')
        if configs.get_end   or configs.get_positions : result_cols.append(configs.output_col_prefix+'_endings')
        if configs.get_embeds                         : result_cols.append(configs.output_col_prefix+'_embeddings')
        # find all metadata fields generated by compoent
        for col in df.columns :
            if 'meta_'+ configs.output_col_prefix in col:
                result_cols.append('meta_'+ configs.output_col_prefix + '_' + col.split('_')[-1])

        return result_cols


