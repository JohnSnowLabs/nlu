{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU all components Demo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij3ai9X4CmZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "78b40f86-5725-4fa1-f3be-f7627897453d"
      },
      "source": [
        "# Setup Java and NLU\n",
        "import os\n",
        "! apt-get update -qq > /dev/null   \n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "! pip install nlu==2.5rc1 -qq > /dev/null   \n",
        "\n",
        "import nlu "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqUqu9DUTVqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2de80e2-63f1-4f61-cbcd-b49e467eee0e"
      },
      "source": [
        "# any string inside of <> can be apssed to nlu.load()\n",
        " nlu.print_all_nlu_components()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default NLU reference : < lang > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < lang.7 > of type : pipe  points to :  detect_language_7 Points to Spark NLP reference :  ('detect_language_7', 'pipe')\n",
            "Default NLU reference : < lang.20 > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang.20 > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang.7 > of type : pipe  points to :  detect_language_7 Points to Spark NLP reference :  ('detect_language_7', 'pipe')\n",
            "Default NLU reference : < classify > of type : pipe  points to :  analyze_sentiment Points to Spark NLP reference :  ('analyze_sentiment', 'pipe')\n",
            "Default NLU reference : < explain > of type : pipe  points to :  explain_document_ml Points to Spark NLP reference :  ('explain_document_ml', 'pipe')\n",
            "Default NLU reference : < explain.ml > of type : pipe  points to :  explain_document_ml Points to Spark NLP reference :  ('explain_document_ml', 'pipe')\n",
            "Default NLU reference : < explain.dl > of type : pipe  points to :  explain_document_dl Points to Spark NLP reference :  ('explain_document_dl', 'pipe')\n",
            "Default NLU reference : < ner > of type : pipe  points to :  recognize_entities_dl Points to Spark NLP reference :  ('recognize_entities_dl', 'pipe')\n",
            "Default NLU reference : < ner.dl > of type : pipe  points to :  recognize_entities_dl Points to Spark NLP reference :  ('recognize_entities_dl', 'pipe')\n",
            "Default NLU reference : < ner.bert > of type : pipe  points to :  recognize_entities_bert Points to Spark NLP reference :  ('recognize_entities_bert', 'pipe')\n",
            "Default NLU reference : < ner.onto > of type : pipe  points to :  onto_recognize_entities_sm Points to Spark NLP reference :  ('onto_recognize_entities_sm', 'pipe')\n",
            "Default NLU reference : < ner.onto.sm > of type : pipe  points to :  onto_recognize_entities_sm Points to Spark NLP reference :  ('onto_recognize_entities_sm', 'pipe')\n",
            "Default NLU reference : < ner.onto.lg > of type : pipe  points to :  onto_recognize_entities_lg Points to Spark NLP reference :  ('onto_recognize_entities_lg', 'pipe')\n",
            "Default NLU reference : < match.datetime > of type : pipe  points to :  match_datetime Points to Spark NLP reference :  ('match_datetime', 'pipe')\n",
            "Default NLU reference : < match.text > of type : model  points to :  text_matcher Points to Spark NLP reference :  ('text_matcher', 'model')\n",
            "Default NLU reference : < match.regex > of type : model  points to :  regex_matcher Points to Spark NLP reference :  ('regex_matcher', 'model')\n",
            "Default NLU reference : < match.pattern > of type : pipe  points to :  match_pattern Points to Spark NLP reference :  ('match_pattern', 'pipe')\n",
            "Default NLU reference : < match.chunks > of type : pipe  points to :  match_chunks Points to Spark NLP reference :  ('match_chunks', 'pipe')\n",
            "Default NLU reference : < match.phrases > of type : pipe  points to :  match_phrases Points to Spark NLP reference :  ('match_phrases', 'pipe')\n",
            "Default NLU reference : < clean.stop > of type : pipe  points to :  clean_stop Points to Spark NLP reference :  ('clean_stop', 'pipe')\n",
            "Default NLU reference : < clean.pattern > of type : pipe  points to :  clean_pattern Points to Spark NLP reference :  ('clean_pattern', 'pipe')\n",
            "Default NLU reference : < clean.slang > of type : pipe  points to :  clean_slang Points to Spark NLP reference :  ('clean_slang', 'pipe')\n",
            "Default NLU reference : < spell > of type : pipe  points to :  check_spelling Points to Spark NLP reference :  ('check_spelling', 'pipe')\n",
            "Default NLU reference : < spell.dl > of type : pipe  points to :  check_spelling_dl Points to Spark NLP reference :  ('check_spelling_dl', 'pipe')\n",
            "Default NLU reference : < sentiment > of type : pipe  points to :  analyze_sentiment Points to Spark NLP reference :  ('analyze_sentiment', 'pipe')\n",
            "Default NLU reference : < emotion > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < sentiment.imdb > of type : pipe  points to :  analyze_sentimentdl_use_imdb Points to Spark NLP reference :  ('analyze_sentimentdl_use_imdb', 'pipe')\n",
            "Default NLU reference : < sentiment.imdb.use > of type : pipe  points to :  analyze_sentimentdl_use_imdb Points to Spark NLP reference :  ('analyze_sentimentdl_use_imdb', 'pipe')\n",
            "Default NLU reference : < sentiment.twitter.use > of type : pipe  points to :  analyze_sentimentdl_use_twitter Points to Spark NLP reference :  ('analyze_sentimentdl_use_twitter', 'pipe')\n",
            "Default NLU reference : < sentiment.twitter > of type : pipe  points to :  analyze_sentimentdl_use_twitter Points to Spark NLP reference :  ('analyze_sentimentdl_use_twitter', 'pipe')\n",
            "Default NLU reference : < tokenize > of type : model  points to :  spark_nlp_tokenizer Points to Spark NLP reference :  ('spark_nlp_tokenizer', 'model')\n",
            "Default NLU reference : < stem > of type : model  points to :  stemmer Points to Spark NLP reference :  ('stemmer', 'model')\n",
            "Default NLU reference : < norm > of type : model  points to :  normalizer Points to Spark NLP reference :  ('normalizer', 'model')\n",
            "Default NLU reference : < chunk > of type : model  points to :  default_chunker Points to Spark NLP reference :  ('default_chunker', 'model')\n",
            "Default NLU reference : < embed_chunk > of type : model  points to :  chunk_embeddings Points to Spark NLP reference :  ('chunk_embeddings', 'model')\n",
            "Default NLU reference : < ngram > of type : model  points to :  ngram Points to Spark NLP reference :  ('ngram', 'model')\n",
            "Default NLU reference : < lemma > of type : model  points to :  lemma_antbnc Points to Spark NLP reference :  ('lemma_antbnc', 'model')\n",
            "Default NLU reference : < lemma.antbnc > of type : model  points to :  lemma_antbnc Points to Spark NLP reference :  ('lemma_antbnc', 'model')\n",
            "Default NLU reference : < pos > of type : model  points to :  pos_anc Points to Spark NLP reference :  ('pos_anc', 'model')\n",
            "Default NLU reference : < pos.anc > of type : model  points to :  pos_anc Points to Spark NLP reference :  ('pos_anc', 'model')\n",
            "Default NLU reference : < pos.ud_ewt > of type : model  points to :  pos_ud_ewt Points to Spark NLP reference :  ('pos_ud_ewt', 'model')\n",
            "Default NLU reference : < ner.dl.glove_6B_100d > of type : model  points to :  ner_dl Points to Spark NLP reference :  ('ner_dl', 'model')\n",
            "Default NLU reference : < ner.dl.bert > of type : model  points to :  ner_dl_bert Points to Spark NLP reference :  ('ner_dl_bert', 'model')\n",
            "Default NLU reference : < ner.onto.glove_6B_100d > of type : model  points to :  onto_100 Points to Spark NLP reference :  ('onto_100', 'model')\n",
            "Default NLU reference : < ner.onto.glove_6B_300d > of type : model  points to :  onto_300 Points to Spark NLP reference :  ('onto_300', 'model')\n",
            "Default NLU reference : < sentence_detector > of type : model  points to :  ner_dl_sentence Points to Spark NLP reference :  ('ner_dl_sentence', 'model')\n",
            "Default NLU reference : < sentence_detector.deep > of type : model  points to :  ner_dl_sentence Points to Spark NLP reference :  ('ner_dl_sentence', 'model')\n",
            "Default NLU reference : < spell.norivg > of type : model  points to :  spellcheck_norvig Points to Spark NLP reference :  ('spellcheck_norvig', 'model')\n",
            "Default NLU reference : < sentiment.vivekn > of type : model  points to :  sentiment_vivekn Points to Spark NLP reference :  ('sentiment_vivekn', 'model')\n",
            "Default NLU reference : < dep.untyped.conllu > of type : model  points to :  dependency_conllu Points to Spark NLP reference :  ('dependency_conllu', 'model')\n",
            "Default NLU reference : < dep.untyped > of type : model  points to :  dependency_conllu.untyped Points to Spark NLP reference :  ('dependency_conllu.untyped', 'model')\n",
            "Default NLU reference : < dep > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < dep.typed > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < dep.typed.conllu > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < stopwords > of type : model  points to :  stopwords_en Points to Spark NLP reference :  ('stopwords_en', 'model')\n",
            "Default NLU reference : < embed > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < glove > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < embed.glove > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < embed.glove_100d > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < bert > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert_base_uncased > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert_base_cased > of type : model  points to :  bert_base_cased Points to Spark NLP reference :  ('bert_base_cased', 'model')\n",
            "Default NLU reference : < biobert > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_pubmed_base_cased > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_pubmed_pmc_base_cased > of type : model  points to :  biobert_pubmed_pmc_base_cased Points to Spark NLP reference :  ('biobert_pubmed_pmc_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_clinical_base_cased > of type : model  points to :  biobert_clinical_base_cased Points to Spark NLP reference :  ('biobert_clinical_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_discharge_base_cased > of type : model  points to :  biobert_discharge_base_cased Points to Spark NLP reference :  ('biobert_discharge_base_cased', 'model')\n",
            "Default NLU reference : < elmo > of type : model  points to :  elmo Points to Spark NLP reference :  ('elmo', 'model')\n",
            "Default NLU reference : < embed.elmo > of type : model  points to :  elmo Points to Spark NLP reference :  ('elmo', 'model')\n",
            "Default NLU reference : < embed_sentence > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.tfhub_use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.use_lg > of type : model  points to :  tfhub_use_lg Points to Spark NLP reference :  ('tfhub_use_lg', 'model')\n",
            "Default NLU reference : < embed_sentence.tfhub_use_lg > of type : model  points to :  tfhub_use_lg Points to Spark NLP reference :  ('tfhub_use_lg', 'model')\n",
            "Default NLU reference : < albert > of type : model  points to :  albert_base_uncased Points to Spark NLP reference :  ('albert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.albert_base_uncased > of type : model  points to :  albert_base_uncased Points to Spark NLP reference :  ('albert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.xlnet > of type : model  points to :  xlnet_base_cased Points to Spark NLP reference :  ('xlnet_base_cased', 'model')\n",
            "Default NLU reference : < xlnet > of type : model  points to :  xlnet_base_cased Points to Spark NLP reference :  ('xlnet_base_cased', 'model')\n",
            "Default NLU reference : < classify.trec6.use > of type : model  points to :  classifierdl_use_trec6 Points to Spark NLP reference :  ('classifierdl_use_trec6', 'model')\n",
            "Default NLU reference : < classify.trec50.use > of type : model  points to :  classifierdl_use_trec50 Points to Spark NLP reference :  ('classifierdl_use_trec50', 'model')\n",
            "Default NLU reference : < classify.spam.use > of type : model  points to :  classifierdl_use_spam Points to Spark NLP reference :  ('classifierdl_use_spam', 'model')\n",
            "Default NLU reference : < classify.fakenews.use > of type : model  points to :  classifierdl_use_fakenews Points to Spark NLP reference :  ('classifierdl_use_fakenews', 'model')\n",
            "Default NLU reference : < classify.emotion.use > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < classify.cyberbullying.use > of type : model  points to :  classifierdl_use_cyberbullying Points to Spark NLP reference :  ('classifierdl_use_cyberbullying', 'model')\n",
            "Default NLU reference : < classify.sarcasm.use > of type : model  points to :  classifierdl_use_sarcasm Points to Spark NLP reference :  ('classifierdl_use_sarcasm', 'model')\n",
            "Default NLU reference : < sentiment.imdb.glove > of type : model  points to :  sentimentdl_glove_imdb Points to Spark NLP reference :  ('sentimentdl_glove_imdb', 'model')\n",
            "Default NLU reference : < classify.trec6 > of type : model  points to :  classifierdl_use_trec6 Points to Spark NLP reference :  ('classifierdl_use_trec6', 'model')\n",
            "Default NLU reference : < classify.trec50 > of type : model  points to :  classifierdl_use_trec50 Points to Spark NLP reference :  ('classifierdl_use_trec50', 'model')\n",
            "Default NLU reference : < classify.spam > of type : model  points to :  classifierdl_use_spam Points to Spark NLP reference :  ('classifierdl_use_spam', 'model')\n",
            "Default NLU reference : < classify.fakenews > of type : model  points to :  classifierdl_use_fakenews Points to Spark NLP reference :  ('classifierdl_use_fakenews', 'model')\n",
            "Default NLU reference : < classify.emotion > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < classify.cyberbullying > of type : model  points to :  classifierdl_use_cyberbullying Points to Spark NLP reference :  ('classifierdl_use_cyberbullying', 'model')\n",
            "Default NLU reference : < classify.sarcasm > of type : model  points to :  classifierdl_use_sarcasm Points to Spark NLP reference :  ('classifierdl_use_sarcasm', 'model')\n",
            "Default NLU reference : < embed.glove_840B_300 > of type : model  points to :  glove_840B_300 Points to Spark NLP reference :  ('glove_840B_300', 'model')\n",
            "Default NLU reference : < embed.glove_6B_300 > of type : model  points to :  glove_6B_300 Points to Spark NLP reference :  ('glove_6B_300', 'model')\n",
            "Default NLU reference : < embed.bert_multi_cased > of type : model  points to :  bert_multi_cased Points to Spark NLP reference :  ('bert_multi_cased', 'model')\n",
            "Default NLU reference : < classify.wiki_7 > of type : model  points to :  ld_wiki_7 Points to Spark NLP reference :  ('ld_wiki_7', 'model')\n",
            "Default NLU reference : < classify.wiki_20 > of type : model  points to :  ld_wiki_20 Points to Spark NLP reference :  ('ld_wiki_20', 'model')\n",
            "NLU reference < nl.explain > for lang nl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < nl.explain.sm > for lang nl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < nl.explain.md > for lang nl  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < nl.explain.lg > for lang nl  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < nl.ner > for lang nl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < nl.ner.sm > for lang nl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < nl.ner.md > for lang nl  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < nl.ner.lg > for lang nl  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < en.classify > for lang en  points to SPARK NLP model : analyze_sentiment\n",
            "NLU reference < en.explain > for lang en  points to SPARK NLP model : explain_document_ml\n",
            "NLU reference < en.explain.ml > for lang en  points to SPARK NLP model : explain_document_ml\n",
            "NLU reference < en.explain.dl > for lang en  points to SPARK NLP model : explain_document_dl\n",
            "NLU reference < en.ner > for lang en  points to SPARK NLP model : recognize_entities_dl\n",
            "NLU reference < en.ner.dl > for lang en  points to SPARK NLP model : recognize_entities_dl\n",
            "NLU reference < en.ner.bert > for lang en  points to SPARK NLP model : recognize_entities_bert\n",
            "NLU reference < en.ner.onto > for lang en  points to SPARK NLP model : onto_recognize_entities_sm\n",
            "NLU reference < en.ner.onto.sm > for lang en  points to SPARK NLP model : onto_recognize_entities_sm\n",
            "NLU reference < en.ner.onto.lg > for lang en  points to SPARK NLP model : onto_recognize_entities_lg\n",
            "NLU reference < en.match.datetime > for lang en  points to SPARK NLP model : match_datetime\n",
            "NLU reference < en.match.pattern > for lang en  points to SPARK NLP model : match_pattern\n",
            "NLU reference < en.match.chunks > for lang en  points to SPARK NLP model : match_chunks\n",
            "NLU reference < en.match.phrases > for lang en  points to SPARK NLP model : match_phrases\n",
            "NLU reference < en.clean.stop > for lang en  points to SPARK NLP model : clean_stop\n",
            "NLU reference < en.clean.pattern > for lang en  points to SPARK NLP model : clean_pattern\n",
            "NLU reference < en.clean.slang > for lang en  points to SPARK NLP model : clean_slang\n",
            "NLU reference < en.spell > for lang en  points to SPARK NLP model : check_spelling\n",
            "NLU reference < en.spell.dl > for lang en  points to SPARK NLP model : check_spelling_dl\n",
            "NLU reference < en.spell.context > for lang en  points to SPARK NLP model : check_spelling_dl\n",
            "NLU reference < en.sentiment > for lang en  points to SPARK NLP model : analyze_sentiment\n",
            "NLU reference < en.sentiment.imdb > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.imdb.use > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.twitter.use > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.twitter > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_twitter\n",
            "NLU reference < fr.explain > for lang fr  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < fr.explain.lg > for lang fr  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < fr.explain.md > for lang fr  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < fr.ner > for lang fr  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < fr.ner.lg > for lang fr  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < fr.ner.md > for lang fr  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.explain.document > for lang de  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < de.explain.document.md > for lang de  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < de.explain.document.lg > for lang de  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < de.ner.recognizer > for lang de  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.ner.recognizer.md > for lang de  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.ner.recognizer.lg > for lang de  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < it.explain.document > for lang it  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < it.explain.document.md > for lang it  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < it.explain.document.lg > for lang it  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < it.ner > for lang it  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < it.ner.md > for lang it  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < it.ner.lg > for lang it  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < no.explain > for lang no  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < no.explain.sm > for lang no  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < no.explain.md > for lang no  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < no.explain.lg > for lang no  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < no.ner > for lang no  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < no.ner.sm > for lang no  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < no.ner.md > for lang no  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < no.ner.lg > for lang no  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < pl.explain > for lang pl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pl.explain.sm > for lang pl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pl.explain.md > for lang pl  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < pl.explain.lg > for lang pl  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < pl.ner > for lang pl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pl.ner.sm > for lang pl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pl.ner.md > for lang pl  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < pl.ner.lg > for lang pl  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < pt.explain > for lang pt  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pt.explain.sm > for lang pt  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pt.explain.md > for lang pt  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < pt.explain.lg > for lang pt  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < pt.ner > for lang pt  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pt.ner.sm > for lang pt  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pt.ner.md > for lang pt  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < pt.ner.lg > for lang pt  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < ru.explain > for lang ru  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < ru.explain.sm > for lang ru  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < ru.explain.md > for lang ru  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < ru.explain.lg > for lang ru  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < ru.ner > for lang ru  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < ru.ner.sm > for lang ru  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < ru.ner.md > for lang ru  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < ru.ner.lg > for lang ru  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < es.explain > for lang es  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < es.explain.sm > for lang es  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < es.explain.md > for lang es  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < es.explain.lg > for lang es  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < es.ner > for lang es  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < es.ner.sm > for lang es  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < es.ner.md > for lang es  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < es.ner.lg > for lang es  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < lang > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < lang.7 > for lang xx  points to SPARK NLP model : detect_language_7\n",
            "NLU reference < lang.20 > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang.20 > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang.7 > for lang xx  points to SPARK NLP model : detect_language_7\n",
            "NLU reference < nl.lemma > for lang nl  points to SPARK NLP model : lemma\n",
            "NLU reference < nl.pos > for lang nl  points to SPARK NLP model : pos_ud_alpino\n",
            "NLU reference < nl.pos.ud_alpino > for lang nl  points to SPARK NLP model : pos_ud_alpino\n",
            "NLU reference < nl.ner > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner.glove_6B_100 > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner.glove_6B_300 > for lang nl  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < nl.ner.wikiner.glove_840B_300 > for lang nl  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < en.stem > for lang en  points to SPARK NLP model : stemmer\n",
            "NLU reference < en.tokenize > for lang en  points to SPARK NLP model : spark_nlp_tokenizer\n",
            "NLU reference < en.norm > for lang en  points to SPARK NLP model : norm\n",
            "NLU reference < en.chunk > for lang en  points to SPARK NLP model : default_chunker\n",
            "NLU reference < en.ngram > for lang en  points to SPARK NLP model : ngram\n",
            "NLU reference < en.embed_chunk > for lang en  points to SPARK NLP model : chunk_embeddings\n",
            "NLU reference < en.lemma > for lang en  points to SPARK NLP model : lemma_antbnc\n",
            "NLU reference < en.lemma.antbnc > for lang en  points to SPARK NLP model : lemma_antbnc\n",
            "NLU reference < en.pos > for lang en  points to SPARK NLP model : pos_anc\n",
            "NLU reference < en.pos.anc > for lang en  points to SPARK NLP model : pos_anc\n",
            "NLU reference < en.pos.ud_ewt > for lang en  points to SPARK NLP model : pos_ud_ewt\n",
            "NLU reference < en.ner > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl.glove_6B_100d > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl.bert > for lang en  points to SPARK NLP model : ner_dl_bert\n",
            "NLU reference < en.ner.onto > for lang en  points to SPARK NLP model : onto_100\n",
            "NLU reference < en.ner.onto.glove_6B_100d > for lang en  points to SPARK NLP model : onto_100\n",
            "NLU reference < en.ner.onto.glove_6B_300d > for lang en  points to SPARK NLP model : onto_300\n",
            "NLU reference < en.ner.glove_100d > for lang en  points to SPARK NLP model : ner_dl_sentence\n",
            "NLU reference < en.spell.norvig > for lang en  points to SPARK NLP model : spellcheck_norvig\n",
            "NLU reference < en.sentiment.vivekn > for lang en  points to SPARK NLP model : sentiment_vivekn\n",
            "NLU reference < en.dep.untyped.conllu > for lang en  points to SPARK NLP model : dependency_conllu\n",
            "NLU reference < en.dep.untyped > for lang en  points to SPARK NLP model : dependency_conllu\n",
            "NLU reference < en.stopwords > for lang en  points to SPARK NLP model : stopwords_en\n",
            "NLU reference < en.glove > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed.glove > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed.glove_100d > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.bert > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert_base_uncased > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert_base_cased > for lang en  points to SPARK NLP model : bert_base_cased\n",
            "NLU reference < biobert > for lang en  points to SPARK NLP model : biobert_pubmed_base_cased\n",
            "NLU reference < en.embed.biobert > for lang en  points to SPARK NLP model : biobert_pubmed_base_cased\n",
            "NLU reference < en.embed.biobert_pubmed_pmc_base_cased > for lang en  points to SPARK NLP model : biobert_pubmed_pmc_base_cased\n",
            "NLU reference < en.embed.biobert_clinical_base_cased > for lang en  points to SPARK NLP model : biobert_clinical_base_cased\n",
            "NLU reference < en.embed.biobert_discharge_base_cased > for lang en  points to SPARK NLP model : biobert_discharge_base_cased\n",
            "NLU reference < en.embed.elmo > for lang en  points to SPARK NLP model : elmo\n",
            "NLU reference < en.embed_sentence > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.tfhub_use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.use_lg > for lang en  points to SPARK NLP model : tfhub_use_lg\n",
            "NLU reference < en.embed_sentence.tfhub_use_lg > for lang en  points to SPARK NLP model : tfhub_use_lg\n",
            "NLU reference < en.embed_sentence.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.albert_base_uncased > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.xlnet > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.xlnet > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.embed.xlnet_base_cased > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.classify.trec6.use > for lang en  points to SPARK NLP model : classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50.use > for lang en  points to SPARK NLP model : classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam.use > for lang en  points to SPARK NLP model : classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews.use > for lang en  points to SPARK NLP model : classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion.use > for lang en  points to SPARK NLP model : classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying.use > for lang en  points to SPARK NLP model : classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm.use > for lang en  points to SPARK NLP model : classifierdl_use_sarcasm\n",
            "NLU reference < en.sentiment.imdb.use > for lang en  points to SPARK NLP model : sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.twitter.use > for lang en  points to SPARK NLP model : sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.imdb.glove > for lang en  points to SPARK NLP model : sentimentdl_glove_imdb\n",
            "NLU reference < en.classify.trec6 > for lang en  points to SPARK NLP model : classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50 > for lang en  points to SPARK NLP model : classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam > for lang en  points to SPARK NLP model : classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews > for lang en  points to SPARK NLP model : classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion > for lang en  points to SPARK NLP model : classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying > for lang en  points to SPARK NLP model : classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm > for lang en  points to SPARK NLP model : classifierdl_use_sarcasm\n",
            "NLU reference < en.sentiment.twitter > for lang en  points to SPARK NLP model : sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.imdb > for lang en  points to SPARK NLP model : sentimentdl_glove_imdb\n",
            "NLU reference < fr.lemma > for lang fr  points to SPARK NLP model : lemma\n",
            "NLU reference < fr.pos > for lang fr  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < fr.pos.ud_gsd > for lang fr  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < fr.ner > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.ner.wikiner > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.ner.wikiner.glove_840B_300 > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.stopwords > for lang fr  points to SPARK NLP model : stopwords_fr\n",
            "NLU reference < de.lemma > for lang de  points to SPARK NLP model : lemma\n",
            "NLU reference < de.pos.ud_hdt > for lang de  points to SPARK NLP model : pos_ud_hdt\n",
            "NLU reference < de.pos > for lang de  points to SPARK NLP model : pos_ud_hdt\n",
            "NLU reference < de.ner > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.ner.wikiner > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.ner.wikiner.glove_840B_300 > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.stopwords > for lang de  points to SPARK NLP model : stopwords_de\n",
            "NLU reference < it.lemma > for lang it  points to SPARK NLP model : lemma_dxc\n",
            "NLU reference < it.lemma.dxc > for lang it  points to SPARK NLP model : lemma_dxc\n",
            "NLU reference < it.sentiment.dxc > for lang it  points to SPARK NLP model : sentiment_dxc\n",
            "NLU reference < it.sentiment > for lang it  points to SPARK NLP model : sentiment_dxc\n",
            "NLU reference < it.pos > for lang it  points to SPARK NLP model : pos_ud_isdt\n",
            "NLU reference < it.pos.ud_isdt > for lang it  points to SPARK NLP model : pos_ud_isdt\n",
            "NLU reference < it.ner > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.ner.wikiner > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.ner.wikiner.glove_840B_300 > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.stopwords > for lang it  points to SPARK NLP model : stopwords_it\n",
            "NLU reference < nb.lemma > for lang nb  points to SPARK NLP model : lemma\n",
            "NLU reference < nb.pos.ud_bokmaal > for lang nb  points to SPARK NLP model : pos_ud_bokmaal\n",
            "NLU reference < no.ner > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne.glove_6B_100 > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne.glove_6B_300 > for lang no  points to SPARK NLP model : norne_6B_300\n",
            "NLU reference < no.ner.norne.glove_840B_300 > for lang no  points to SPARK NLP model : norne_840B_300\n",
            "NLU reference < nn.pos > for lang nn  points to SPARK NLP model : pos_ud_nynorsk\n",
            "NLU reference < nn.pos.ud_nynorsk > for lang nn  points to SPARK NLP model : pos_ud_nynorsk\n",
            "NLU reference < pl.lemma > for lang pl  points to SPARK NLP model : lemma\n",
            "NLU reference < pl.pos > for lang pl  points to SPARK NLP model : pos_ud_lfg\n",
            "NLU reference < pl.pos.ud_lfg > for lang pl  points to SPARK NLP model : pos_ud_lfg\n",
            "NLU reference < pl.ner > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner.glove_6B_100 > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner.glove_6B_300 > for lang pl  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < pl.ner.wikiner.glove_840B_300 > for lang pl  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < pl.stopwords > for lang pl  points to SPARK NLP model : stopwords_pl\n",
            "NLU reference < pt.lemma > for lang pt  points to SPARK NLP model : lemma\n",
            "NLU reference < pt.pos.ud_bosque > for lang pt  points to SPARK NLP model : pos_ud_bosque\n",
            "NLU reference < pt.pos > for lang pt  points to SPARK NLP model : pos_ud_bosque\n",
            "NLU reference < pt.ner > for lang pt  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pt.ner.wikiner.glove_6B_100 > for lang pt  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pt.ner.wikiner.glove_6B_300 > for lang pt  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < pt.ner.wikiner.glove_840B_300 > for lang pt  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < pt.stopwords > for lang pt  points to SPARK NLP model : stopwords_pt\n",
            "NLU reference < ru.lemma > for lang ru  points to SPARK NLP model : lemma\n",
            "NLU reference < ru.pos.ud_gsd > for lang ru  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ru.pos > for lang ru  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ru.ner > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner.glove_6B_100 > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner.glove_6B_300 > for lang ru  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < ru.ner.wikiner.glove_840B_300 > for lang ru  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < ru.stopwords > for lang ru  points to SPARK NLP model : stopwords_ru\n",
            "NLU reference < es.lemma > for lang es  points to SPARK NLP model : lemma\n",
            "NLU reference < es.pos > for lang es  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < es.pos.ud_gsd > for lang es  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < es.ner > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner.glove_6B_100 > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner.glove_6B_300 > for lang es  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < es.ner.wikiner.glove_840B_300 > for lang es  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < es.stopwords_es > for lang es  points to SPARK NLP model : stopwords_es\n",
            "NLU reference < af.stopwords > for lang af  points to SPARK NLP model : stopwords_af\n",
            "NLU reference < ar.stopwords_ar > for lang ar  points to SPARK NLP model : stopwords_ar\n",
            "NLU reference < hy.stopwords > for lang hy  points to SPARK NLP model : stopwords_hy\n",
            "NLU reference < hy.lemma > for lang hy  points to SPARK NLP model : lemma\n",
            "NLU reference < hy.pos > for lang hy  points to SPARK NLP model : pos_ud_armtdp\n",
            "NLU reference < eu.stopwords > for lang eu  points to SPARK NLP model : stopwords_eu\n",
            "NLU reference < eu.lemma > for lang eu  points to SPARK NLP model : lemma\n",
            "NLU reference < eu.pos > for lang eu  points to SPARK NLP model : pos_ud_bdt\n",
            "NLU reference < bn.stopwords > for lang bn  points to SPARK NLP model : stopwords_bn\n",
            "NLU reference < br.stopwords > for lang br  points to SPARK NLP model : stopwords_br\n",
            "NLU reference < br.lemma > for lang br  points to SPARK NLP model : lemma\n",
            "NLU reference < br.pos > for lang br  points to SPARK NLP model : pos_ud_keb\n",
            "NLU reference < bg.lemma > for lang bg  points to SPARK NLP model : lemma\n",
            "NLU reference < bg.pos > for lang bg  points to SPARK NLP model : pos_ud_btb\n",
            "NLU reference < bg.pos.ud_btb > for lang bg  points to SPARK NLP model : pos_ud_btb\n",
            "NLU reference < bg.stopwords > for lang bg  points to SPARK NLP model : stopwords_bg\n",
            "NLU reference < ca.stopwords > for lang ca  points to SPARK NLP model : stopwords_ca\n",
            "NLU reference < ca.lemma > for lang ca  points to SPARK NLP model : lemma\n",
            "NLU reference < ca.pos > for lang ca  points to SPARK NLP model : pos_ud_ancora\n",
            "NLU reference < cs.lemma > for lang cs  points to SPARK NLP model : lemma\n",
            "NLU reference < cs.pos > for lang cs  points to SPARK NLP model : pos_ud_pdt\n",
            "NLU reference < cs.pos.ud_pdt > for lang cs  points to SPARK NLP model : pos_ud_pdt\n",
            "NLU reference < cs.stopwords > for lang cs  points to SPARK NLP model : stopwords_cs\n",
            "NLU reference < eo.stopwords > for lang eo  points to SPARK NLP model : stopwords_eo\n",
            "NLU reference < fi.lemma > for lang fi  points to SPARK NLP model : lemma\n",
            "NLU reference < fi.pos.ud_tdt > for lang fi  points to SPARK NLP model : pos_ud_tdt\n",
            "NLU reference < fi.pos > for lang fi  points to SPARK NLP model : pos_ud_tdt\n",
            "NLU reference < fi.stopwords > for lang fi  points to SPARK NLP model : stopwords_fi\n",
            "NLU reference < gl.stopwords > for lang gl  points to SPARK NLP model : stopwords_gl\n",
            "NLU reference < gl.lemma > for lang gl  points to SPARK NLP model : lemma\n",
            "NLU reference < gl.pos > for lang gl  points to SPARK NLP model : pos_ud_treegal\n",
            "NLU reference < el.lemma > for lang el  points to SPARK NLP model : lemma\n",
            "NLU reference < el.pos > for lang el  points to SPARK NLP model : pos_ud_gdt\n",
            "NLU reference < el.pos.ud_gdt > for lang el  points to SPARK NLP model : pos_ud_gdt\n",
            "NLU reference < el.stopwords > for lang el  points to SPARK NLP model : stopwords_el\n",
            "NLU reference < ha.stopwords > for lang ha  points to SPARK NLP model : stopwords_ha\n",
            "NLU reference < he.stopwords > for lang he  points to SPARK NLP model : stopwords_he\n",
            "NLU reference < hi.stopwords > for lang hi  points to SPARK NLP model : stopwords_hi\n",
            "NLU reference < hi.lemma > for lang hi  points to SPARK NLP model : lemma\n",
            "NLU reference < hi.pos > for lang hi  points to SPARK NLP model : pos_ud_hdtb\n",
            "NLU reference < hu.lemma > for lang hu  points to SPARK NLP model : lemma\n",
            "NLU reference < hu.pos > for lang hu  points to SPARK NLP model : pos_ud_szeged\n",
            "NLU reference < hu.pos.ud_szeged > for lang hu  points to SPARK NLP model : pos_ud_szeged\n",
            "NLU reference < hu.stopwords > for lang hu  points to SPARK NLP model : stopwords_hu\n",
            "NLU reference < id.stopwords > for lang id  points to SPARK NLP model : stopwords_id\n",
            "NLU reference < id.lemma > for lang id  points to SPARK NLP model : lemma\n",
            "NLU reference < id.pos > for lang id  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ga.stopwords > for lang ga  points to SPARK NLP model : stopwords_ga\n",
            "NLU reference < ga.lemma > for lang ga  points to SPARK NLP model : lemma\n",
            "NLU reference < ga.pos > for lang ga  points to SPARK NLP model : pos_ud_idt\n",
            "NLU reference < da.lemma > for lang da  points to SPARK NLP model : lemma\n",
            "NLU reference < da.pos > for lang da  points to SPARK NLP model : pos_ud_ddt\n",
            "NLU reference < ja.stopwords > for lang ja  points to SPARK NLP model : stopwords_ja\n",
            "NLU reference < la.stopwords > for lang la  points to SPARK NLP model : stopwords_la\n",
            "NLU reference < la.lemma > for lang la  points to SPARK NLP model : lemma\n",
            "NLU reference < la.pos > for lang la  points to SPARK NLP model : pos_ud_llct\n",
            "NLU reference < lv.stopwords > for lang lv  points to SPARK NLP model : stopwords_lv\n",
            "NLU reference < lv.lemma > for lang lv  points to SPARK NLP model : lemma\n",
            "NLU reference < lv.pos > for lang lv  points to SPARK NLP model : pos_ud_lvtb\n",
            "NLU reference < mr.stopwords > for lang mr  points to SPARK NLP model : stopwords_mr\n",
            "NLU reference < mr.lemma > for lang mr  points to SPARK NLP model : lemma\n",
            "NLU reference < mr.pos > for lang mr  points to SPARK NLP model : pos_ud_ufal\n",
            "NLU reference < fa.stopwords > for lang fa  points to SPARK NLP model : stopwords_fa\n",
            "NLU reference < ro.lemma > for lang ro  points to SPARK NLP model : lemma\n",
            "NLU reference < ro.pos > for lang ro  points to SPARK NLP model : pos_ud_rrt\n",
            "NLU reference < ro.pos.ud_rrt > for lang ro  points to SPARK NLP model : pos_ud_rrt\n",
            "NLU reference < ro.stopwords > for lang ro  points to SPARK NLP model : stopwords_ro\n",
            "NLU reference < sk.lemma > for lang sk  points to SPARK NLP model : lemma\n",
            "NLU reference < sk.pos > for lang sk  points to SPARK NLP model : pos_ud_snk\n",
            "NLU reference < sk.pos.ud_snk > for lang sk  points to SPARK NLP model : pos_ud_snk\n",
            "NLU reference < sk.stopwords > for lang sk  points to SPARK NLP model : stopwords_sk\n",
            "NLU reference < sl.stopwords > for lang sl  points to SPARK NLP model : stopwords_sl\n",
            "NLU reference < sl.lemma > for lang sl  points to SPARK NLP model : lemma\n",
            "NLU reference < sl.pos > for lang sl  points to SPARK NLP model : pos_ud_ssj\n",
            "NLU reference < so.stopwords > for lang so  points to SPARK NLP model : stopwords_so\n",
            "NLU reference < st.stopwords > for lang st  points to SPARK NLP model : stopwords_st\n",
            "NLU reference < sw.stopwords > for lang sw  points to SPARK NLP model : stopwords_sw\n",
            "NLU reference < sv.lemma > for lang sv  points to SPARK NLP model : lemma\n",
            "NLU reference < sv.pos > for lang sv  points to SPARK NLP model : pos_ud_tal\n",
            "NLU reference < sv.pos.ud_tal > for lang sv  points to SPARK NLP model : pos_ud_tal\n",
            "NLU reference < sv.stopwords > for lang sv  points to SPARK NLP model : stopwords_sv\n",
            "NLU reference < th.stopwords > for lang th  points to SPARK NLP model : stopwords_th\n",
            "NLU reference < tr.lemma > for lang tr  points to SPARK NLP model : lemma\n",
            "NLU reference < tr.pos > for lang tr  points to SPARK NLP model : pos_ud_imst\n",
            "NLU reference < tr.pos.ud_imst > for lang tr  points to SPARK NLP model : pos_ud_imst\n",
            "NLU reference < tr.stopwords > for lang tr  points to SPARK NLP model : stopwords_tr\n",
            "NLU reference < uk.lemma > for lang uk  points to SPARK NLP model : lemma\n",
            "NLU reference < uk.pos > for lang uk  points to SPARK NLP model : pos_ud_iu\n",
            "NLU reference < uk.pos.ud_iu > for lang uk  points to SPARK NLP model : pos_ud_iu\n",
            "NLU reference < yo.stopwords > for lang yo  points to SPARK NLP model : stopwords_yo\n",
            "NLU reference < yo.lemma > for lang yo  points to SPARK NLP model : lemma\n",
            "NLU reference < yo.pos > for lang yo  points to SPARK NLP model : pos_ud_ytb\n",
            "NLU reference < zu.stopwords > for lang zu  points to SPARK NLP model : stopwords_zu\n",
            "NLU reference < xx.embed.glove_840B_300 > for lang xx  points to SPARK NLP model : glove_840B_300\n",
            "NLU reference < xx.embed.glove_6B_300 > for lang xx  points to SPARK NLP model : glove_6B_300\n",
            "NLU reference < xx.embed.bert_multi_cased > for lang xx  points to SPARK NLP model : bert_multi_cased\n",
            "NLU reference < xx.classify.wiki_7 > for lang xx  points to SPARK NLP model : ld_wiki_7\n",
            "NLU reference < xx.classify.wiki_20 > for lang xx  points to SPARK NLP model : ld_wiki_20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGuetrAYpk1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "912d4f47-8c66-4a16-9e53-6179d323fc3b"
      },
      "source": [
        "# any string inside of <> can be apssed to nlu.load()\n",
        "nlu.print_all_nlu_components_for_lang(lang='de')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Pipelines for language de \n",
            "\n",
            "NLU pipe reference :  de.explain.document  points to Spark NLP Pipeline: explain_document_md\n",
            "NLU pipe reference :  de.explain.document.md  points to Spark NLP Pipeline: explain_document_md\n",
            "NLU pipe reference :  de.explain.document.lg  points to Spark NLP Pipeline: explain_document_lg\n",
            "NLU pipe reference :  de.ner.recognizer  points to Spark NLP Pipeline: entity_recognizer_md\n",
            "NLU pipe reference :  de.ner.recognizer.md  points to Spark NLP Pipeline: entity_recognizer_md\n",
            "NLU pipe reference :  de.ner.recognizer.lg  points to Spark NLP Pipeline: entity_recognizer_lg\n",
            "All Pipelines for language de \n",
            " {'de.lemma': 'lemma', 'de.pos.ud_hdt': 'pos_ud_hdt', 'de.pos': 'pos_ud_hdt', 'de.ner': 'wikiner_840B_300', 'de.ner.wikiner': 'wikiner_840B_300', 'de.ner.wikiner.glove_840B_300': 'wikiner_840B_300', 'de.stopwords': 'stopwords_de'}\n",
            "NLU reference :  de.lemma  points to Spark NLP Model:  lemma\n",
            "NLU reference :  de.pos.ud_hdt  points to Spark NLP Model:  pos_ud_hdt\n",
            "NLU reference :  de.pos  points to Spark NLP Model:  pos_ud_hdt\n",
            "NLU reference :  de.ner  points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference :  de.ner.wikiner  points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference :  de.ner.wikiner.glove_840B_300  points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference :  de.stopwords  points to Spark NLP Model:  stopwords_de\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQsSdLMBVlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import nlu\n",
        "import pandas as pd \n",
        "def get_sample_pdf() :\n",
        "    data = {\"text\": ['This day sucks. But tmorow will be beter', 'Donald Trump had a meeting with Tim Apple. ', 'This is your last chance. After this, there is no turning back. You take the blue pill — the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill — you stay in Wonderland and I show you how deep the rabbit-hole goes.' ]}\n",
        "    text_df = pd.DataFrame(data)\n",
        "    return text_df\n",
        "sample_pdf = get_sample_pdf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK3iVVxLFANO",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvq5WpEnFBia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "f38b0f68-e2c0-4069-ff32-4ee9ec3bb193"
      },
      "source": [
        "nlu.load('tokenize').predict('Each word and Symbol in a sentence will generate token.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Each</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>word</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Symbol</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sentence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>will</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>generate</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>token</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>.</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       token          id\n",
              "0       Each  8589934592\n",
              "1       word  8589934592\n",
              "2        and  8589934592\n",
              "3     Symbol  8589934592\n",
              "4         in  8589934592\n",
              "5          a  8589934592\n",
              "6   sentence  8589934592\n",
              "7       will  8589934592\n",
              "8   generate  8589934592\n",
              "9      token  8589934592\n",
              "10         .  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6eOMTMFUsW",
        "colab_type": "text"
      },
      "source": [
        "# Remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnxbz-a3FSku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "199212ed-eb56-49b3-843e-53155783653a"
      },
      "source": [
        "nlu.load('stopwords').predict('I want you to remove stopewords from this sentence please')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stopwords_en download started this may take some time.\n",
            "Approximate size to download 2.9 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleanTokens</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>remove</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stopewords</td>\n",
              "      <td>want</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentence</td>\n",
              "      <td>you</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "      <td>remove</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>None</td>\n",
              "      <td>stopewords</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>None</td>\n",
              "      <td>from</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>None</td>\n",
              "      <td>this</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>None</td>\n",
              "      <td>sentence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>None</td>\n",
              "      <td>please</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  cleanTokens       token          id\n",
              "0      remove           I  8589934592\n",
              "1  stopewords        want  8589934592\n",
              "2    sentence         you  8589934592\n",
              "3        None          to  8589934592\n",
              "4        None      remove  8589934592\n",
              "5        None  stopewords  8589934592\n",
              "6        None        from  8589934592\n",
              "7        None        this  8589934592\n",
              "8        None    sentence  8589934592\n",
              "9        None      please  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4tndNMFp68",
        "colab_type": "text"
      },
      "source": [
        "# Normalize as sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2MI8lgvFO9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "d2adf429-d6b1-49ca-d48d-85d2b4b7046b"
      },
      "source": [
        "nlu.load('norm').predict('I want the stopwords removed from this sentence')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normalized</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>want</td>\n",
              "      <td>want</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopwords</td>\n",
              "      <td>stopwords</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>removed</td>\n",
              "      <td>removed</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>from</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>this</td>\n",
              "      <td>this</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sentence</td>\n",
              "      <td>sentence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  normalized      token          id\n",
              "0          I          I  8589934592\n",
              "1       want       want  8589934592\n",
              "2        the        the  8589934592\n",
              "3  stopwords  stopwords  8589934592\n",
              "4    removed    removed  8589934592\n",
              "5       from       from  8589934592\n",
              "6       this       this  8589934592\n",
              "7   sentence   sentence  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A81OKGHXGXgZ",
        "colab_type": "text"
      },
      "source": [
        "# Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxNEncqGbEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "69bd7b66-eace-421e-a1a6-5cc7b79c2aa7"
      },
      "source": [
        "nlu.load('stem').predict('NLU can get you the stem of a word')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stem</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nlu</td>\n",
              "      <td>NLU</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>can</td>\n",
              "      <td>can</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>get</td>\n",
              "      <td>get</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>you</td>\n",
              "      <td>you</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>stem</td>\n",
              "      <td>stem</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>word</td>\n",
              "      <td>word</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stem token          id\n",
              "0   nlu   NLU  8589934592\n",
              "1   can   can  8589934592\n",
              "2   get   get  8589934592\n",
              "3   you   you  8589934592\n",
              "4   the   the  8589934592\n",
              "5  stem  stem  8589934592\n",
              "6    of    of  8589934592\n",
              "7     a     a  8589934592\n",
              "8  word  word  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaSsjgMKIBhb",
        "colab_type": "text"
      },
      "source": [
        "# POS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMurEqcIBBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "463e6bbc-bc00-452b-c0c5-7e6071991843"
      },
      "source": [
        "nlu.load('pos').predict('Part of speech assigns each token in a sentence a grammatcial label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NN</td>\n",
              "      <td>Part</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NN</td>\n",
              "      <td>speech</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NNS</td>\n",
              "      <td>assigns</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DT</td>\n",
              "      <td>each</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>token</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>IN</td>\n",
              "      <td>in</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NN</td>\n",
              "      <td>sentence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>JJ</td>\n",
              "      <td>grammatcial</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NN</td>\n",
              "      <td>label</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pos        token          id\n",
              "0    NN         Part  8589934592\n",
              "1    IN           of  8589934592\n",
              "2    NN       speech  8589934592\n",
              "3   NNS      assigns  8589934592\n",
              "4    DT         each  8589934592\n",
              "5    NN        token  8589934592\n",
              "6    IN           in  8589934592\n",
              "7    DT            a  8589934592\n",
              "8    NN     sentence  8589934592\n",
              "9    DT            a  8589934592\n",
              "10   JJ  grammatcial  8589934592\n",
              "11   NN        label  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ury8E3fINGh",
        "colab_type": "text"
      },
      "source": [
        "# Emotions Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGKJ_kefIJXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "49ebee1c-db6a-4197-f3cf-1936a657941a"
      },
      "source": [
        "nlu.load('classify.emotion').predict(\"I love NLU!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_emotion download started this may take some time.\n",
            "Approximate size to download 20.7 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_surprise</th>\n",
              "      <th>category_sadness</th>\n",
              "      <th>category_joy</th>\n",
              "      <th>category_fear</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.027570432052016258, -0.052647676318883896, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.012899903</td>\n",
              "      <td>0.0015578865</td>\n",
              "      <td>0.9760173</td>\n",
              "      <td>0.0095249</td>\n",
              "      <td>I love NLU!</td>\n",
              "      <td>joy</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.027570432052016258, -0.052647676318883896, ...  ...  8589934592\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-nQoMUIMBu",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn__rI5oILfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "898db530-4265-4112-c029-c334bbaade36"
      },
      "source": [
        "nlu.load('sentiment').predict(\"I hate this guy Sami\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>checked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5778</td>\n",
              "      <td>I hate this guy Sami</td>\n",
              "      <td>negative</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[I, hate, this, guy, Sami]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment_confidence  ...                     checked\n",
              "0               0.5778  ...  [I, hate, this, guy, Sami]\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMq33Df0IpWY",
        "colab_type": "text"
      },
      "source": [
        "# Spell checking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKxJ0tZ_Iva0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "9444fe73-2aca-4632-af8d-f4b839011596"
      },
      "source": [
        "nlu.load('spell').predict('I liek pentut butr and jelli')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check_spelling download started this may take some time.\n",
            "Approx size to download 892.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checked</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>like</td>\n",
              "      <td>liek</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pentut</td>\n",
              "      <td>pentut</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>butt</td>\n",
              "      <td>butr</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jelly</td>\n",
              "      <td>jelli</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  checked   token          id\n",
              "0       I       I  8589934592\n",
              "1    like    liek  8589934592\n",
              "2  pentut  pentut  8589934592\n",
              "3    butt    butr  8589934592\n",
              "4     and     and  8589934592\n",
              "5   jelly   jelli  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WNVlCAVIpD2",
        "colab_type": "text"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vrfEGjILdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "4f5e90cd-496c-4e99-ceef-9913e46e760a"
      },
      "source": [
        "nlu.load('lemma').predict('Lemmatizing generates a less noisy version of the inputted tokens')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lemmatizing</td>\n",
              "      <td>Lemmatizing</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>generate</td>\n",
              "      <td>generates</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>less</td>\n",
              "      <td>less</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>noisy</td>\n",
              "      <td>noisy</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>version</td>\n",
              "      <td>version</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>input</td>\n",
              "      <td>inputted</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>token</td>\n",
              "      <td>tokens</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lemma        token          id\n",
              "0  Lemmatizing  Lemmatizing  8589934592\n",
              "1     generate    generates  8589934592\n",
              "2            a            a  8589934592\n",
              "3         less         less  8589934592\n",
              "4        noisy        noisy  8589934592\n",
              "5      version      version  8589934592\n",
              "6           of           of  8589934592\n",
              "7          the          the  8589934592\n",
              "8        input     inputted  8589934592\n",
              "9        token       tokens  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ72zsvuI8Jg",
        "colab_type": "text"
      },
      "source": [
        "# Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8DyiMFPILWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "38a49384-5b84-4e52-eef8-aef3acf0d732"
      },
      "source": [
        "nlu.load('norm').predict('@CKL_IT says that #normalizers are pretty useful to clean #structured_strings in #NLU like tweets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normalized</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CKLIT</td>\n",
              "      <td>@CKL_IT</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>says</td>\n",
              "      <td>says</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that</td>\n",
              "      <td>that</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>normalizers</td>\n",
              "      <td>#normalizers</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>are</td>\n",
              "      <td>are</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>pretty</td>\n",
              "      <td>pretty</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>useful</td>\n",
              "      <td>useful</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>to</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>clean</td>\n",
              "      <td>clean</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>structuredstrings</td>\n",
              "      <td>#structured_strings</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NLU</td>\n",
              "      <td>#NLU</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>like</td>\n",
              "      <td>like</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>tweets</td>\n",
              "      <td>tweets</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           normalized                token          id\n",
              "0               CKLIT              @CKL_IT  8589934592\n",
              "1                says                 says  8589934592\n",
              "2                that                 that  8589934592\n",
              "3         normalizers         #normalizers  8589934592\n",
              "4                 are                  are  8589934592\n",
              "5              pretty               pretty  8589934592\n",
              "6              useful               useful  8589934592\n",
              "7                  to                   to  8589934592\n",
              "8               clean                clean  8589934592\n",
              "9   structuredstrings  #structured_strings  8589934592\n",
              "10                 in                   in  8589934592\n",
              "11                NLU                 #NLU  8589934592\n",
              "12               like                 like  8589934592\n",
              "13             tweets               tweets  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlW3GGxPI_Bk",
        "colab_type": "text"
      },
      "source": [
        "# Language Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPJQo5nOILPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "e9aa3f4e-d725-45f8-dcdd-68067b409025"
      },
      "source": [
        "nlu.load('lang').predict(['NLU is an open-source text processing library for advanced natural language processing for the Python.','NLU est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python.'])\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detect_language_20 download started this may take some time.\n",
            "Approx size to download 3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>language_de</th>\n",
              "      <th>language_no</th>\n",
              "      <th>language_ru</th>\n",
              "      <th>language_sv</th>\n",
              "      <th>language_fi</th>\n",
              "      <th>language_pt</th>\n",
              "      <th>language_bg</th>\n",
              "      <th>language_el</th>\n",
              "      <th>language_en</th>\n",
              "      <th>language_hr</th>\n",
              "      <th>language_it</th>\n",
              "      <th>language_fr</th>\n",
              "      <th>language_hu</th>\n",
              "      <th>language_es</th>\n",
              "      <th>language_cs</th>\n",
              "      <th>language_uk</th>\n",
              "      <th>language_sk</th>\n",
              "      <th>language_pl</th>\n",
              "      <th>language_ro</th>\n",
              "      <th>language_tr</th>\n",
              "      <th>document</th>\n",
              "      <th>language</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10927795E-4</td>\n",
              "      <td>1.786265E-4</td>\n",
              "      <td>3.09676E-5</td>\n",
              "      <td>0.005297283</td>\n",
              "      <td>1.085274E-5</td>\n",
              "      <td>4.7062217E-6</td>\n",
              "      <td>6.4429906E-7</td>\n",
              "      <td>0.0011827932</td>\n",
              "      <td>0.9854069</td>\n",
              "      <td>1.6956832E-6</td>\n",
              "      <td>1.4030554E-5</td>\n",
              "      <td>1.466399E-4</td>\n",
              "      <td>3.2495E-6</td>\n",
              "      <td>0.007108454</td>\n",
              "      <td>8.250847E-5</td>\n",
              "      <td>1.2385209E-4</td>\n",
              "      <td>2.4604517E-6</td>\n",
              "      <td>1.9354234E-4</td>\n",
              "      <td>1.2024728E-5</td>\n",
              "      <td>8.7725675E-5</td>\n",
              "      <td>NLU is an open-source text processing library ...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.9392602E-6</td>\n",
              "      <td>3.7423422E-5</td>\n",
              "      <td>1.5859371E-6</td>\n",
              "      <td>4.2966826E-6</td>\n",
              "      <td>2.0913217E-6</td>\n",
              "      <td>3.0820165E-6</td>\n",
              "      <td>3.3508215E-7</td>\n",
              "      <td>8.960027E-8</td>\n",
              "      <td>2.0083774E-6</td>\n",
              "      <td>4.2742064E-7</td>\n",
              "      <td>4.701018E-5</td>\n",
              "      <td>0.9998223</td>\n",
              "      <td>6.3511393E-6</td>\n",
              "      <td>6.6381785E-5</td>\n",
              "      <td>8.5110266E-8</td>\n",
              "      <td>1.8988333E-6</td>\n",
              "      <td>1.6178213E-8</td>\n",
              "      <td>3.8753042E-10</td>\n",
              "      <td>9.3351207E-7</td>\n",
              "      <td>6.692207E-7</td>\n",
              "      <td>NLU est une bibliothèque de traitement de text...</td>\n",
              "      <td>fr</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     language_de   language_no  ... language          id\n",
              "0  1.10927795E-4   1.786265E-4  ...       en           0\n",
              "1   2.9392602E-6  3.7423422E-5  ...       fr  8589934592\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfzGkmoJc8H",
        "colab_type": "text"
      },
      "source": [
        "#Named Entity Recognition ( NER ) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yKrHWIgJUTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9c18f82f-5ffe-49a9-8169-a472618dafe1"
      },
      "source": [
        "nlu.load('ner').predict('Donald Trump had a meeting with Tim Appe in new York!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recognize_entities_dl download started this may take some time.\n",
            "Approx size to download 159 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>embeddings_embeddings</th>\n",
              "      <th>entities</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump had a meeting with Tim Appe in ne...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[-0.5496799945831299, -0.488319993019104, 0.5...</td>\n",
              "      <td>[Donald Trump, Tim Appe, York!]</td>\n",
              "      <td>[B-PER, I-PER, O, O, O, O, B-PER, I-PER, O, O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  ...                                                ner\n",
              "0  Donald Trump had a meeting with Tim Appe in ne...  ...  [B-PER, I-PER, O, O, O, O, B-PER, I-PER, O, O,...\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE375gf4JlMS",
        "colab_type": "text"
      },
      "source": [
        "# Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqFz6sgdJhej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9b6f7d55-242c-418e-da95-eae8a83206e0"
      },
      "source": [
        "nlu.load('ngram').predict('To be or not to be ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>ngrams</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To be or not to be</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[To, be, or, not, to, be, To be, be or, or not...</td>\n",
              "      <td>[TO, VB, CC, RB, TO, VB]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             document  ...                       pos\n",
              "0  To be or not to be  ...  [TO, VB, CC, RB, TO, VB]\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAhHfrujJ01j",
        "colab_type": "text"
      },
      "source": [
        "# Regex Matching\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcp5zFfbKAtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlu.load('match.regex').predict('Wht a wondful day!')  # work in Progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd1gk3TUKFNy",
        "colab_type": "text"
      },
      "source": [
        "# text matching "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbebUDHyKHFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlu.load('match.text').predict('Wht a wondful day!')  # work in Progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvcmsI-TKbPC",
        "colab_type": "text"
      },
      "source": [
        "# Date Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBukfPeHKawP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4af32052-f334-4f25-d7b0-593999e1c944"
      },
      "source": [
        "nlu.load('match.datetime').predict(' In the years 2000/01/01 to 2010/01/01 a lot of things happend') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_datetime download started this may take some time.\n",
            "Approx size to download 12.9 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the years 2000/01/01 to 2010/01/01 a lot o...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[2000/01/01, 2001/01/01]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  ...                      date\n",
              "0   In the years 2000/01/01 to 2010/01/01 a lot o...  ...  [2000/01/01, 2001/01/01]\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4d3N3VsKpqA",
        "colab_type": "text"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Ww77_vKOiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "4d726612-9577-4143-b112-f175875bffb9"
      },
      "source": [
        "# First we load the pipeline\n",
        "pipe = nlu.load('match.chunks')\n",
        "# Now we print the info to see at which index which com,ponent is and what parameters we can configure on them \n",
        "pipe.print_info()\n",
        "# Lets set our Chunker to only match NN\n",
        "pipe.pipe_components[4].model.setRegexParsers(['<NN>+'])\n",
        "# Now we can predict with the configured pipeline\n",
        "pipe.predict(\"Jim and Joe went to the market next to the town hall\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_chunks download started this may take some time.\n",
            "Approx size to download 4.3 MB\n",
            "[OK!]\n",
            "-------------------------------------At pipe.pipe_components[0].model  : document_assembler with configurable parameters: --------------------------------------\n",
            "Param Name [ cleanupMode ] :  Param Info : possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full  currenlty Configured as :  disabled\n",
            "-------------------------------------At pipe.pipe_components[1].model  : document_assembler with configurable parameters: --------------------------------------\n",
            "Param Name [ customBounds ] :  Param Info : characters used to explicitly mark sentence bounds  currenlty Configured as :  []\n",
            "Param Name [ explodeSentences ] :  Param Info : whether to explode each sentence into a different row, for better parallelization. Defaults to false.  currenlty Configured as :  False\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ maxLength ] :  Param Info : Set the maximum allowed length for each sentence  currenlty Configured as :  99999\n",
            "Param Name [ minLength ] :  Param Info : Set the minimum allowed length for each sentence.  currenlty Configured as :  0\n",
            "Param Name [ useAbbreviations ] :  Param Info : whether to apply abbreviations at sentence detection  currenlty Configured as :  True\n",
            "Param Name [ useCustomBoundsOnly ] :  Param Info : Only utilize custom bounds in sentence detection  currenlty Configured as :  False\n",
            "----------------------------------------At pipe.pipe_components[2].model  : date_matcher with configurable parameters: -----------------------------------------\n",
            "Param Name [ caseSensitiveExceptions ] :  Param Info : Whether to care for case sensitiveness in exceptions  currenlty Configured as :  True\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ targetPattern ] :  Param Info : pattern to grab from text as token candidates. Defaults \\S+  currenlty Configured as :  \\S+\n",
            "Param Name [ maxLength ] :  Param Info : Set the maximum allowed length for each token  currenlty Configured as :  99999\n",
            "Param Name [ minLength ] :  Param Info : Set the minimum allowed length for each token  currenlty Configured as :  0\n",
            "----------------------------------------At pipe.pipe_components[3].model  : sentiment_dl  with configurable parameters: ----------------------------------------\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "---------------------------------------At pipe.pipe_components[4].model  : default_chunker with configurable parameters: ---------------------------------------\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ regexParsers ] :  Param Info : an array of grammar based chunk parsers  currenlty Configured as :  ['<NN>+']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>id</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>market</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>town hall</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       chunk          id                                                pos\n",
              "0     market  8589934592  [NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...\n",
              "1  town hall  8589934592  [NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kusWLGiiK7y4",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Detector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlILHSIoK0dL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "796f32e2-d5e1-4b74-883a-d27cea6e8d63"
      },
      "source": [
        "nlu.load('sentence_detector').predict('NLU can detect things. Like beginning and endings of sentences. It can also do much more!', output_level ='sentence')   # todo buggy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_dl_sentence download started this may take some time.\n",
            "Approximate size to download 13.3 MB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>ner</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLU can detect things.</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Like beginning and endings of sentences.</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It can also do much more!</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   sentence  ...                                                pos\n",
              "0                    NLU can detect things.  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "1  Like beginning and endings of sentences.  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "2                 It can also do much more!  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLDT5FJJLS2Z",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Biobert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er9ACOKKMBJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "0e685060-2fc7-43be-cb8b-f9003a6b36d0"
      },
      "source": [
        "nlu.load('biobert').predict('Biobert was pretrained on a medical dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biobert_pubmed_base_cased download started this may take some time.\n",
            "Approximate size to download 384.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-1.1250135898590088, -1.468251347541809, 0.45...</td>\n",
              "      <td>Biobert</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.2141287326812744, 0.513595461845398, 0.108...</td>\n",
              "      <td>was</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0130831003189087, -1.8673359155654907, -0.6...</td>\n",
              "      <td>pretrained</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.03375675156712532, 0.03003840334713459, 0.0...</td>\n",
              "      <td>on</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.2643347382545471, 1.3394858837127686, -0.15...</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.0770333781838417, -1.6575649976730347, -0....</td>\n",
              "      <td>medical</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.7789418697357178, 0.11998558044433594, 0.01...</td>\n",
              "      <td>dataset</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     bert_embeddings       token          id\n",
              "0  [-1.1250135898590088, -1.468251347541809, 0.45...     Biobert  8589934592\n",
              "1  [-1.2141287326812744, 0.513595461845398, 0.108...         was  8589934592\n",
              "2  [1.0130831003189087, -1.8673359155654907, -0.6...  pretrained  8589934592\n",
              "3  [0.03375675156712532, 0.03003840334713459, 0.0...          on  8589934592\n",
              "4  [0.2643347382545471, 1.3394858837127686, -0.15...           a  8589934592\n",
              "5  [-0.0770333781838417, -1.6575649976730347, -0....     medical  8589934592\n",
              "6  [1.7789418697357178, 0.11998558044433594, 0.01...     dataset  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDpfxO1Ta3Kn",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVfo2nCaLSiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "25bd6587-432a-4f3c-a60e-ea74ad7940d1"
      },
      "source": [
        "nlu.load('bert').predict('NLU offers the latest embeddings in one line ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_uncased download started this may take some time.\n",
            "Approximate size to download 392.5 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.3253086805343628, -0.574441134929657, -0.08...</td>\n",
              "      <td>Nlu</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-0.6288262605667114, 0.016402214765548706, -0...</td>\n",
              "      <td>please</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.29604873061180115, -1.0217010974884033, -0...</td>\n",
              "      <td>embed</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.45352810621261597, 0.20624420046806335, -0...</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-1.0501813888549805, -0.271099328994751, -0.5...</td>\n",
              "      <td>tokens</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.38442206382751465, 0.2812134623527527, 0.2...</td>\n",
              "      <td>with</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.647590160369873, 0.4462052583694458, 0.5612...</td>\n",
              "      <td>Bert</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0.5412184000015259, -0.18504130840301514, -0....</td>\n",
              "      <td>!</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     bert_embeddings   token          id\n",
              "0  [0.3253086805343628, -0.574441134929657, -0.08...     Nlu  8589934592\n",
              "1  [-0.6288262605667114, 0.016402214765548706, -0...  please  8589934592\n",
              "2  [-0.29604873061180115, -1.0217010974884033, -0...   embed  8589934592\n",
              "3  [-0.45352810621261597, 0.20624420046806335, -0...     the  8589934592\n",
              "4  [-1.0501813888549805, -0.271099328994751, -0.5...  tokens  8589934592\n",
              "5  [-0.38442206382751465, 0.2812134623527527, 0.2...    with  8589934592\n",
              "6  [0.647590160369873, 0.4462052583694458, 0.5612...    Bert  8589934592\n",
              "7  [0.5412184000015259, -0.18504130840301514, -0....       !  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQUJHqBa5z1",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Albert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpPbMuzMRQ9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "9f39be24-cd5c-4d93-e7e3-390bfe3d9a34"
      },
      "source": [
        "nlu.load('albert').predict('Albert uses a collection of many berts to generate embeddigns ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "albert_base_uncased download started this may take some time.\n",
            "Approximate size to download 42.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>albert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.08257609605789185, -0.8017427325248718, 1....</td>\n",
              "      <td>Albert</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.8256351947784424, -1.5144840478897095, 0.90...</td>\n",
              "      <td>uses</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.22089454531669617, -0.24295514822006226, 3...</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.2136894017457962, -0.8225528597831726, -0....</td>\n",
              "      <td>collection</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.7623294591903687, -1.113651156425476, 0.800...</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.6415284872055054, -0.04533941298723221, 1.9...</td>\n",
              "      <td>many</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-0.5591965317726135, -1.1773797273635864, -0....</td>\n",
              "      <td>berts</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0956681966781616, -1.4180747270584106, -0.2...</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.6759272813796997, -1.3546931743621826, 1.6...</td>\n",
              "      <td>generate</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.0035803020000457764, -0.35928264260292053,...</td>\n",
              "      <td>embeddigns</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   albert_embeddings       token          id\n",
              "0  [-0.08257609605789185, -0.8017427325248718, 1....      Albert  8589934592\n",
              "1  [0.8256351947784424, -1.5144840478897095, 0.90...        uses  8589934592\n",
              "2  [-0.22089454531669617, -0.24295514822006226, 3...           a  8589934592\n",
              "3  [-0.2136894017457962, -0.8225528597831726, -0....  collection  8589934592\n",
              "4  [1.7623294591903687, -1.113651156425476, 0.800...          of  8589934592\n",
              "5  [0.6415284872055054, -0.04533941298723221, 1.9...        many  8589934592\n",
              "6  [-0.5591965317726135, -1.1773797273635864, -0....       berts  8589934592\n",
              "7  [1.0956681966781616, -1.4180747270584106, -0.2...          to  8589934592\n",
              "8  [-0.6759272813796997, -1.3546931743621826, 1.6...    generate  8589934592\n",
              "9  [-0.0035803020000457764, -0.35928264260292053,...  embeddigns  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSIqvC_4a8A4",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Elmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4prfr_0BR5NS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "39dbe320-2cb6-42cd-c329-9c160451e70f"
      },
      "source": [
        "nlu.load('elmo').predict('Elmo was trained on Left to right masked to learn its embeddings')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo download started this may take some time.\n",
            "Approximate size to download 334.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>elmo_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.6083735227584839, 0.20089012384414673, 0.42...</td>\n",
              "      <td>Elmo</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.2980785369873047, -0.07382500916719437, -0....</td>\n",
              "      <td>was</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.39923471212387085, 0.17155063152313232, 0....</td>\n",
              "      <td>trained</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.04337821900844574, 0.1392083466053009, -0.4...</td>\n",
              "      <td>on</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.4468783736228943, -0.623046875, 0.771505534...</td>\n",
              "      <td>Left</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.18209676444530487, 0.03812692314386368, 0....</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.23305709660053253, -0.6459438800811768, 0.5...</td>\n",
              "      <td>right</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-0.7243442535400391, 0.10247116535902023, 0.1...</td>\n",
              "      <td>masked</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.18209676444530487, 0.03812692314386368, 0....</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.2942464351654053, 0.7376189231872559, -0.58...</td>\n",
              "      <td>learn</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.055951207876205444, 0.19218483567237854, -0...</td>\n",
              "      <td>its</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[-1.31377112865448, 0.7727609872817993, 0.6748...</td>\n",
              "      <td>embeddings</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      elmo_embeddings       token          id\n",
              "0   [0.6083735227584839, 0.20089012384414673, 0.42...        Elmo  8589934592\n",
              "1   [0.2980785369873047, -0.07382500916719437, -0....         was  8589934592\n",
              "2   [-0.39923471212387085, 0.17155063152313232, 0....     trained  8589934592\n",
              "3   [0.04337821900844574, 0.1392083466053009, -0.4...          on  8589934592\n",
              "4   [0.4468783736228943, -0.623046875, 0.771505534...        Left  8589934592\n",
              "5   [-0.18209676444530487, 0.03812692314386368, 0....          to  8589934592\n",
              "6   [0.23305709660053253, -0.6459438800811768, 0.5...       right  8589934592\n",
              "7   [-0.7243442535400391, 0.10247116535902023, 0.1...      masked  8589934592\n",
              "8   [-0.18209676444530487, 0.03812692314386368, 0....          to  8589934592\n",
              "9   [1.2942464351654053, 0.7376189231872559, -0.58...       learn  8589934592\n",
              "10  [0.055951207876205444, 0.19218483567237854, -0...         its  8589934592\n",
              "11  [-1.31377112865448, 0.7727609872817993, 0.6748...  embeddings  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkczWqZ6bEgm",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings Xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMNdg7U_TXeW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "525ef73f-2a94-4cd4-9d07-32b1b9eeec21"
      },
      "source": [
        "import nlu \n",
        "nlu.load('xlnet').predict('XLNET computes contextualized word representations using combination of Autoregressive Language Model and Permutation Language Model')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xlnet_base_cased download started this may take some time.\n",
            "Approximate size to download 415.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xlnet_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.02719488926231861, -1.7693557739257812, -0...</td>\n",
              "      <td>XLNET</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.8262947797775269, 0.8455266356468201, 0.57...</td>\n",
              "      <td>computes</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2.8446314334869385, -0.3564329445362091, -2.1...</td>\n",
              "      <td>contextualized</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.6143839359283447, -1.7368144989013672, -0....</td>\n",
              "      <td>word</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.30445945262908936, -1.2129613161087036, 0....</td>\n",
              "      <td>representations</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.07423821836709976, -0.02561005763709545, -0...</td>\n",
              "      <td>using</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-0.5387097597122192, -1.1827564239501953, 0.5...</td>\n",
              "      <td>combination</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-1.403516411781311, 0.3108177185058594, -0.32...</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-1.0869172811508179, 0.7135171890258789, -0.2...</td>\n",
              "      <td>Autoregressive</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.33215752243995667, -1.4108021259307861, -0...</td>\n",
              "      <td>Language</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[-1.6097160577774048, -0.2548254430294037, 0.0...</td>\n",
              "      <td>Model</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[0.7884324789047241, -1.507911205291748, 0.677...</td>\n",
              "      <td>and</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[0.6049966812133789, -0.157279372215271, -0.06...</td>\n",
              "      <td>Permutation</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[-0.33215752243995667, -1.4108021259307861, -0...</td>\n",
              "      <td>Language</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[-1.6097160577774048, -0.2548254430294037, 0.0...</td>\n",
              "      <td>Model</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     xlnet_embeddings  ...          id\n",
              "0   [-0.02719488926231861, -1.7693557739257812, -0...  ...  8589934592\n",
              "1   [-1.8262947797775269, 0.8455266356468201, 0.57...  ...  8589934592\n",
              "2   [2.8446314334869385, -0.3564329445362091, -2.1...  ...  8589934592\n",
              "3   [-0.6143839359283447, -1.7368144989013672, -0....  ...  8589934592\n",
              "4   [-0.30445945262908936, -1.2129613161087036, 0....  ...  8589934592\n",
              "5   [0.07423821836709976, -0.02561005763709545, -0...  ...  8589934592\n",
              "6   [-0.5387097597122192, -1.1827564239501953, 0.5...  ...  8589934592\n",
              "7   [-1.403516411781311, 0.3108177185058594, -0.32...  ...  8589934592\n",
              "8   [-1.0869172811508179, 0.7135171890258789, -0.2...  ...  8589934592\n",
              "9   [-0.33215752243995667, -1.4108021259307861, -0...  ...  8589934592\n",
              "10  [-1.6097160577774048, -0.2548254430294037, 0.0...  ...  8589934592\n",
              "11  [0.7884324789047241, -1.507911205291748, 0.677...  ...  8589934592\n",
              "12  [0.6049966812133789, -0.157279372215271, -0.06...  ...  8589934592\n",
              "13  [-0.33215752243995667, -1.4108021259307861, -0...  ...  8589934592\n",
              "14  [-1.6097160577774048, -0.2548254430294037, 0.0...  ...  8589934592\n",
              "\n",
              "[15 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ5gWHp4bGtI",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZTQl5j7SGHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "b9ec5c9c-6c66-4cd0-f881-ccf1fa0b3a71"
      },
      "source": [
        "nlu.load('glove').predict('Glove embeddings are generated by aggregating global word-word co-occurrence matrix from a corpus')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>glove_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.3677999973297119, 0.37073999643325806, 0.32...</td>\n",
              "      <td>Glove</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.732479989528656, 0.3734700083732605, 0.0188...</td>\n",
              "      <td>embeddings</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.5153300166130066, 0.8318600058555603, 0.22...</td>\n",
              "      <td>are</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.35510000586509705, 0.6115900278091431, 0.4...</td>\n",
              "      <td>generated</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.20874999463558197, -0.11739999800920486, 0...</td>\n",
              "      <td>by</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.5133699774742126, 0.04489300027489662, 0.1...</td>\n",
              "      <td>aggregating</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.24281999468803406, 0.6170300245285034, 0.66...</td>\n",
              "      <td>global</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>word-word</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0.16384999454021454, -0.3178800046443939, 0.1...</td>\n",
              "      <td>co-occurrence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.2663800120353699, 0.4449099898338318, 0.32...</td>\n",
              "      <td>matrix</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.30730998516082764, 0.24737000465393066, 0.6...</td>\n",
              "      <td>from</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[-0.2708599865436554, 0.04400600120425224, -0....</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[0.39937999844551086, 0.15894000232219696, -0....</td>\n",
              "      <td>corpus</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     glove_embeddings  ...          id\n",
              "0   [0.3677999973297119, 0.37073999643325806, 0.32...  ...  8589934592\n",
              "1   [0.732479989528656, 0.3734700083732605, 0.0188...  ...  8589934592\n",
              "2   [-0.5153300166130066, 0.8318600058555603, 0.22...  ...  8589934592\n",
              "3   [-0.35510000586509705, 0.6115900278091431, 0.4...  ...  8589934592\n",
              "4   [-0.20874999463558197, -0.11739999800920486, 0...  ...  8589934592\n",
              "5   [-0.5133699774742126, 0.04489300027489662, 0.1...  ...  8589934592\n",
              "6   [0.24281999468803406, 0.6170300245285034, 0.66...  ...  8589934592\n",
              "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  8589934592\n",
              "8   [0.16384999454021454, -0.3178800046443939, 0.1...  ...  8589934592\n",
              "9   [-0.2663800120353699, 0.4449099898338318, 0.32...  ...  8589934592\n",
              "10  [0.30730998516082764, 0.24737000465393066, 0.6...  ...  8589934592\n",
              "11  [-0.2708599865436554, 0.04400600120425224, -0....  ...  8589934592\n",
              "12  [0.39937999844551086, 0.15894000232219696, -0....  ...  8589934592\n",
              "\n",
              "[13 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id8zubzibIeG",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Embeddings Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H64s1vH6V8Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "070d4f65-13b5-4c5c-cbe1-6233e5044d3b"
      },
      "source": [
        "nlu.load('use').predict('USE is designed to encode whole sentences and documents into vectors that can be used for text classification, sementic similarirty, clustering or oder NLP tasks')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_embeddings</th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.03302069380879402, -0.004255455918610096, -...</td>\n",
              "      <td>USE is designed to encode whole sentences and ...</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      use_embeddings  ...          id\n",
              "0  [0.03302069380879402, -0.004255455918610096, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MyOnSpYLXQM",
        "colab_type": "text"
      },
      "source": [
        "# Chunk embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOs-2IHiLIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlu.load('embed_chunk').predict('a wondful day') # work in progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyo_d5bILbeO",
        "colab_type": "text"
      },
      "source": [
        "# sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Cwk1skLaQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "8038a7be-cf52-401a-8579-e02cd9a0ee5e"
      },
      "source": [
        "nlu.load('embed_sentence').predict('Embed all the sentences. In the input please')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_embeddings</th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.01943225972354412, 0.012606756761670113, -...</td>\n",
              "      <td>Embed all the sentences.</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.02531137689948082, -0.016110848635435104, -...</td>\n",
              "      <td>In the input please</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      use_embeddings  ...          id\n",
              "0  [-0.01943225972354412, 0.012606756761670113, -...  ...  8589934592\n",
              "1  [0.02531137689948082, -0.016110848635435104, -...  ...  8589934592\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0tcHMcCLfUH",
        "colab_type": "text"
      },
      "source": [
        "# multiple embeddings at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAw80hveLdfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlu.load('elmo bert glove').predict('You can get multiple embeddings at once in NLU in just one line!') #watch out RAM killer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_jscfJLiWE",
        "colab_type": "text"
      },
      "source": [
        "# Dependency parsing untyped \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw3ypT1Ljfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "a0ca330e-e781-42ce-a3b2-06559328f3d1"
      },
      "source": [
        "nlu.load('dep.untyped').predict('Untyped Dependencies represent a grammatical tree structure')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.6 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>token</th>\n",
              "      <th>dependency</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNP</td>\n",
              "      <td>Untyped</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NNP</td>\n",
              "      <td>Dependencies</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBD</td>\n",
              "      <td>represent</td>\n",
              "      <td>Untyped</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JJ</td>\n",
              "      <td>grammatical</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>tree</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NN</td>\n",
              "      <td>structure</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pos         token dependency          id\n",
              "0  NNP       Untyped       ROOT  8589934592\n",
              "1  NNP  Dependencies  represent  8589934592\n",
              "2  VBD     represent    Untyped  8589934592\n",
              "3   DT             a  structure  8589934592\n",
              "4   JJ   grammatical  structure  8589934592\n",
              "5   NN          tree  structure  8589934592\n",
              "6   NN     structure  represent  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx2P6P_zLnSC",
        "colab_type": "text"
      },
      "source": [
        "# Dependency parsing typed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YpvBNBjLm6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "93780354-31df-4b24-f7b9-5acf4024d300"
      },
      "source": [
        "import nlu \n",
        "\n",
        "nlu.load('dep').predict('Typed Dependencies represent a grammatical tree structure where every edge has a label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_typed_conllu download started this may take some time.\n",
            "Approximate size to download 257.4 KB\n",
            "[OK!]\n",
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.6 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>labled_dependency</th>\n",
              "      <th>dependency</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNP</td>\n",
              "      <td>root</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>Typed</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>represent</td>\n",
              "      <td>Dependencies</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBD</td>\n",
              "      <td>parataxis</td>\n",
              "      <td>Typed</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>structure</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JJ</td>\n",
              "      <td>amod</td>\n",
              "      <td>structure</td>\n",
              "      <td>grammatical</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>flat</td>\n",
              "      <td>structure</td>\n",
              "      <td>tree</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>represent</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WRB</td>\n",
              "      <td>mark</td>\n",
              "      <td>structure</td>\n",
              "      <td>where</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>edge</td>\n",
              "      <td>every</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>where</td>\n",
              "      <td>edge</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>VBZ</td>\n",
              "      <td>root</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>has</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>label</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>has</td>\n",
              "      <td>label</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pos labled_dependency dependency         token          id\n",
              "0   NNP              root       ROOT         Typed  8589934592\n",
              "1   NNP             nsubj  represent  Dependencies  8589934592\n",
              "2   VBD         parataxis      Typed     represent  8589934592\n",
              "3    DT             nsubj  structure             a  8589934592\n",
              "4    JJ              amod  structure   grammatical  8589934592\n",
              "5    NN              flat  structure          tree  8589934592\n",
              "6    NN             nsubj  represent     structure  8589934592\n",
              "7   WRB              mark  structure         where  8589934592\n",
              "8    DT             nsubj       edge         every  8589934592\n",
              "9    NN             nsubj      where          edge  8589934592\n",
              "10  VBZ              root       ROOT           has  8589934592\n",
              "11   DT             nsubj      label             a  8589934592\n",
              "12   NN             nsubj        has         label  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DqtDX-xGYS7",
        "colab_type": "text"
      },
      "source": [
        "# Classify Twitter sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jrDm6E9QeuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9796d00f-e95e-4428-d790-2fdec49edb3c"
      },
      "source": [
        "nlu.load('en.sentiment.twitter').predict('@elonmusk Tesla stock price is too high imo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentimentdl_use_twitter download started this may take some time.\n",
            "Approx size to download 928.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_positive</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@elonmusk Tesla stock price is too high imo</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.08604438602924347, 0.04703635722398758, -0...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[1.692714735043349e-36]</td>\n",
              "      <td>[negative]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      document  ...   sentiment\n",
              "0  @elonmusk Tesla stock price is too high imo  ...  [negative]\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkF6S7yhGV0H",
        "colab_type": "text"
      },
      "source": [
        "# Classify Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVpv6bWFQhh7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "60f060d3-84e5-45ff-c95e-88ffa8cf0b52"
      },
      "source": [
        "nlu.load('en.sentiment.imdb').predict('The Matrix was a pretty good movie')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentimentdl_use_imdb download started this may take some time.\n",
            "Approx size to download 935.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_positive</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Matrix was a pretty good movie</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.04629608988761902, -0.020867452025413513, ...</td>\n",
              "      <td>[2.7235753918830596e-07]</td>\n",
              "      <td>[2.7235753918830596e-07]</td>\n",
              "      <td>[0.9999997615814209]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             document  ...   sentiment\n",
              "0  The Matrix was a pretty good movie  ...  [positive]\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gNma9kAGS_w",
        "colab_type": "text"
      },
      "source": [
        "# Classify Sarcasm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgLay6JFSa1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "ac28d6fd-293e-4dd3-b703-488903e9f5c2"
      },
      "source": [
        "nlu.load('en.classify.sarcasm').predict('gotta love the teachers who give examns on the day after halloween') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_sarcasm download started this may take some time.\n",
            "Approximate size to download 21.5 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_normal</th>\n",
              "      <th>category_sarcasm</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.03146284446120262, 0.04071342945098877, 0....</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5087321E-5</td>\n",
              "      <td>0.99998486</td>\n",
              "      <td>gotta love the teachers who give examns on the...</td>\n",
              "      <td>sarcasm</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.03146284446120262, 0.04071342945098877, 0....  ...  8589934592\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28Jq2VfGRGe",
        "colab_type": "text"
      },
      "source": [
        "# Classify Cyberbullying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6embGXF5b4j5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "778a2581-bf3b-4885-8517-bd37807720f9"
      },
      "source": [
        "nlu.load('en.classify.cyberbullying').predict('Women belong in the kitchen.') # Sorry we dont mean it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_cyberbullying download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_sexism</th>\n",
              "      <th>category_neutral</th>\n",
              "      <th>category_racism</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.054944973438978195, -0.022223370149731636,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>1.87933E-6</td>\n",
              "      <td>7.737535E-8</td>\n",
              "      <td>Women belong in the kitchen.</td>\n",
              "      <td>sexism</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.054944973438978195, -0.022223370149731636,...  ...  8589934592\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXe9M0wYGO4f",
        "colab_type": "text"
      },
      "source": [
        "# Classify Fakenews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obJtaic1b4c4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "bfc354e8-5d0f-4768-e55a-34f7bb40ba7e"
      },
      "source": [
        "nlu.load('en.classify.fakenews').predict('Unicorns have been sighted on Mars!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_fakenews download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_REAL</th>\n",
              "      <th>category_FAKE</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.01756167598068714, 0.015006818808615208, -...</td>\n",
              "      <td>0</td>\n",
              "      <td>3.1013436E-16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Unicorns have been sighted on Mars!</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.01756167598068714, 0.015006818808615208, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op-HwFFUGNkT",
        "colab_type": "text"
      },
      "source": [
        "# Classify Spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2q6eYStVjq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "da5a7173-fd4a-4652-e725-227f6558afef"
      },
      "source": [
        "nlu.load('en.classify.spam').predict('Please sign up for this FREE membership it costs $$NO MONEY$$ just your mobile number!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_spam download started this may take some time.\n",
            "Approximate size to download 21.5 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_spam</th>\n",
              "      <th>category_ham</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.008322705514729023, 0.009957313537597656, 0...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.211698E-11</td>\n",
              "      <td>Please sign up for this FREE membership it cos...</td>\n",
              "      <td>spam</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.008322705514729023, 0.009957313537597656, 0...  ...  8589934592\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzWc9H1MGLgJ",
        "colab_type": "text"
      },
      "source": [
        "# Classify 6 Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEWhlCFScET0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "bae96022-5392-4ea5-f176-c2bbd98f1b3f"
      },
      "source": [
        "nlu.load('en.classify.trec6').predict('Where is the next food store?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_trec6 download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_ ABBR</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_ DESC</th>\n",
              "      <th>category_ NUM</th>\n",
              "      <th>category_ ENTY</th>\n",
              "      <th>category_ LOC</th>\n",
              "      <th>category_ HUM</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.05699703469872475, 0.039651867002248764, -...</td>\n",
              "      <td>2.2486939E-13</td>\n",
              "      <td>0</td>\n",
              "      <td>1.2741682E-9</td>\n",
              "      <td>2.3458482E-9</td>\n",
              "      <td>1.6385917E-8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.901978E-9</td>\n",
              "      <td>Where is the next food store?</td>\n",
              "      <td>LOC</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.05699703469872475, 0.039651867002248764, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYPEuOPlGIdG",
        "colab_type": "text"
      },
      "source": [
        "# Classify 50 Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXZEaTlcECH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "5b15e13f-a620-4b21-dbec-e14ac30b2a37"
      },
      "source": [
        "nlu.load('en.classify.trec50').predict('How expensive is the Watch?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_trec50 download started this may take some time.\n",
            "Approximate size to download 21.2 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_ ENTY_letter</th>\n",
              "      <th>category_ DESC_reason</th>\n",
              "      <th>category_ ENTY_word</th>\n",
              "      <th>category_ LOC_country</th>\n",
              "      <th>category_ ENTY_other</th>\n",
              "      <th>category_ ENTY_instru</th>\n",
              "      <th>category_ HUM_desc</th>\n",
              "      <th>category_ LOC_mount</th>\n",
              "      <th>category_ NUM_dist</th>\n",
              "      <th>category_ ENTY_symbol</th>\n",
              "      <th>category_ DESC_manner</th>\n",
              "      <th>category_ ABBR_abb</th>\n",
              "      <th>category_ NUM_count</th>\n",
              "      <th>category_ DESC_def</th>\n",
              "      <th>category_ ENTY_color</th>\n",
              "      <th>category_sentence</th>\n",
              "      <th>category_ ENTY_event</th>\n",
              "      <th>category_ ENTY_veh</th>\n",
              "      <th>category_ NUM_ord</th>\n",
              "      <th>category_ NUM_other</th>\n",
              "      <th>category_ NUM_code</th>\n",
              "      <th>category_ DESC_desc</th>\n",
              "      <th>category_ HUM_title</th>\n",
              "      <th>category_ NUM_weight</th>\n",
              "      <th>category_ ENTY_substance</th>\n",
              "      <th>category_ HUM_gr</th>\n",
              "      <th>category_ ENTY_body</th>\n",
              "      <th>category_ HUM_ind</th>\n",
              "      <th>category_ LOC_other</th>\n",
              "      <th>category_ NUM_speed</th>\n",
              "      <th>category_ NUM_volsize</th>\n",
              "      <th>category_ ENTY_religion</th>\n",
              "      <th>category_ ENTY_dismed</th>\n",
              "      <th>category_ ABBR_exp</th>\n",
              "      <th>category_ ENTY_product</th>\n",
              "      <th>category_ ENTY_currency</th>\n",
              "      <th>category_ ENTY_sport</th>\n",
              "      <th>category_ NUM_perc</th>\n",
              "      <th>category_ ENTY_cremat</th>\n",
              "      <th>category_ ENTY_plant</th>\n",
              "      <th>category_ NUM_money</th>\n",
              "      <th>category_ NUM_temp</th>\n",
              "      <th>category_ ENTY_animal</th>\n",
              "      <th>category_ NUM_period</th>\n",
              "      <th>category_ ENTY_lang</th>\n",
              "      <th>category_ NUM_date</th>\n",
              "      <th>category_ ENTY_techmeth</th>\n",
              "      <th>category_ ENTY_termeq</th>\n",
              "      <th>category_ LOC_state</th>\n",
              "      <th>category_ LOC_city</th>\n",
              "      <th>category_ ENTY_food</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.051809534430503845, 0.03128402680158615, -0...</td>\n",
              "      <td>8.459624E-23</td>\n",
              "      <td>3.4999828E-14</td>\n",
              "      <td>3.5003475E-23</td>\n",
              "      <td>5.841585E-20</td>\n",
              "      <td>4.958728E-9</td>\n",
              "      <td>1.2962921E-22</td>\n",
              "      <td>3.265098E-23</td>\n",
              "      <td>2.7989845E-22</td>\n",
              "      <td>1.1879483E-22</td>\n",
              "      <td>3.3417754E-23</td>\n",
              "      <td>1.1275173E-10</td>\n",
              "      <td>2.0653923E-23</td>\n",
              "      <td>0.91943645</td>\n",
              "      <td>1.1442368E-20</td>\n",
              "      <td>6.967735E-21</td>\n",
              "      <td>0</td>\n",
              "      <td>6.008502E-24</td>\n",
              "      <td>4.201739E-23</td>\n",
              "      <td>1.9875016E-25</td>\n",
              "      <td>7.991396E-25</td>\n",
              "      <td>5.3842046E-22</td>\n",
              "      <td>3.6065938E-8</td>\n",
              "      <td>9.143437E-24</td>\n",
              "      <td>5.219503E-23</td>\n",
              "      <td>4.7433556E-23</td>\n",
              "      <td>4.337756E-19</td>\n",
              "      <td>2.7589757E-23</td>\n",
              "      <td>5.1413796E-18</td>\n",
              "      <td>7.611624E-14</td>\n",
              "      <td>4.302599E-21</td>\n",
              "      <td>4.125377E-23</td>\n",
              "      <td>5.2841376E-21</td>\n",
              "      <td>1.519717E-21</td>\n",
              "      <td>7.546624E-23</td>\n",
              "      <td>8.906359E-22</td>\n",
              "      <td>2.617869E-25</td>\n",
              "      <td>1.8343639E-24</td>\n",
              "      <td>3.1659472E-22</td>\n",
              "      <td>2.537563E-21</td>\n",
              "      <td>4.2162198E-24</td>\n",
              "      <td>2.3034208E-20</td>\n",
              "      <td>1.740357E-22</td>\n",
              "      <td>4.4444944E-23</td>\n",
              "      <td>7.8172316E-20</td>\n",
              "      <td>8.119662E-24</td>\n",
              "      <td>0.0805635</td>\n",
              "      <td>4.5887844E-21</td>\n",
              "      <td>3.4650898E-22</td>\n",
              "      <td>2.3069434E-23</td>\n",
              "      <td>4.631075E-22</td>\n",
              "      <td>1.0575064E-22</td>\n",
              "      <td>How expensive is the Watch?</td>\n",
              "      <td>NUM_count</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.051809534430503845, 0.03128402680158615, -0...  ...  8589934592\n",
              "\n",
              "[1 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0SLrkKCHk2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJDMpFLf9Pq",
        "colab_type": "text"
      },
      "source": [
        "# Output Level demonstration\n",
        "\n",
        "## Token level output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2tHz8XwgRCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8154468b-80da-4be4-9c7a-9ae5a232c1fb"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun! It can also be quite helpful to people.', 'I love the city New-York'], output_level='token', output_positions=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checked</th>\n",
              "      <th>checked_begin</th>\n",
              "      <th>checked_end</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "      <th>document_begin</th>\n",
              "      <th>document_end</th>\n",
              "      <th>sentence_begin</th>\n",
              "      <th>sentence_end</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_begin</th>\n",
              "      <th>sentiment_end</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>love</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>data</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>science</td>\n",
              "      <td>12</td>\n",
              "      <td>18</td>\n",
              "      <td>science</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>It</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>is</td>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>so</td>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>much</td>\n",
              "      <td>30</td>\n",
              "      <td>33</td>\n",
              "      <td>much</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>fun</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>fun</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>!</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>It</td>\n",
              "      <td>40</td>\n",
              "      <td>41</td>\n",
              "      <td>It</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>can</td>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>can</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>also</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "      <td>also</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>be</td>\n",
              "      <td>52</td>\n",
              "      <td>53</td>\n",
              "      <td>be</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>quite</td>\n",
              "      <td>55</td>\n",
              "      <td>59</td>\n",
              "      <td>quite</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>helpful</td>\n",
              "      <td>61</td>\n",
              "      <td>67</td>\n",
              "      <td>helpful</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>to</td>\n",
              "      <td>69</td>\n",
              "      <td>70</td>\n",
              "      <td>to</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>people</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "      <td>people</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>78</td>\n",
              "      <td>78</td>\n",
              "      <td>.</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[0, 21, 40]</td>\n",
              "      <td>[19, 38, 78]</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>love</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>love</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>the</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>city</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>city</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>New-York</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>New-York</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[positive]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     checked  checked_begin  ...  sentiment_end                       sentiment\n",
              "0          I              0  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "1       love              2  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "2       data              7  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "3    science             12  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "4          !             19  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "5         It             21  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "6         is             24  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "7         so             27  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "8       much             30  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "9        fun             35  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "10         !             38  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "11        It             40  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "12       can             43  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "13      also             47  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "14        be             52  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "15     quite             55  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "16   helpful             61  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "17        to             69  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "18    people             72  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "19         .             78  ...   [19, 38, 78]  [positive, positive, positive]\n",
              "20         I              0  ...           [23]                      [positive]\n",
              "21      love              2  ...           [23]                      [positive]\n",
              "22       the              7  ...           [23]                      [positive]\n",
              "23      city             11  ...           [23]                      [positive]\n",
              "24  New-York             16  ...           [23]                      [positive]\n",
              "\n",
              "[25 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4i6OdQRgB-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "c1ac3ffc-4116-4be6-c0b1-4185e71d8d82"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun! It can also be quite helpful to people.', 'I love the city New-York'], output_level='chunk')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d8159fc8dc1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I love data science! It is so much fun! It can also be quite helpful to people.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I love the city New-York'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'chunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nlu' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ASEw1GmevS5",
        "colab_type": "text"
      },
      "source": [
        "## Sentence level output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJdMRydRgYYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "10687045-12b1-44e5-981e-4803fc49794e"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun! It can also be quite helpful to people.', 'I love the city New-York'], output_level='sentence', output_positions=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_begin</th>\n",
              "      <th>sentiment_end</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>document_begin</th>\n",
              "      <th>document_end</th>\n",
              "      <th>token_begin</th>\n",
              "      <th>token_end</th>\n",
              "      <th>checked_begin</th>\n",
              "      <th>checked_end</th>\n",
              "      <th>checked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7540</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>I love data science!</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.6121</td>\n",
              "      <td>21</td>\n",
              "      <td>38</td>\n",
              "      <td>It is so much fun!</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.4895</td>\n",
              "      <td>40</td>\n",
              "      <td>78</td>\n",
              "      <td>It can also be quite helpful to people.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[78]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38, 40, ...</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38, 41,...</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.7342</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>I love the city New-York</td>\n",
              "      <td>positive</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0, 2, 7, 11, 16]</td>\n",
              "      <td>[0, 5, 9, 14, 23]</td>\n",
              "      <td>[0, 2, 7, 11, 16]</td>\n",
              "      <td>[0, 5, 9, 14, 23]</td>\n",
              "      <td>[I, love, the, city, New-York]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment_confidence  ...                                            checked\n",
              "0               0.7540  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "1               0.6121  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "2               0.4895  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "3               0.7342  ...                     [I, love, the, city, New-York]\n",
              "\n",
              "[4 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsg-tkBHexvV",
        "colab_type": "text"
      },
      "source": [
        "## Document level output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbFG5qD1gZSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "41d36494-94d9-4535-b1a4-5c377d6bfec7"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun! It can also be quite helpful to people.', 'I love the city New-York'], output_level='document',)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>checked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love data science! It is so much fun! It can...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.7540000081062317, 0.6121000051498413, 0.489...</td>\n",
              "      <td>[positive, positive, positive]</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love the city New-York</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0.7342000007629395]</td>\n",
              "      <td>[positive]</td>\n",
              "      <td>[I, love, the, city, New-York]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  ...                                            checked\n",
              "0  I love data science! It is so much fun! It can...  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "1                           I love the city New-York  ...                     [I, love, the, city, New-York]\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzKLswYJezjp",
        "colab_type": "text"
      },
      "source": [
        "## Chunk level output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPbzhIdYcISU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Work in progress"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}