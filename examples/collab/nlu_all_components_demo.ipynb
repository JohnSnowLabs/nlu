{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU - Main Components Demo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn4Wv5ylHeEf",
        "colab_type": "text"
      },
      "source": [
        "# Install NLU\n",
        "We install Java 8 into our collab machine and then NLU and we are good to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij3ai9X4CmZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "68ed241a-80a6-4926-ce85-60ef40c0ca49"
      },
      "source": [
        "# Setup Java and NLU\n",
        "import os\n",
        "! apt-get update -qq > /dev/null   \n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "! pip install nlu qq > /dev/null   \n",
        "\n",
        "import nlu "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqUqu9DUTVqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faf534e8-efcf-43f0-fccd-f90e8112960f"
      },
      "source": [
        "# any string inside of <> can be apssed to nlu.load()\n",
        "nlu.print_components()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default NLU reference : < lang > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < lang.7 > of type : pipe  points to :  detect_language_7 Points to Spark NLP reference :  ('detect_language_7', 'pipe')\n",
            "Default NLU reference : < lang.20 > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang.20 > of type : pipe  points to :  detect_language_20 Points to Spark NLP reference :  ('detect_language_20', 'pipe')\n",
            "Default NLU reference : < classify.lang.7 > of type : pipe  points to :  detect_language_7 Points to Spark NLP reference :  ('detect_language_7', 'pipe')\n",
            "Default NLU reference : < classify > of type : pipe  points to :  analyze_sentiment Points to Spark NLP reference :  ('analyze_sentiment', 'pipe')\n",
            "Default NLU reference : < explain > of type : pipe  points to :  explain_document_ml Points to Spark NLP reference :  ('explain_document_ml', 'pipe')\n",
            "Default NLU reference : < explain.ml > of type : pipe  points to :  explain_document_ml Points to Spark NLP reference :  ('explain_document_ml', 'pipe')\n",
            "Default NLU reference : < explain.dl > of type : pipe  points to :  explain_document_dl Points to Spark NLP reference :  ('explain_document_dl', 'pipe')\n",
            "Default NLU reference : < ner > of type : pipe  points to :  recognize_entities_dl Points to Spark NLP reference :  ('recognize_entities_dl', 'pipe')\n",
            "Default NLU reference : < ner.dl > of type : pipe  points to :  recognize_entities_dl Points to Spark NLP reference :  ('recognize_entities_dl', 'pipe')\n",
            "Default NLU reference : < ner.bert > of type : pipe  points to :  recognize_entities_bert Points to Spark NLP reference :  ('recognize_entities_bert', 'pipe')\n",
            "Default NLU reference : < ner.onto > of type : pipe  points to :  onto_recognize_entities_sm Points to Spark NLP reference :  ('onto_recognize_entities_sm', 'pipe')\n",
            "Default NLU reference : < ner.onto.sm > of type : pipe  points to :  onto_recognize_entities_sm Points to Spark NLP reference :  ('onto_recognize_entities_sm', 'pipe')\n",
            "Default NLU reference : < ner.onto.lg > of type : pipe  points to :  onto_recognize_entities_lg Points to Spark NLP reference :  ('onto_recognize_entities_lg', 'pipe')\n",
            "Default NLU reference : < match.datetime > of type : pipe  points to :  match_datetime Points to Spark NLP reference :  ('match_datetime', 'pipe')\n",
            "Default NLU reference : < match.text > of type : model  points to :  text_matcher Points to Spark NLP reference :  ('text_matcher', 'model')\n",
            "Default NLU reference : < match.regex > of type : model  points to :  regex_matcher Points to Spark NLP reference :  ('regex_matcher', 'model')\n",
            "Default NLU reference : < match.pattern > of type : pipe  points to :  match_pattern Points to Spark NLP reference :  ('match_pattern', 'pipe')\n",
            "Default NLU reference : < match.chunks > of type : pipe  points to :  match_chunks Points to Spark NLP reference :  ('match_chunks', 'pipe')\n",
            "Default NLU reference : < match.phrases > of type : pipe  points to :  match_phrases Points to Spark NLP reference :  ('match_phrases', 'pipe')\n",
            "Default NLU reference : < clean.stop > of type : pipe  points to :  clean_stop Points to Spark NLP reference :  ('clean_stop', 'pipe')\n",
            "Default NLU reference : < clean.pattern > of type : pipe  points to :  clean_pattern Points to Spark NLP reference :  ('clean_pattern', 'pipe')\n",
            "Default NLU reference : < clean.slang > of type : pipe  points to :  clean_slang Points to Spark NLP reference :  ('clean_slang', 'pipe')\n",
            "Default NLU reference : < spell > of type : pipe  points to :  check_spelling_dl Points to Spark NLP reference :  ('check_spelling_dl', 'pipe')\n",
            "Default NLU reference : < sentiment > of type : pipe  points to :  analyze_sentiment Points to Spark NLP reference :  ('analyze_sentiment', 'pipe')\n",
            "Default NLU reference : < emotion > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < sentiment.imdb > of type : pipe  points to :  analyze_sentimentdl_use_imdb Points to Spark NLP reference :  ('analyze_sentimentdl_use_imdb', 'pipe')\n",
            "Default NLU reference : < sentiment.imdb.use > of type : pipe  points to :  analyze_sentimentdl_use_imdb Points to Spark NLP reference :  ('analyze_sentimentdl_use_imdb', 'pipe')\n",
            "Default NLU reference : < sentiment.twitter.use > of type : pipe  points to :  analyze_sentimentdl_use_twitter Points to Spark NLP reference :  ('analyze_sentimentdl_use_twitter', 'pipe')\n",
            "Default NLU reference : < sentiment.twitter > of type : pipe  points to :  analyze_sentimentdl_use_twitter Points to Spark NLP reference :  ('analyze_sentimentdl_use_twitter', 'pipe')\n",
            "Default NLU reference : < tokenize > of type : model  points to :  spark_nlp_tokenizer Points to Spark NLP reference :  ('spark_nlp_tokenizer', 'model')\n",
            "Default NLU reference : < stem > of type : model  points to :  stemmer Points to Spark NLP reference :  ('stemmer', 'model')\n",
            "Default NLU reference : < norm > of type : model  points to :  normalizer Points to Spark NLP reference :  ('normalizer', 'model')\n",
            "Default NLU reference : < chunk > of type : model  points to :  default_chunker Points to Spark NLP reference :  ('default_chunker', 'model')\n",
            "Default NLU reference : < embed_chunk > of type : model  points to :  chunk_embeddings Points to Spark NLP reference :  ('chunk_embeddings', 'model')\n",
            "Default NLU reference : < ngram > of type : model  points to :  ngram Points to Spark NLP reference :  ('ngram', 'model')\n",
            "Default NLU reference : < lemma > of type : model  points to :  lemma_antbnc Points to Spark NLP reference :  ('lemma_antbnc', 'model')\n",
            "Default NLU reference : < lemma.antbnc > of type : model  points to :  lemma_antbnc Points to Spark NLP reference :  ('lemma_antbnc', 'model')\n",
            "Default NLU reference : < pos > of type : model  points to :  pos_anc Points to Spark NLP reference :  ('pos_anc', 'model')\n",
            "Default NLU reference : < pos.anc > of type : model  points to :  pos_anc Points to Spark NLP reference :  ('pos_anc', 'model')\n",
            "Default NLU reference : < pos.ud_ewt > of type : model  points to :  pos_ud_ewt Points to Spark NLP reference :  ('pos_ud_ewt', 'model')\n",
            "Default NLU reference : < ner.dl.glove_6B_100d > of type : model  points to :  ner_dl Points to Spark NLP reference :  ('ner_dl', 'model')\n",
            "Default NLU reference : < ner.dl.bert > of type : model  points to :  ner_dl_bert Points to Spark NLP reference :  ('ner_dl_bert', 'model')\n",
            "Default NLU reference : < ner.onto.glove_6B_100d > of type : model  points to :  onto_100 Points to Spark NLP reference :  ('onto_100', 'model')\n",
            "Default NLU reference : < ner.onto.glove_6B_300d > of type : model  points to :  onto_300 Points to Spark NLP reference :  ('onto_300', 'model')\n",
            "Default NLU reference : < sentence_detector > of type : model  points to :  ner_dl_sentence Points to Spark NLP reference :  ('ner_dl_sentence', 'model')\n",
            "Default NLU reference : < sentence_detector.deep > of type : model  points to :  ner_dl_sentence Points to Spark NLP reference :  ('ner_dl_sentence', 'model')\n",
            "Default NLU reference : < spell.norivg > of type : model  points to :  spellcheck_norvig Points to Spark NLP reference :  ('spellcheck_norvig', 'model')\n",
            "Default NLU reference : < sentiment.vivekn > of type : model  points to :  sentiment_vivekn Points to Spark NLP reference :  ('sentiment_vivekn', 'model')\n",
            "Default NLU reference : < dep.untyped.conllu > of type : model  points to :  dependency_conllu Points to Spark NLP reference :  ('dependency_conllu', 'model')\n",
            "Default NLU reference : < dep.untyped > of type : model  points to :  dependency_conllu.untyped Points to Spark NLP reference :  ('dependency_conllu.untyped', 'model')\n",
            "Default NLU reference : < dep > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < dep.typed > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < dep.typed.conllu > of type : model  points to :  dependency_typed_conllu Points to Spark NLP reference :  ('dependency_typed_conllu', 'model')\n",
            "Default NLU reference : < stopwords > of type : model  points to :  stopwords_en Points to Spark NLP reference :  ('stopwords_en', 'model')\n",
            "Default NLU reference : < embed > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < glove > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < embed.glove > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < embed.glove_100d > of type : model  points to :  glove_100d Points to Spark NLP reference :  ('glove_100d', 'model')\n",
            "Default NLU reference : < bert > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert_base_uncased > of type : model  points to :  bert_base_uncased Points to Spark NLP reference :  ('bert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.bert_base_cased > of type : model  points to :  bert_base_cased Points to Spark NLP reference :  ('bert_base_cased', 'model')\n",
            "Default NLU reference : < biobert > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_pubmed_base_cased > of type : model  points to :  biobert_pubmed_base_cased Points to Spark NLP reference :  ('biobert_pubmed_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_pubmed_pmc_base_cased > of type : model  points to :  biobert_pubmed_pmc_base_cased Points to Spark NLP reference :  ('biobert_pubmed_pmc_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_clinical_base_cased > of type : model  points to :  biobert_clinical_base_cased Points to Spark NLP reference :  ('biobert_clinical_base_cased', 'model')\n",
            "Default NLU reference : < embed.biobert_discharge_base_cased > of type : model  points to :  biobert_discharge_base_cased Points to Spark NLP reference :  ('biobert_discharge_base_cased', 'model')\n",
            "Default NLU reference : < elmo > of type : model  points to :  elmo Points to Spark NLP reference :  ('elmo', 'model')\n",
            "Default NLU reference : < embed.elmo > of type : model  points to :  elmo Points to Spark NLP reference :  ('elmo', 'model')\n",
            "Default NLU reference : < embed_sentence > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.tfhub_use > of type : model  points to :  tfhub_use Points to Spark NLP reference :  ('tfhub_use', 'model')\n",
            "Default NLU reference : < embed_sentence.use_lg > of type : model  points to :  tfhub_use_lg Points to Spark NLP reference :  ('tfhub_use_lg', 'model')\n",
            "Default NLU reference : < embed_sentence.tfhub_use_lg > of type : model  points to :  tfhub_use_lg Points to Spark NLP reference :  ('tfhub_use_lg', 'model')\n",
            "Default NLU reference : < albert > of type : model  points to :  albert_base_uncased Points to Spark NLP reference :  ('albert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.albert_base_uncased > of type : model  points to :  albert_base_uncased Points to Spark NLP reference :  ('albert_base_uncased', 'model')\n",
            "Default NLU reference : < embed.xlnet > of type : model  points to :  xlnet_base_cased Points to Spark NLP reference :  ('xlnet_base_cased', 'model')\n",
            "Default NLU reference : < xlnet > of type : model  points to :  xlnet_base_cased Points to Spark NLP reference :  ('xlnet_base_cased', 'model')\n",
            "Default NLU reference : < classify.trec6.use > of type : model  points to :  classifierdl_use_trec6 Points to Spark NLP reference :  ('classifierdl_use_trec6', 'model')\n",
            "Default NLU reference : < classify.trec50.use > of type : model  points to :  classifierdl_use_trec50 Points to Spark NLP reference :  ('classifierdl_use_trec50', 'model')\n",
            "Default NLU reference : < classify.spam.use > of type : model  points to :  classifierdl_use_spam Points to Spark NLP reference :  ('classifierdl_use_spam', 'model')\n",
            "Default NLU reference : < classify.fakenews.use > of type : model  points to :  classifierdl_use_fakenews Points to Spark NLP reference :  ('classifierdl_use_fakenews', 'model')\n",
            "Default NLU reference : < classify.emotion.use > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < classify.cyberbullying.use > of type : model  points to :  classifierdl_use_cyberbullying Points to Spark NLP reference :  ('classifierdl_use_cyberbullying', 'model')\n",
            "Default NLU reference : < classify.sarcasm.use > of type : model  points to :  classifierdl_use_sarcasm Points to Spark NLP reference :  ('classifierdl_use_sarcasm', 'model')\n",
            "Default NLU reference : < sentiment.imdb.glove > of type : model  points to :  sentimentdl_glove_imdb Points to Spark NLP reference :  ('sentimentdl_glove_imdb', 'model')\n",
            "Default NLU reference : < classify.trec6 > of type : model  points to :  classifierdl_use_trec6 Points to Spark NLP reference :  ('classifierdl_use_trec6', 'model')\n",
            "Default NLU reference : < classify.trec50 > of type : model  points to :  classifierdl_use_trec50 Points to Spark NLP reference :  ('classifierdl_use_trec50', 'model')\n",
            "Default NLU reference : < classify.spam > of type : model  points to :  classifierdl_use_spam Points to Spark NLP reference :  ('classifierdl_use_spam', 'model')\n",
            "Default NLU reference : < classify.fakenews > of type : model  points to :  classifierdl_use_fakenews Points to Spark NLP reference :  ('classifierdl_use_fakenews', 'model')\n",
            "Default NLU reference : < classify.emotion > of type : model  points to :  classifierdl_use_emotion Points to Spark NLP reference :  ('classifierdl_use_emotion', 'model')\n",
            "Default NLU reference : < classify.cyberbullying > of type : model  points to :  classifierdl_use_cyberbullying Points to Spark NLP reference :  ('classifierdl_use_cyberbullying', 'model')\n",
            "Default NLU reference : < classify.sarcasm > of type : model  points to :  classifierdl_use_sarcasm Points to Spark NLP reference :  ('classifierdl_use_sarcasm', 'model')\n",
            "Default NLU reference : < embed.glove_840B_300 > of type : model  points to :  glove_840B_300 Points to Spark NLP reference :  ('glove_840B_300', 'model')\n",
            "Default NLU reference : < embed.glove_6B_300 > of type : model  points to :  glove_6B_300 Points to Spark NLP reference :  ('glove_6B_300', 'model')\n",
            "Default NLU reference : < embed.bert_multi_cased > of type : model  points to :  bert_multi_cased Points to Spark NLP reference :  ('bert_multi_cased', 'model')\n",
            "Default NLU reference : < classify.wiki_7 > of type : model  points to :  ld_wiki_7 Points to Spark NLP reference :  ('ld_wiki_7', 'model')\n",
            "Default NLU reference : < classify.wiki_20 > of type : model  points to :  ld_wiki_20 Points to Spark NLP reference :  ('ld_wiki_20', 'model')\n",
            "NLU reference < nl.explain > for lang nl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < nl.explain.sm > for lang nl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < nl.explain.md > for lang nl  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < nl.explain.lg > for lang nl  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < nl.ner > for lang nl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < nl.ner.sm > for lang nl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < nl.ner.md > for lang nl  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < nl.ner.lg > for lang nl  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < en.classify > for lang en  points to SPARK NLP model : analyze_sentiment\n",
            "NLU reference < en.explain > for lang en  points to SPARK NLP model : explain_document_ml\n",
            "NLU reference < en.explain.ml > for lang en  points to SPARK NLP model : explain_document_ml\n",
            "NLU reference < en.explain.dl > for lang en  points to SPARK NLP model : explain_document_dl\n",
            "NLU reference < en.ner > for lang en  points to SPARK NLP model : recognize_entities_dl\n",
            "NLU reference < en.ner.dl > for lang en  points to SPARK NLP model : recognize_entities_dl\n",
            "NLU reference < en.ner.bert > for lang en  points to SPARK NLP model : recognize_entities_bert\n",
            "NLU reference < en.ner.onto > for lang en  points to SPARK NLP model : onto_recognize_entities_sm\n",
            "NLU reference < en.ner.onto.sm > for lang en  points to SPARK NLP model : onto_recognize_entities_sm\n",
            "NLU reference < en.ner.onto.lg > for lang en  points to SPARK NLP model : onto_recognize_entities_lg\n",
            "NLU reference < en.match.datetime > for lang en  points to SPARK NLP model : match_datetime\n",
            "NLU reference < en.match.pattern > for lang en  points to SPARK NLP model : match_pattern\n",
            "NLU reference < en.match.chunks > for lang en  points to SPARK NLP model : match_chunks\n",
            "NLU reference < en.match.phrases > for lang en  points to SPARK NLP model : match_phrases\n",
            "NLU reference < en.clean.stop > for lang en  points to SPARK NLP model : clean_stop\n",
            "NLU reference < en.clean.pattern > for lang en  points to SPARK NLP model : clean_pattern\n",
            "NLU reference < en.clean.slang > for lang en  points to SPARK NLP model : clean_slang\n",
            "NLU reference < en.spell > for lang en  points to SPARK NLP model : check_spelling_dl\n",
            "NLU reference < en.spell.dl > for lang en  points to SPARK NLP model : check_spelling_dl\n",
            "NLU reference < en.spell.context > for lang en  points to SPARK NLP model : check_spelling_dl\n",
            "NLU reference < en.sentiment > for lang en  points to SPARK NLP model : analyze_sentiment\n",
            "NLU reference < en.sentiment.imdb > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.imdb.use > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.twitter.use > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.twitter > for lang en  points to SPARK NLP model : analyze_sentimentdl_use_twitter\n",
            "NLU reference < fr.explain > for lang fr  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < fr.explain.lg > for lang fr  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < fr.explain.md > for lang fr  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < fr.ner > for lang fr  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < fr.ner.lg > for lang fr  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < fr.ner.md > for lang fr  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.explain.document > for lang de  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < de.explain.document.md > for lang de  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < de.explain.document.lg > for lang de  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < de.ner.recognizer > for lang de  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.ner.recognizer.md > for lang de  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < de.ner.recognizer.lg > for lang de  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < it.explain.document > for lang it  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < it.explain.document.md > for lang it  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < it.explain.document.lg > for lang it  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < it.ner > for lang it  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < it.ner.md > for lang it  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < it.ner.lg > for lang it  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < no.explain > for lang no  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < no.explain.sm > for lang no  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < no.explain.md > for lang no  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < no.explain.lg > for lang no  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < no.ner > for lang no  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < no.ner.sm > for lang no  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < no.ner.md > for lang no  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < no.ner.lg > for lang no  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < pl.explain > for lang pl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pl.explain.sm > for lang pl  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pl.explain.md > for lang pl  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < pl.explain.lg > for lang pl  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < pl.ner > for lang pl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pl.ner.sm > for lang pl  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pl.ner.md > for lang pl  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < pl.ner.lg > for lang pl  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < pt.explain > for lang pt  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pt.explain.sm > for lang pt  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < pt.explain.md > for lang pt  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < pt.explain.lg > for lang pt  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < pt.ner > for lang pt  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pt.ner.sm > for lang pt  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < pt.ner.md > for lang pt  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < pt.ner.lg > for lang pt  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < ru.explain > for lang ru  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < ru.explain.sm > for lang ru  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < ru.explain.md > for lang ru  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < ru.explain.lg > for lang ru  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < ru.ner > for lang ru  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < ru.ner.sm > for lang ru  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < ru.ner.md > for lang ru  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < ru.ner.lg > for lang ru  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < es.explain > for lang es  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < es.explain.sm > for lang es  points to SPARK NLP model : explain_document_sm\n",
            "NLU reference < es.explain.md > for lang es  points to SPARK NLP model : explain_document_md\n",
            "NLU reference < es.explain.lg > for lang es  points to SPARK NLP model : explain_document_lg\n",
            "NLU reference < es.ner > for lang es  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < es.ner.sm > for lang es  points to SPARK NLP model : entity_recognizer_sm\n",
            "NLU reference < es.ner.md > for lang es  points to SPARK NLP model : entity_recognizer_md\n",
            "NLU reference < es.ner.lg > for lang es  points to SPARK NLP model : entity_recognizer_lg\n",
            "NLU reference < lang > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < lang.7 > for lang xx  points to SPARK NLP model : detect_language_7\n",
            "NLU reference < lang.20 > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang.20 > for lang xx  points to SPARK NLP model : detect_language_20\n",
            "NLU reference < xx.classify.lang.7 > for lang xx  points to SPARK NLP model : detect_language_7\n",
            "NLU reference < nl.lemma > for lang nl  points to SPARK NLP model : lemma\n",
            "NLU reference < nl.pos > for lang nl  points to SPARK NLP model : pos_ud_alpino\n",
            "NLU reference < nl.pos.ud_alpino > for lang nl  points to SPARK NLP model : pos_ud_alpino\n",
            "NLU reference < nl.ner > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner.glove_6B_100 > for lang nl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < nl.ner.wikiner.glove_6B_300 > for lang nl  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < nl.ner.wikiner.glove_840B_300 > for lang nl  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < en.stem > for lang en  points to SPARK NLP model : stemmer\n",
            "NLU reference < en.tokenize > for lang en  points to SPARK NLP model : spark_nlp_tokenizer\n",
            "NLU reference < en.norm > for lang en  points to SPARK NLP model : norm\n",
            "NLU reference < en.chunk > for lang en  points to SPARK NLP model : default_chunker\n",
            "NLU reference < en.ngram > for lang en  points to SPARK NLP model : ngram\n",
            "NLU reference < en.embed_chunk > for lang en  points to SPARK NLP model : chunk_embeddings\n",
            "NLU reference < en.lemma > for lang en  points to SPARK NLP model : lemma_antbnc\n",
            "NLU reference < en.lemma.antbnc > for lang en  points to SPARK NLP model : lemma_antbnc\n",
            "NLU reference < en.pos > for lang en  points to SPARK NLP model : pos_anc\n",
            "NLU reference < en.pos.anc > for lang en  points to SPARK NLP model : pos_anc\n",
            "NLU reference < en.pos.ud_ewt > for lang en  points to SPARK NLP model : pos_ud_ewt\n",
            "NLU reference < en.ner > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl.glove_6B_100d > for lang en  points to SPARK NLP model : ner_dl\n",
            "NLU reference < en.ner.dl.bert > for lang en  points to SPARK NLP model : ner_dl_bert\n",
            "NLU reference < en.ner.onto > for lang en  points to SPARK NLP model : onto_100\n",
            "NLU reference < en.ner.onto.glove_6B_100d > for lang en  points to SPARK NLP model : onto_100\n",
            "NLU reference < en.ner.onto.glove_6B_300d > for lang en  points to SPARK NLP model : onto_300\n",
            "NLU reference < en.ner.glove_100d > for lang en  points to SPARK NLP model : ner_dl_sentence\n",
            "NLU reference < en.spell.norvig > for lang en  points to SPARK NLP model : spellcheck_norvig\n",
            "NLU reference < en.sentiment.vivekn > for lang en  points to SPARK NLP model : sentiment_vivekn\n",
            "NLU reference < en.dep.untyped.conllu > for lang en  points to SPARK NLP model : dependency_conllu\n",
            "NLU reference < en.dep.untyped > for lang en  points to SPARK NLP model : dependency_conllu\n",
            "NLU reference < en.stopwords > for lang en  points to SPARK NLP model : stopwords_en\n",
            "NLU reference < en.glove > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed.glove > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.embed.glove_100d > for lang en  points to SPARK NLP model : glove_100d\n",
            "NLU reference < en.bert > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert_base_uncased > for lang en  points to SPARK NLP model : bert_base_uncased\n",
            "NLU reference < en.embed.bert_base_cased > for lang en  points to SPARK NLP model : bert_base_cased\n",
            "NLU reference < biobert > for lang en  points to SPARK NLP model : biobert_pubmed_base_cased\n",
            "NLU reference < en.embed.biobert > for lang en  points to SPARK NLP model : biobert_pubmed_base_cased\n",
            "NLU reference < en.embed.biobert_pubmed_pmc_base_cased > for lang en  points to SPARK NLP model : biobert_pubmed_pmc_base_cased\n",
            "NLU reference < en.embed.biobert_clinical_base_cased > for lang en  points to SPARK NLP model : biobert_clinical_base_cased\n",
            "NLU reference < en.embed.biobert_discharge_base_cased > for lang en  points to SPARK NLP model : biobert_discharge_base_cased\n",
            "NLU reference < en.embed.elmo > for lang en  points to SPARK NLP model : elmo\n",
            "NLU reference < en.embed_sentence > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed.use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.tfhub_use > for lang en  points to SPARK NLP model : tfhub_use\n",
            "NLU reference < en.embed_sentence.use_lg > for lang en  points to SPARK NLP model : tfhub_use_lg\n",
            "NLU reference < en.embed_sentence.tfhub_use_lg > for lang en  points to SPARK NLP model : tfhub_use_lg\n",
            "NLU reference < en.embed_sentence.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.albert > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.albert_base_uncased > for lang en  points to SPARK NLP model : albert_base_uncased\n",
            "NLU reference < en.embed.xlnet > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.xlnet > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.embed.xlnet_base_cased > for lang en  points to SPARK NLP model : xlnet_base_cased\n",
            "NLU reference < en.classify.trec6.use > for lang en  points to SPARK NLP model : classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50.use > for lang en  points to SPARK NLP model : classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam.use > for lang en  points to SPARK NLP model : classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews.use > for lang en  points to SPARK NLP model : classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion.use > for lang en  points to SPARK NLP model : classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying.use > for lang en  points to SPARK NLP model : classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm.use > for lang en  points to SPARK NLP model : classifierdl_use_sarcasm\n",
            "NLU reference < en.sentiment.imdb.use > for lang en  points to SPARK NLP model : sentimentdl_use_imdb\n",
            "NLU reference < en.sentiment.twitter.use > for lang en  points to SPARK NLP model : sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.imdb.glove > for lang en  points to SPARK NLP model : sentimentdl_glove_imdb\n",
            "NLU reference < en.classify.trec6 > for lang en  points to SPARK NLP model : classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50 > for lang en  points to SPARK NLP model : classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam > for lang en  points to SPARK NLP model : classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews > for lang en  points to SPARK NLP model : classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion > for lang en  points to SPARK NLP model : classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying > for lang en  points to SPARK NLP model : classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm > for lang en  points to SPARK NLP model : classifierdl_use_sarcasm\n",
            "NLU reference < en.sentiment.twitter > for lang en  points to SPARK NLP model : sentimentdl_use_twitter\n",
            "NLU reference < en.sentiment.imdb > for lang en  points to SPARK NLP model : sentimentdl_glove_imdb\n",
            "NLU reference < fr.lemma > for lang fr  points to SPARK NLP model : lemma\n",
            "NLU reference < fr.pos > for lang fr  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < fr.pos.ud_gsd > for lang fr  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < fr.ner > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.ner.wikiner > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.ner.wikiner.glove_840B_300 > for lang fr  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < fr.stopwords > for lang fr  points to SPARK NLP model : stopwords_fr\n",
            "NLU reference < de.lemma > for lang de  points to SPARK NLP model : lemma\n",
            "NLU reference < de.pos.ud_hdt > for lang de  points to SPARK NLP model : pos_ud_hdt\n",
            "NLU reference < de.pos > for lang de  points to SPARK NLP model : pos_ud_hdt\n",
            "NLU reference < de.ner > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.ner.wikiner > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.ner.wikiner.glove_840B_300 > for lang de  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < de.stopwords > for lang de  points to SPARK NLP model : stopwords_de\n",
            "NLU reference < it.lemma > for lang it  points to SPARK NLP model : lemma_dxc\n",
            "NLU reference < it.lemma.dxc > for lang it  points to SPARK NLP model : lemma_dxc\n",
            "NLU reference < it.sentiment.dxc > for lang it  points to SPARK NLP model : sentiment_dxc\n",
            "NLU reference < it.sentiment > for lang it  points to SPARK NLP model : sentiment_dxc\n",
            "NLU reference < it.pos > for lang it  points to SPARK NLP model : pos_ud_isdt\n",
            "NLU reference < it.pos.ud_isdt > for lang it  points to SPARK NLP model : pos_ud_isdt\n",
            "NLU reference < it.ner > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.ner.wikiner > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.ner.wikiner.glove_840B_300 > for lang it  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < it.stopwords > for lang it  points to SPARK NLP model : stopwords_it\n",
            "NLU reference < nb.lemma > for lang nb  points to SPARK NLP model : lemma\n",
            "NLU reference < nb.pos.ud_bokmaal > for lang nb  points to SPARK NLP model : pos_ud_bokmaal\n",
            "NLU reference < no.ner > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne.glove_6B_100 > for lang no  points to SPARK NLP model : norne_6B_100\n",
            "NLU reference < no.ner.norne.glove_6B_300 > for lang no  points to SPARK NLP model : norne_6B_300\n",
            "NLU reference < no.ner.norne.glove_840B_300 > for lang no  points to SPARK NLP model : norne_840B_300\n",
            "NLU reference < nn.pos > for lang nn  points to SPARK NLP model : pos_ud_nynorsk\n",
            "NLU reference < nn.pos.ud_nynorsk > for lang nn  points to SPARK NLP model : pos_ud_nynorsk\n",
            "NLU reference < pl.lemma > for lang pl  points to SPARK NLP model : lemma\n",
            "NLU reference < pl.pos > for lang pl  points to SPARK NLP model : pos_ud_lfg\n",
            "NLU reference < pl.pos.ud_lfg > for lang pl  points to SPARK NLP model : pos_ud_lfg\n",
            "NLU reference < pl.ner > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner.glove_6B_100 > for lang pl  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pl.ner.wikiner.glove_6B_300 > for lang pl  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < pl.ner.wikiner.glove_840B_300 > for lang pl  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < pl.stopwords > for lang pl  points to SPARK NLP model : stopwords_pl\n",
            "NLU reference < pt.lemma > for lang pt  points to SPARK NLP model : lemma\n",
            "NLU reference < pt.pos.ud_bosque > for lang pt  points to SPARK NLP model : pos_ud_bosque\n",
            "NLU reference < pt.pos > for lang pt  points to SPARK NLP model : pos_ud_bosque\n",
            "NLU reference < pt.ner > for lang pt  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pt.ner.wikiner.glove_6B_100 > for lang pt  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < pt.ner.wikiner.glove_6B_300 > for lang pt  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < pt.ner.wikiner.glove_840B_300 > for lang pt  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < pt.stopwords > for lang pt  points to SPARK NLP model : stopwords_pt\n",
            "NLU reference < ru.lemma > for lang ru  points to SPARK NLP model : lemma\n",
            "NLU reference < ru.pos.ud_gsd > for lang ru  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ru.pos > for lang ru  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ru.ner > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner.glove_6B_100 > for lang ru  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < ru.ner.wikiner.glove_6B_300 > for lang ru  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < ru.ner.wikiner.glove_840B_300 > for lang ru  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < ru.stopwords > for lang ru  points to SPARK NLP model : stopwords_ru\n",
            "NLU reference < es.lemma > for lang es  points to SPARK NLP model : lemma\n",
            "NLU reference < es.pos > for lang es  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < es.pos.ud_gsd > for lang es  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < es.ner > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner.glove_6B_100 > for lang es  points to SPARK NLP model : wikiner_6B_100\n",
            "NLU reference < es.ner.wikiner.glove_6B_300 > for lang es  points to SPARK NLP model : wikiner_6B_300\n",
            "NLU reference < es.ner.wikiner.glove_840B_300 > for lang es  points to SPARK NLP model : wikiner_840B_300\n",
            "NLU reference < es.stopwords_es > for lang es  points to SPARK NLP model : stopwords_es\n",
            "NLU reference < af.stopwords > for lang af  points to SPARK NLP model : stopwords_af\n",
            "NLU reference < ar.stopwords_ar > for lang ar  points to SPARK NLP model : stopwords_ar\n",
            "NLU reference < hy.stopwords > for lang hy  points to SPARK NLP model : stopwords_hy\n",
            "NLU reference < hy.lemma > for lang hy  points to SPARK NLP model : lemma\n",
            "NLU reference < hy.pos > for lang hy  points to SPARK NLP model : pos_ud_armtdp\n",
            "NLU reference < eu.stopwords > for lang eu  points to SPARK NLP model : stopwords_eu\n",
            "NLU reference < eu.lemma > for lang eu  points to SPARK NLP model : lemma\n",
            "NLU reference < eu.pos > for lang eu  points to SPARK NLP model : pos_ud_bdt\n",
            "NLU reference < bn.stopwords > for lang bn  points to SPARK NLP model : stopwords_bn\n",
            "NLU reference < br.stopwords > for lang br  points to SPARK NLP model : stopwords_br\n",
            "NLU reference < br.lemma > for lang br  points to SPARK NLP model : lemma\n",
            "NLU reference < br.pos > for lang br  points to SPARK NLP model : pos_ud_keb\n",
            "NLU reference < bg.lemma > for lang bg  points to SPARK NLP model : lemma\n",
            "NLU reference < bg.pos > for lang bg  points to SPARK NLP model : pos_ud_btb\n",
            "NLU reference < bg.pos.ud_btb > for lang bg  points to SPARK NLP model : pos_ud_btb\n",
            "NLU reference < bg.stopwords > for lang bg  points to SPARK NLP model : stopwords_bg\n",
            "NLU reference < ca.stopwords > for lang ca  points to SPARK NLP model : stopwords_ca\n",
            "NLU reference < ca.lemma > for lang ca  points to SPARK NLP model : lemma\n",
            "NLU reference < ca.pos > for lang ca  points to SPARK NLP model : pos_ud_ancora\n",
            "NLU reference < cs.lemma > for lang cs  points to SPARK NLP model : lemma\n",
            "NLU reference < cs.pos > for lang cs  points to SPARK NLP model : pos_ud_pdt\n",
            "NLU reference < cs.pos.ud_pdt > for lang cs  points to SPARK NLP model : pos_ud_pdt\n",
            "NLU reference < cs.stopwords > for lang cs  points to SPARK NLP model : stopwords_cs\n",
            "NLU reference < eo.stopwords > for lang eo  points to SPARK NLP model : stopwords_eo\n",
            "NLU reference < fi.lemma > for lang fi  points to SPARK NLP model : lemma\n",
            "NLU reference < fi.pos.ud_tdt > for lang fi  points to SPARK NLP model : pos_ud_tdt\n",
            "NLU reference < fi.pos > for lang fi  points to SPARK NLP model : pos_ud_tdt\n",
            "NLU reference < fi.stopwords > for lang fi  points to SPARK NLP model : stopwords_fi\n",
            "NLU reference < gl.stopwords > for lang gl  points to SPARK NLP model : stopwords_gl\n",
            "NLU reference < gl.lemma > for lang gl  points to SPARK NLP model : lemma\n",
            "NLU reference < gl.pos > for lang gl  points to SPARK NLP model : pos_ud_treegal\n",
            "NLU reference < el.lemma > for lang el  points to SPARK NLP model : lemma\n",
            "NLU reference < el.pos > for lang el  points to SPARK NLP model : pos_ud_gdt\n",
            "NLU reference < el.pos.ud_gdt > for lang el  points to SPARK NLP model : pos_ud_gdt\n",
            "NLU reference < el.stopwords > for lang el  points to SPARK NLP model : stopwords_el\n",
            "NLU reference < ha.stopwords > for lang ha  points to SPARK NLP model : stopwords_ha\n",
            "NLU reference < he.stopwords > for lang he  points to SPARK NLP model : stopwords_he\n",
            "NLU reference < hi.stopwords > for lang hi  points to SPARK NLP model : stopwords_hi\n",
            "NLU reference < hi.lemma > for lang hi  points to SPARK NLP model : lemma\n",
            "NLU reference < hi.pos > for lang hi  points to SPARK NLP model : pos_ud_hdtb\n",
            "NLU reference < hu.lemma > for lang hu  points to SPARK NLP model : lemma\n",
            "NLU reference < hu.pos > for lang hu  points to SPARK NLP model : pos_ud_szeged\n",
            "NLU reference < hu.pos.ud_szeged > for lang hu  points to SPARK NLP model : pos_ud_szeged\n",
            "NLU reference < hu.stopwords > for lang hu  points to SPARK NLP model : stopwords_hu\n",
            "NLU reference < id.stopwords > for lang id  points to SPARK NLP model : stopwords_id\n",
            "NLU reference < id.lemma > for lang id  points to SPARK NLP model : lemma\n",
            "NLU reference < id.pos > for lang id  points to SPARK NLP model : pos_ud_gsd\n",
            "NLU reference < ga.stopwords > for lang ga  points to SPARK NLP model : stopwords_ga\n",
            "NLU reference < ga.lemma > for lang ga  points to SPARK NLP model : lemma\n",
            "NLU reference < ga.pos > for lang ga  points to SPARK NLP model : pos_ud_idt\n",
            "NLU reference < da.lemma > for lang da  points to SPARK NLP model : lemma\n",
            "NLU reference < da.pos > for lang da  points to SPARK NLP model : pos_ud_ddt\n",
            "NLU reference < ja.stopwords > for lang ja  points to SPARK NLP model : stopwords_ja\n",
            "NLU reference < la.stopwords > for lang la  points to SPARK NLP model : stopwords_la\n",
            "NLU reference < la.lemma > for lang la  points to SPARK NLP model : lemma\n",
            "NLU reference < la.pos > for lang la  points to SPARK NLP model : pos_ud_llct\n",
            "NLU reference < lv.stopwords > for lang lv  points to SPARK NLP model : stopwords_lv\n",
            "NLU reference < lv.lemma > for lang lv  points to SPARK NLP model : lemma\n",
            "NLU reference < lv.pos > for lang lv  points to SPARK NLP model : pos_ud_lvtb\n",
            "NLU reference < mr.stopwords > for lang mr  points to SPARK NLP model : stopwords_mr\n",
            "NLU reference < mr.lemma > for lang mr  points to SPARK NLP model : lemma\n",
            "NLU reference < mr.pos > for lang mr  points to SPARK NLP model : pos_ud_ufal\n",
            "NLU reference < fa.stopwords > for lang fa  points to SPARK NLP model : stopwords_fa\n",
            "NLU reference < ro.lemma > for lang ro  points to SPARK NLP model : lemma\n",
            "NLU reference < ro.pos > for lang ro  points to SPARK NLP model : pos_ud_rrt\n",
            "NLU reference < ro.pos.ud_rrt > for lang ro  points to SPARK NLP model : pos_ud_rrt\n",
            "NLU reference < ro.stopwords > for lang ro  points to SPARK NLP model : stopwords_ro\n",
            "NLU reference < sk.lemma > for lang sk  points to SPARK NLP model : lemma\n",
            "NLU reference < sk.pos > for lang sk  points to SPARK NLP model : pos_ud_snk\n",
            "NLU reference < sk.pos.ud_snk > for lang sk  points to SPARK NLP model : pos_ud_snk\n",
            "NLU reference < sk.stopwords > for lang sk  points to SPARK NLP model : stopwords_sk\n",
            "NLU reference < sl.stopwords > for lang sl  points to SPARK NLP model : stopwords_sl\n",
            "NLU reference < sl.lemma > for lang sl  points to SPARK NLP model : lemma\n",
            "NLU reference < sl.pos > for lang sl  points to SPARK NLP model : pos_ud_ssj\n",
            "NLU reference < so.stopwords > for lang so  points to SPARK NLP model : stopwords_so\n",
            "NLU reference < st.stopwords > for lang st  points to SPARK NLP model : stopwords_st\n",
            "NLU reference < sw.stopwords > for lang sw  points to SPARK NLP model : stopwords_sw\n",
            "NLU reference < sv.lemma > for lang sv  points to SPARK NLP model : lemma\n",
            "NLU reference < sv.pos > for lang sv  points to SPARK NLP model : pos_ud_tal\n",
            "NLU reference < sv.pos.ud_tal > for lang sv  points to SPARK NLP model : pos_ud_tal\n",
            "NLU reference < sv.stopwords > for lang sv  points to SPARK NLP model : stopwords_sv\n",
            "NLU reference < th.stopwords > for lang th  points to SPARK NLP model : stopwords_th\n",
            "NLU reference < tr.lemma > for lang tr  points to SPARK NLP model : lemma\n",
            "NLU reference < tr.pos > for lang tr  points to SPARK NLP model : pos_ud_imst\n",
            "NLU reference < tr.pos.ud_imst > for lang tr  points to SPARK NLP model : pos_ud_imst\n",
            "NLU reference < tr.stopwords > for lang tr  points to SPARK NLP model : stopwords_tr\n",
            "NLU reference < uk.lemma > for lang uk  points to SPARK NLP model : lemma\n",
            "NLU reference < uk.pos > for lang uk  points to SPARK NLP model : pos_ud_iu\n",
            "NLU reference < uk.pos.ud_iu > for lang uk  points to SPARK NLP model : pos_ud_iu\n",
            "NLU reference < yo.stopwords > for lang yo  points to SPARK NLP model : stopwords_yo\n",
            "NLU reference < yo.lemma > for lang yo  points to SPARK NLP model : lemma\n",
            "NLU reference < yo.pos > for lang yo  points to SPARK NLP model : pos_ud_ytb\n",
            "NLU reference < zu.stopwords > for lang zu  points to SPARK NLP model : stopwords_zu\n",
            "NLU reference < xx.embed.glove_840B_300 > for lang xx  points to SPARK NLP model : glove_840B_300\n",
            "NLU reference < xx.embed.glove_6B_300 > for lang xx  points to SPARK NLP model : glove_6B_300\n",
            "NLU reference < xx.embed.bert_multi_cased > for lang xx  points to SPARK NLP model : bert_multi_cased\n",
            "NLU reference < xx.classify.wiki_7 > for lang xx  points to SPARK NLP model : ld_wiki_7\n",
            "NLU reference < xx.classify.wiki_20 > for lang xx  points to SPARK NLP model : ld_wiki_20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnTiz427LmeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "c34c4ac3-e932-46a6-f003-4e4c5b3433e1"
      },
      "source": [
        "\n",
        "# any string inside of <> can be apssed to nlu.load()\n",
        "nlu.print_components(action='classify')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For language < en > NLU provides the following Models : \n",
            "NLU reference < en.classify.trec6.use > points to Spark NLP reference classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50.use > points to Spark NLP reference classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam.use > points to Spark NLP reference classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews.use > points to Spark NLP reference classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion.use > points to Spark NLP reference classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying.use > points to Spark NLP reference classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm.use > points to Spark NLP reference classifierdl_use_sarcasm\n",
            "NLU reference < en.classify.trec6 > points to Spark NLP reference classifierdl_use_trec6\n",
            "NLU reference < en.classify.trec50 > points to Spark NLP reference classifierdl_use_trec50\n",
            "NLU reference < en.classify.spam > points to Spark NLP reference classifierdl_use_spam\n",
            "NLU reference < en.classify.fakenews > points to Spark NLP reference classifierdl_use_fakenews\n",
            "NLU reference < en.classify.emotion > points to Spark NLP reference classifierdl_use_emotion\n",
            "NLU reference < en.classify.cyberbullying > points to Spark NLP reference classifierdl_use_cyberbullying\n",
            "NLU reference < en.classify.sarcasm > points to Spark NLP reference classifierdl_use_sarcasm\n",
            "For language < xx > NLU provides the following Models : \n",
            "NLU reference < xx.classify.wiki_7 > points to Spark NLP reference ld_wiki_7\n",
            "NLU reference < xx.classify.wiki_20 > points to Spark NLP reference ld_wiki_20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGuetrAYpk1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c0d5c87d-ffe7-488b-81f4-7caf0ed74722"
      },
      "source": [
        "# any string inside of <> can be apssed to nlu.load()\n",
        "nlu.print_components(lang='de')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLU pipe reference :  < de.explain.document > points to Spark NLP Pipeline: explain_document_md\n",
            "NLU pipe reference :  < de.explain.document.md > points to Spark NLP Pipeline: explain_document_md\n",
            "NLU pipe reference :  < de.explain.document.lg > points to Spark NLP Pipeline: explain_document_lg\n",
            "NLU pipe reference :  < de.ner.recognizer > points to Spark NLP Pipeline: entity_recognizer_md\n",
            "NLU pipe reference :  < de.ner.recognizer.md > points to Spark NLP Pipeline: entity_recognizer_md\n",
            "NLU pipe reference :  < de.ner.recognizer.lg > points to Spark NLP Pipeline: entity_recognizer_lg\n",
            "All Pipelines for language de \n",
            " {'de.lemma': 'lemma', 'de.pos.ud_hdt': 'pos_ud_hdt', 'de.pos': 'pos_ud_hdt', 'de.ner': 'wikiner_840B_300', 'de.ner.wikiner': 'wikiner_840B_300', 'de.ner.wikiner.glove_840B_300': 'wikiner_840B_300', 'de.stopwords': 'stopwords_de'}\n",
            "NLU reference : < de.lemma > points to Spark NLP Model:  lemma\n",
            "NLU reference : < de.pos.ud_hdt > points to Spark NLP Model:  pos_ud_hdt\n",
            "NLU reference : < de.pos > points to Spark NLP Model:  pos_ud_hdt\n",
            "NLU reference : < de.ner > points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference : < de.ner.wikiner > points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference : < de.ner.wikiner.glove_840B_300 > points to Spark NLP Model:  wikiner_840B_300\n",
            "NLU reference : < de.stopwords > points to Spark NLP Model:  stopwords_de\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQsSdLMBVlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import nlu\n",
        "import pandas as pd \n",
        "def get_sample_pdf() :\n",
        "    data = {\"text\": ['This day sucks. But tmorow will be beter', 'Donald Trump had a meeting with Tim Apple. ', 'This is your last chance. After this, there is no turning back. You take the blue pill — the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill — you stay in Wonderland and I show you how deep the rabbit-hole goes.' ]}\n",
        "    text_df = pd.DataFrame(data)\n",
        "    return text_df\n",
        "sample_pdf = get_sample_pdf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfzGkmoJc8H",
        "colab_type": "text"
      },
      "source": [
        "#Named Entity Recognition ( NER ) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yKrHWIgJUTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "aa9c7b04-8a5d-40b5-b579-043226c73a48"
      },
      "source": [
        "nlu.load('ner').predict('Angela Merkel from Germany and the American Donald Trump dont share many oppinions')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recognize_entities_dl download started this may take some time.\n",
            "Approx size to download 159 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>embeddings_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.563759982585907, 0.26958999037742615, 0.35...</td>\n",
              "      <td>Angela</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.000499963760376, 0.41997000575065613, 0.59...</td>\n",
              "      <td>Merkel</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.30730998516082764, 0.24737000465393066, 0.6...</td>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.6208900213241577, 0.7105100154876709, 0.495...</td>\n",
              "      <td>Germany</td>\n",
              "      <td>B-LOC</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.07195299863815308, 0.23127000033855438, 0....</td>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.03819400072097778, -0.24487000703811646, 0...</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.38666000962257385, 0.6482700109481812, 0.72...</td>\n",
              "      <td>American</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-0.5496799945831299, -0.488319993019104, 0.59...</td>\n",
              "      <td>Donald</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.15730999410152435, -0.7550299763679504, 0....</td>\n",
              "      <td>Trump</td>\n",
              "      <td>I-PER</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[0.0024119000881910324, 0.5014399886131287, 0....</td>\n",
              "      <td>dont</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.5208799839019775, 0.761210024356842, 0.2608...</td>\n",
              "      <td>share</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[-0.3291400074958801, 0.8288699984550476, -0.1...</td>\n",
              "      <td>many</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>oppinions</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Angela Merkel, Germany, American, Donald Trump]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                embeddings_embeddings  ...                                          entities\n",
              "0   [-0.563759982585907, 0.26958999037742615, 0.35...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "1   [-1.000499963760376, 0.41997000575065613, 0.59...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "2   [0.30730998516082764, 0.24737000465393066, 0.6...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "3   [0.6208900213241577, 0.7105100154876709, 0.495...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "4   [-0.07195299863815308, 0.23127000033855438, 0....  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "5   [-0.03819400072097778, -0.24487000703811646, 0...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "6   [0.38666000962257385, 0.6482700109481812, 0.72...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "7   [-0.5496799945831299, -0.488319993019104, 0.59...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "8   [-0.15730999410152435, -0.7550299763679504, 0....  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "9   [0.0024119000881910324, 0.5014399886131287, 0....  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "10  [0.5208799839019775, 0.761210024356842, 0.2608...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "11  [-0.3291400074958801, 0.8288699984550476, -0.1...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  [Angela Merkel, Germany, American, Donald Trump]\n",
              "\n",
              "[13 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWdBYDQ6_Wbt",
        "colab_type": "text"
      },
      "source": [
        "# NER with Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDZ4IJv_R1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "7b0dc162-1281-4f05-f525-85179f98eb70"
      },
      "source": [
        "nlu.load('en.ner.bert').predict('The NLU library is a machine learning library, simmilar to Tensorflow and Keras')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recognize_entities_bert download started this may take some time.\n",
            "Approx size to download 404.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>embeddings_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-1.1980020999908447, 0.3962576389312744, 0.74...</td>\n",
              "      <td>The</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-0.10782831907272339, -0.6500216722488403, -1...</td>\n",
              "      <td>NLU</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.3750513195991516, -0.6567936539649963, -0.0...</td>\n",
              "      <td>library</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-1.045413851737976, 0.6961295008659363, 0.448...</td>\n",
              "      <td>is</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.43795740604400635, 0.6937284469604492, -0....</td>\n",
              "      <td>a</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.42621397972106934, -0.9140262603759766, -0....</td>\n",
              "      <td>machine</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-0.33394142985343933, -0.019593283534049988, ...</td>\n",
              "      <td>learning</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0.7461637854576111, -0.5252398252487183, 0.14...</td>\n",
              "      <td>library</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.6470324397087097, 0.6674264073371887, -0.2...</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.22510823607444763, -0.08115461468696594, 0...</td>\n",
              "      <td>simmilar</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.7086092829704285, 0.1666388064622879, -0.09...</td>\n",
              "      <td>to</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[0.9141660928726196, -0.8774193525314331, -0.4...</td>\n",
              "      <td>Tensorflow</td>\n",
              "      <td>B-ORG</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[-0.3223542869091034, 0.1606275737285614, -0.1...</td>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[0.5242998003959656, 0.9928402900695801, -1.24...</td>\n",
              "      <td>Keras</td>\n",
              "      <td>B-ORG</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NLU, Tensorflow, Keras]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                embeddings_embeddings  ...                  entities\n",
              "0   [-1.1980020999908447, 0.3962576389312744, 0.74...  ...  [NLU, Tensorflow, Keras]\n",
              "1   [-0.10782831907272339, -0.6500216722488403, -1...  ...  [NLU, Tensorflow, Keras]\n",
              "2   [0.3750513195991516, -0.6567936539649963, -0.0...  ...  [NLU, Tensorflow, Keras]\n",
              "3   [-1.045413851737976, 0.6961295008659363, 0.448...  ...  [NLU, Tensorflow, Keras]\n",
              "4   [-0.43795740604400635, 0.6937284469604492, -0....  ...  [NLU, Tensorflow, Keras]\n",
              "5   [0.42621397972106934, -0.9140262603759766, -0....  ...  [NLU, Tensorflow, Keras]\n",
              "6   [-0.33394142985343933, -0.019593283534049988, ...  ...  [NLU, Tensorflow, Keras]\n",
              "7   [0.7461637854576111, -0.5252398252487183, 0.14...  ...  [NLU, Tensorflow, Keras]\n",
              "8   [-0.6470324397087097, 0.6674264073371887, -0.2...  ...  [NLU, Tensorflow, Keras]\n",
              "9   [-0.22510823607444763, -0.08115461468696594, 0...  ...  [NLU, Tensorflow, Keras]\n",
              "10  [0.7086092829704285, 0.1666388064622879, -0.09...  ...  [NLU, Tensorflow, Keras]\n",
              "11  [0.9141660928726196, -0.8774193525314331, -0.4...  ...  [NLU, Tensorflow, Keras]\n",
              "12  [-0.3223542869091034, 0.1606275737285614, -0.1...  ...  [NLU, Tensorflow, Keras]\n",
              "13  [0.5242998003959656, 0.9928402900695801, -1.24...  ...  [NLU, Tensorflow, Keras]\n",
              "\n",
              "[14 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93QXku8v_f1v",
        "colab_type": "text"
      },
      "source": [
        "# German NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnP5c3w6_gvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "dd0b75df-e0fb-4b1b-b7c8-2175f8e66a31"
      },
      "source": [
        "nlu.load('de.ner').predict('NLU ist eine Machine Learning Library, ähnlich zu Keras und Tensorflow', output_level='sentence')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikiner_840B_300 download started this may take some time.\n",
            "Approximate size to download 14 MB\n",
            "[OK!]\n",
            "glove_840B_300 download started this may take some time.\n",
            "Approximate size to download 2.3 GB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>ner</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLU ist eine Machine Learning Library, ähnlich...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.9560800194740295, -0.10012000054121017, 0....</td>\n",
              "      <td>[I-MISC, O, O, O, O, O, O, O, O, I-PER, O, I-PER]</td>\n",
              "      <td>[NNP, NN, NN, NNP, NNP, NNP, ,, JJ, NN, NNP, N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                                pos\n",
              "0  NLU ist eine Machine Learning Library, ähnlich...  ...  [NNP, NN, NN, NNP, NNP, NNP, ,, JJ, NN, NNP, N...\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaSsjgMKIBhb",
        "colab_type": "text"
      },
      "source": [
        "# POS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMurEqcIBBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "94e6bb4a-3c5b-44d8-eb63-db12cbf993db"
      },
      "source": [
        "nlu.load('pos').predict('Part of speech assigns each token in a sentence a grammatcial label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NN</td>\n",
              "      <td>Part</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NN</td>\n",
              "      <td>speech</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NNS</td>\n",
              "      <td>assigns</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DT</td>\n",
              "      <td>each</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>token</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>IN</td>\n",
              "      <td>in</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NN</td>\n",
              "      <td>sentence</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>JJ</td>\n",
              "      <td>grammatcial</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NN</td>\n",
              "      <td>label</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pos        token          id\n",
              "0    NN         Part  8589934592\n",
              "1    IN           of  8589934592\n",
              "2    NN       speech  8589934592\n",
              "3   NNS      assigns  8589934592\n",
              "4    DT         each  8589934592\n",
              "5    NN        token  8589934592\n",
              "6    IN           in  8589934592\n",
              "7    DT            a  8589934592\n",
              "8    NN     sentence  8589934592\n",
              "9    DT            a  8589934592\n",
              "10   JJ  grammatcial  8589934592\n",
              "11   NN        label  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ury8E3fINGh",
        "colab_type": "text"
      },
      "source": [
        "# Emotions Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGKJ_kefIJXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "4f79aa0c-f30d-4439-abb0-98e57e693672"
      },
      "source": [
        "nlu.load('classify.emotion').predict(\"I love NLU!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_emotion download started this may take some time.\n",
            "Approximate size to download 20.7 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.027570432052016258, -0.052647676318883896, ...</td>\n",
              "      <td>0.976017</td>\n",
              "      <td>I love NLU!</td>\n",
              "      <td>joy</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.027570432052016258, -0.052647676318883896, ...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-nQoMUIMBu",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn__rI5oILfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "1338520e-1e02-49c2-909f-da31ed106d49"
      },
      "source": [
        "nlu.load('sentiment').predict(\"I hate this guy Sami\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>id</th>\n",
              "      <th>checked</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>0.577800</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[I, hate, this, guy, Sami]</td>\n",
              "      <td>I hate this guy Sami</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment  ...              sentence\n",
              "0  negative  ...  I hate this guy Sami\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlW3GGxPI_Bk",
        "colab_type": "text"
      },
      "source": [
        "# Language Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPJQo5nOILPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "4ccff37a-7cd5-4307-f2a0-cd836ed23e1e"
      },
      "source": [
        "nlu.load('lang').predict(['NLU is an open-source text processing library for advanced natural language processing for the Python.','NLU est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python.'])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detect_language_20 download started this may take some time.\n",
            "Approx size to download 3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>language_confidence</th>\n",
              "      <th>language</th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.985407</td>\n",
              "      <td>en</td>\n",
              "      <td>NLU is an open-source text processing library ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.999822</td>\n",
              "      <td>fr</td>\n",
              "      <td>NLU est une bibliothèque de traitement de text...</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  language_confidence  ...          id\n",
              "0            0.985407  ...           0\n",
              "1            0.999822  ...  8589934592\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DqtDX-xGYS7",
        "colab_type": "text"
      },
      "source": [
        "# Classify Twitter sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jrDm6E9QeuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ddf387fd-bd4a-4a82-abe6-b8b48f3e64f4"
      },
      "source": [
        "nlu.load('en.sentiment.twitter').predict('@elonmusk Tesla stock price is too high imo')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentimentdl_use_twitter download started this may take some time.\n",
            "Approx size to download 928.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[0.08604438602924347, 0.04703635722398758, -0...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>@elonmusk Tesla stock price is too high imo</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [[0.08604438602924347, 0.04703635722398758, -0...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkF6S7yhGV0H",
        "colab_type": "text"
      },
      "source": [
        "# Classify Movie Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVpv6bWFQhh7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "76552d4e-a67e-4c9e-f5da-c9118ef08cda"
      },
      "source": [
        "nlu.load('en.sentiment.imdb').predict('The Matrix was a pretty good movie')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentimentdl_use_imdb download started this may take some time.\n",
            "Approx size to download 935.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[0.04629608988761902, -0.020867452025413513, ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>The Matrix was a pretty good movie</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [[0.04629608988761902, -0.020867452025413513, ...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gNma9kAGS_w",
        "colab_type": "text"
      },
      "source": [
        "# Classify Sarcasm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgLay6JFSa1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "b8f37812-ef33-4cf1-c0e2-0cfcef004eea"
      },
      "source": [
        "nlu.load('en.classify.sarcasm').predict('gotta love the teachers who give examns on the day after halloween') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_sarcasm download started this may take some time.\n",
            "Approximate size to download 21.5 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.03146284446120262, 0.04071342945098877, 0....</td>\n",
              "      <td>0.999985</td>\n",
              "      <td>gotta love the teachers who give examns on the...</td>\n",
              "      <td>sarcasm</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.03146284446120262, 0.04071342945098877, 0....  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28Jq2VfGRGe",
        "colab_type": "text"
      },
      "source": [
        "# Classify Cyberbullying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6embGXF5b4j5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "f77d89c7-0dc2-4503-a1db-d1d8a71a798e"
      },
      "source": [
        "nlu.load('en.classify.cyberbullying').predict('Women belong in the kitchen.') # Sorry we dont mean it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_cyberbullying download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.054944973438978195, -0.022223370149731636,...</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>Women belong in the kitchen.</td>\n",
              "      <td>sexism</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.054944973438978195, -0.022223370149731636,...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXe9M0wYGO4f",
        "colab_type": "text"
      },
      "source": [
        "# Classify Fake News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obJtaic1b4c4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "44e8f968-290a-4cf1-8970-76b59191c351"
      },
      "source": [
        "nlu.load('en.classify.fakenews').predict('Unicorns have been sighted on Mars!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_fakenews download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.01756167598068714, 0.015006818808615208, -...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Unicorns have been sighted on Mars!</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.01756167598068714, 0.015006818808615208, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op-HwFFUGNkT",
        "colab_type": "text"
      },
      "source": [
        "# Classify Spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2q6eYStVjq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "50183b55-2573-44ba-8777-8830e71e99d9"
      },
      "source": [
        "nlu.load('en.classify.spam').predict('Please sign up for this FREE membership it costs $$NO MONEY$$ just your mobile number!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_spam download started this may take some time.\n",
            "Approximate size to download 21.5 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.008322705514729023, 0.009957313537597656, 0...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Please sign up for this FREE membership it cos...</td>\n",
              "      <td>spam</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.008322705514729023, 0.009957313537597656, 0...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzWc9H1MGLgJ",
        "colab_type": "text"
      },
      "source": [
        "# Classify 6 Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx4yclHQA2Vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "9f8abe15-f66b-442d-e3ca-38cadb41d811"
      },
      "source": [
        "nlu.load('en.classify.trec6').predict('Where is the next food store?', metadata=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_trec6 download started this may take some time.\n",
            "Approximate size to download 21.4 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.05699703469872475, 0.039651867002248764, -...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Where is the next food store?</td>\n",
              "      <td>LOC</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [-0.05699703469872475, 0.039651867002248764, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYPEuOPlGIdG",
        "colab_type": "text"
      },
      "source": [
        "# Classify 50 Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXZEaTlcECH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "a2f3984a-062f-494f-899e-9843c03468e6"
      },
      "source": [
        "nlu.load('en.classify.trec50').predict('How expensive is the watch?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifierdl_use_trec50 download started this may take some time.\n",
            "Approximate size to download 21.2 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embeddings</th>\n",
              "      <th>category_confidence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.051809534430503845, 0.03128402680158615, -0...</td>\n",
              "      <td>0.919436</td>\n",
              "      <td>How expensive is the watch?</td>\n",
              "      <td>NUM_count</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence_embeddings  ...          id\n",
              "0  [0.051809534430503845, 0.03128402680158615, -0...  ...  8589934592\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDpfxO1Ta3Kn",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVfo2nCaLSiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "4715f014-7862-40c2-c3ec-c9899f12f71e"
      },
      "source": [
        "nlu.load('bert').predict('NLU offers the latest embeddings in one line ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_uncased download started this may take some time.\n",
            "Approximate size to download 392.5 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.3253086805343628, -0.574441134929657, -0.08...</td>\n",
              "      <td>NLU</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-0.6660361886024475, -0.1494743824005127, -0....</td>\n",
              "      <td>offers</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.6587662696838379, 0.3323703110218048, 0.16...</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.7552685737609863, 0.17207926511764526, 1.35...</td>\n",
              "      <td>latest</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.09838500618934631, -1.1448147296905518, -1...</td>\n",
              "      <td>embeddings</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.4635896384716034, 0.38369956612586975, 0.0...</td>\n",
              "      <td>in</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.26821616291999817, 0.7025910019874573, 0.15...</td>\n",
              "      <td>one</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-0.31930840015411377, -0.48271292448043823, 0...</td>\n",
              "      <td>line</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     bert_embeddings       token          id\n",
              "0  [0.3253086805343628, -0.574441134929657, -0.08...         NLU  8589934592\n",
              "1  [-0.6660361886024475, -0.1494743824005127, -0....      offers  8589934592\n",
              "2  [-0.6587662696838379, 0.3323703110218048, 0.16...         the  8589934592\n",
              "3  [0.7552685737609863, 0.17207926511764526, 1.35...      latest  8589934592\n",
              "4  [-0.09838500618934631, -1.1448147296905518, -1...  embeddings  8589934592\n",
              "5  [-0.4635896384716034, 0.38369956612586975, 0.0...          in  8589934592\n",
              "6  [0.26821616291999817, 0.7025910019874573, 0.15...         one  8589934592\n",
              "7  [-0.31930840015411377, -0.48271292448043823, 0...        line  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQUJHqBa5z1",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Albert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpPbMuzMRQ9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "0ed34e9f-e68a-4649-8c65-fc7035e40f20"
      },
      "source": [
        "nlu.load('albert').predict('Albert uses a collection of many berts to generate embeddigns ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "albert_base_uncased download started this may take some time.\n",
            "Approximate size to download 42.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>albert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.08257609605789185, -0.8017427325248718, 1....</td>\n",
              "      <td>Albert</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.8256351947784424, -1.5144840478897095, 0.90...</td>\n",
              "      <td>uses</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.22089454531669617, -0.24295514822006226, 3...</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.2136894017457962, -0.8225528597831726, -0....</td>\n",
              "      <td>collection</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.7623294591903687, -1.113651156425476, 0.800...</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.6415284872055054, -0.04533941298723221, 1.9...</td>\n",
              "      <td>many</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-0.5591965317726135, -1.1773797273635864, -0....</td>\n",
              "      <td>berts</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0956681966781616, -1.4180747270584106, -0.2...</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.6759272813796997, -1.3546931743621826, 1.6...</td>\n",
              "      <td>generate</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.0035803020000457764, -0.35928264260292053,...</td>\n",
              "      <td>embeddigns</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   albert_embeddings       token          id\n",
              "0  [-0.08257609605789185, -0.8017427325248718, 1....      Albert  8589934592\n",
              "1  [0.8256351947784424, -1.5144840478897095, 0.90...        uses  8589934592\n",
              "2  [-0.22089454531669617, -0.24295514822006226, 3...           a  8589934592\n",
              "3  [-0.2136894017457962, -0.8225528597831726, -0....  collection  8589934592\n",
              "4  [1.7623294591903687, -1.113651156425476, 0.800...          of  8589934592\n",
              "5  [0.6415284872055054, -0.04533941298723221, 1.9...        many  8589934592\n",
              "6  [-0.5591965317726135, -1.1773797273635864, -0....       berts  8589934592\n",
              "7  [1.0956681966781616, -1.4180747270584106, -0.2...          to  8589934592\n",
              "8  [-0.6759272813796997, -1.3546931743621826, 1.6...    generate  8589934592\n",
              "9  [-0.0035803020000457764, -0.35928264260292053,...  embeddigns  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSIqvC_4a8A4",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Elmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4prfr_0BR5NS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "b7b4f2fe-74de-4a8a-9d3c-d0e94aae4237"
      },
      "source": [
        "nlu.load('elmo').predict('Elmo was trained on Left to right masked to learn its embeddings')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo download started this may take some time.\n",
            "Approximate size to download 334.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>elmo_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.6083735227584839, 0.20089012384414673, 0.42...</td>\n",
              "      <td>Elmo</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.2980785369873047, -0.07382500916719437, -0....</td>\n",
              "      <td>was</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.39923471212387085, 0.17155063152313232, 0....</td>\n",
              "      <td>trained</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.04337821900844574, 0.1392083466053009, -0.4...</td>\n",
              "      <td>on</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.4468783736228943, -0.623046875, 0.771505534...</td>\n",
              "      <td>Left</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.18209676444530487, 0.03812692314386368, 0....</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.23305709660053253, -0.6459438800811768, 0.5...</td>\n",
              "      <td>right</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-0.7243442535400391, 0.10247116535902023, 0.1...</td>\n",
              "      <td>masked</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-0.18209676444530487, 0.03812692314386368, 0....</td>\n",
              "      <td>to</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.2942464351654053, 0.7376189231872559, -0.58...</td>\n",
              "      <td>learn</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.055951207876205444, 0.19218483567237854, -0...</td>\n",
              "      <td>its</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[-1.31377112865448, 0.7727609872817993, 0.6748...</td>\n",
              "      <td>embeddings</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      elmo_embeddings       token          id\n",
              "0   [0.6083735227584839, 0.20089012384414673, 0.42...        Elmo  8589934592\n",
              "1   [0.2980785369873047, -0.07382500916719437, -0....         was  8589934592\n",
              "2   [-0.39923471212387085, 0.17155063152313232, 0....     trained  8589934592\n",
              "3   [0.04337821900844574, 0.1392083466053009, -0.4...          on  8589934592\n",
              "4   [0.4468783736228943, -0.623046875, 0.771505534...        Left  8589934592\n",
              "5   [-0.18209676444530487, 0.03812692314386368, 0....          to  8589934592\n",
              "6   [0.23305709660053253, -0.6459438800811768, 0.5...       right  8589934592\n",
              "7   [-0.7243442535400391, 0.10247116535902023, 0.1...      masked  8589934592\n",
              "8   [-0.18209676444530487, 0.03812692314386368, 0....          to  8589934592\n",
              "9   [1.2942464351654053, 0.7376189231872559, -0.58...       learn  8589934592\n",
              "10  [0.055951207876205444, 0.19218483567237854, -0...         its  8589934592\n",
              "11  [-1.31377112865448, 0.7727609872817993, 0.6748...  embeddings  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkczWqZ6bEgm",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings Xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMNdg7U_TXeW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "819b50d3-602e-4c77-b951-1f2dc1349e33"
      },
      "source": [
        "nlu.load('xlnet').predict('XLNET computes contextualized word representations using combination of Autoregressive Language Model and Permutation Language Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xlnet_base_cased download started this may take some time.\n",
            "Approximate size to download 415.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xlnet_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.02719488926231861, -1.7693557739257812, -0...</td>\n",
              "      <td>XLNET</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.8262947797775269, 0.8455266356468201, 0.57...</td>\n",
              "      <td>computes</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2.8446314334869385, -0.3564329445362091, -2.1...</td>\n",
              "      <td>contextualized</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.6143839359283447, -1.7368144989013672, -0....</td>\n",
              "      <td>word</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.30445945262908936, -1.2129613161087036, 0....</td>\n",
              "      <td>representations</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.07423821836709976, -0.02561005763709545, -0...</td>\n",
              "      <td>using</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-0.5387097597122192, -1.1827564239501953, 0.5...</td>\n",
              "      <td>combination</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-1.403516411781311, 0.3108177185058594, -0.32...</td>\n",
              "      <td>of</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-1.0869172811508179, 0.7135171890258789, -0.2...</td>\n",
              "      <td>Autoregressive</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-0.33215752243995667, -1.4108021259307861, -0...</td>\n",
              "      <td>Language</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[-1.6097160577774048, -0.2548254430294037, 0.0...</td>\n",
              "      <td>Model</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[0.7884324789047241, -1.507911205291748, 0.677...</td>\n",
              "      <td>and</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[0.6049966812133789, -0.157279372215271, -0.06...</td>\n",
              "      <td>Permutation</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[-0.33215752243995667, -1.4108021259307861, -0...</td>\n",
              "      <td>Language</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[-1.6097160577774048, -0.2548254430294037, 0.0...</td>\n",
              "      <td>Model</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     xlnet_embeddings  ...          id\n",
              "0   [-0.02719488926231861, -1.7693557739257812, -0...  ...  8589934592\n",
              "1   [-1.8262947797775269, 0.8455266356468201, 0.57...  ...  8589934592\n",
              "2   [2.8446314334869385, -0.3564329445362091, -2.1...  ...  8589934592\n",
              "3   [-0.6143839359283447, -1.7368144989013672, -0....  ...  8589934592\n",
              "4   [-0.30445945262908936, -1.2129613161087036, 0....  ...  8589934592\n",
              "5   [0.07423821836709976, -0.02561005763709545, -0...  ...  8589934592\n",
              "6   [-0.5387097597122192, -1.1827564239501953, 0.5...  ...  8589934592\n",
              "7   [-1.403516411781311, 0.3108177185058594, -0.32...  ...  8589934592\n",
              "8   [-1.0869172811508179, 0.7135171890258789, -0.2...  ...  8589934592\n",
              "9   [-0.33215752243995667, -1.4108021259307861, -0...  ...  8589934592\n",
              "10  [-1.6097160577774048, -0.2548254430294037, 0.0...  ...  8589934592\n",
              "11  [0.7884324789047241, -1.507911205291748, 0.677...  ...  8589934592\n",
              "12  [0.6049966812133789, -0.157279372215271, -0.06...  ...  8589934592\n",
              "13  [-0.33215752243995667, -1.4108021259307861, -0...  ...  8589934592\n",
              "14  [-1.6097160577774048, -0.2548254430294037, 0.0...  ...  8589934592\n",
              "\n",
              "[15 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ5gWHp4bGtI",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZTQl5j7SGHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "0d8849f8-89f6-49c8-f03c-1575d66c9fd2"
      },
      "source": [
        "nlu.load('glove').predict('Glove embeddings are generated by aggregating global word-word co-occurrence matrix from a corpus')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>glove_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Glove embeddings are generated by aggregating ...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.3677999973297119, 0.37073999643325806, 0.3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                   glove_embeddings\n",
              "0  Glove embeddings are generated by aggregating ...  ...  [[0.3677999973297119, 0.37073999643325806, 0.3...\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLDT5FJJLS2Z",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings Biobert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er9ACOKKMBJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "e40c868a-8a0b-4829-8c3c-6b62b18cc0f0"
      },
      "source": [
        "nlu.load('biobert').predict('Biobert was pretrained on a medical dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biobert_pubmed_base_cased download started this may take some time.\n",
            "Approximate size to download 384.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bert_embeddings</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-1.1250135898590088, -1.468251347541809, 0.45...</td>\n",
              "      <td>Biobert</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.2141287326812744, 0.513595461845398, 0.108...</td>\n",
              "      <td>was</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0130831003189087, -1.8673359155654907, -0.6...</td>\n",
              "      <td>pretrained</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.03375675156712532, 0.03003840334713459, 0.0...</td>\n",
              "      <td>on</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.2643347382545471, 1.3394858837127686, -0.15...</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-0.0770333781838417, -1.6575649976730347, -0....</td>\n",
              "      <td>medical</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.7789418697357178, 0.11998558044433594, 0.01...</td>\n",
              "      <td>dataset</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     bert_embeddings       token          id\n",
              "0  [-1.1250135898590088, -1.468251347541809, 0.45...     Biobert  8589934592\n",
              "1  [-1.2141287326812744, 0.513595461845398, 0.108...         was  8589934592\n",
              "2  [1.0130831003189087, -1.8673359155654907, -0.6...  pretrained  8589934592\n",
              "3  [0.03375675156712532, 0.03003840334713459, 0.0...          on  8589934592\n",
              "4  [0.2643347382545471, 1.3394858837127686, -0.15...           a  8589934592\n",
              "5  [-0.0770333781838417, -1.6575649976730347, -0....     medical  8589934592\n",
              "6  [1.7789418697357178, 0.11998558044433594, 0.01...     dataset  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id8zubzibIeG",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Embeddings Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H64s1vH6V8Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "36a1317b-6d4f-49dc-f532-0a2329716f6b"
      },
      "source": [
        "nlu.load('use').predict('USE is designed to encode whole sentences and documents into vectors that can be used for text classification, sementic similarirty, clustering or oder NLP tasks')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_embeddings</th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.03302069380879402, -0.004255455918610096, -...</td>\n",
              "      <td>USE is designed to encode whole sentences and ...</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      use_embeddings  ...          id\n",
              "0  [0.03302069380879402, -0.004255455918610096, -...  ...  8589934592\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MyOnSpYLXQM",
        "colab_type": "text"
      },
      "source": [
        "# Chunk embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOs-2IHiLIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlu.load('embed_chunk').predict('a wondful day') # work in progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyo_d5bILbeO",
        "colab_type": "text"
      },
      "source": [
        "# Sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Cwk1skLaQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "9689656b-cee3-4000-cc03-7f5f1918e34c"
      },
      "source": [
        "nlu.load('embed_sentence').predict('Embed all the sentences. In the input please')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_embeddings</th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.01943225972354412, 0.012606756761670113, -...</td>\n",
              "      <td>Embed all the sentences.</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.02531137689948082, -0.016110848635435104, -...</td>\n",
              "      <td>In the input please</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      use_embeddings  ...          id\n",
              "0  [-0.01943225972354412, 0.012606756761670113, -...  ...  8589934592\n",
              "1  [0.02531137689948082, -0.016110848635435104, -...  ...  8589934592\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0tcHMcCLfUH",
        "colab_type": "text"
      },
      "source": [
        "# Multiple embeddings at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAw80hveLdfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "b7c6f605-b812-45cd-c613-549be1ae0f63"
      },
      "source": [
        "nlu.load('albert elmo bert xlnet glove use').predict('You can get multiple embeddings at once in NLU in just one line!') #watch out RAM killer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "albert_base_uncased download started this may take some time.\n",
            "Approximate size to download 42.7 MB\n",
            "[OK!]\n",
            "elmo download started this may take some time.\n",
            "Approximate size to download 334.1 MB\n",
            "[OK!]\n",
            "bert_base_uncased download started this may take some time.\n",
            "Approximate size to download 392.5 MB\n",
            "[OK!]\n",
            "xlnet_base_cased download started this may take some time.\n",
            "Approximate size to download 415.8 MB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_embeddings</th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>bert_embeddings</th>\n",
              "      <th>albert_embeddings</th>\n",
              "      <th>glove_embeddings</th>\n",
              "      <th>elmo_embeddings</th>\n",
              "      <th>xlnet_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.025560321286320686, -0.04601415991783142, -...</td>\n",
              "      <td>You can get multiple embeddings at once in NLU...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[-0.28787779808044434, 0.5019065141677856, 0....</td>\n",
              "      <td>[[-0.21463489532470703, -0.45155370235443115, ...</td>\n",
              "      <td>[[-0.4988600015640259, 0.7660199999809265, 0.8...</td>\n",
              "      <td>[[0.6117659211158752, -0.18037280440330505, -0...</td>\n",
              "      <td>[[1.2179217338562012, -1.5724607706069946, 0.3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      use_embeddings  ...                                   xlnet_embeddings\n",
              "0  [0.025560321286320686, -0.04601415991783142, -...  ...  [[1.2179217338562012, -1.5724607706069946, 0.3...\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_jscfJLiWE",
        "colab_type": "text"
      },
      "source": [
        "# Dependency parsing untyped \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw3ypT1Ljfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "ecd6585b-7486-4a07-e792-268dba307d8f"
      },
      "source": [
        "nlu.load('dep.untyped').predict('Untyped Dependencies represent a grammatical tree structure')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.6 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>token</th>\n",
              "      <th>dependency</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNP</td>\n",
              "      <td>Untyped</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NNP</td>\n",
              "      <td>Dependencies</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBD</td>\n",
              "      <td>represent</td>\n",
              "      <td>Untyped</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DT</td>\n",
              "      <td>a</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JJ</td>\n",
              "      <td>grammatical</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>tree</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NN</td>\n",
              "      <td>structure</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pos         token dependency          id\n",
              "0  NNP       Untyped       ROOT  8589934592\n",
              "1  NNP  Dependencies  represent  8589934592\n",
              "2  VBD     represent    Untyped  8589934592\n",
              "3   DT             a  structure  8589934592\n",
              "4   JJ   grammatical  structure  8589934592\n",
              "5   NN          tree  structure  8589934592\n",
              "6   NN     structure  represent  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx2P6P_zLnSC",
        "colab_type": "text"
      },
      "source": [
        "# Dependency parsing typed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YpvBNBjLm6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "e27e09fa-0495-4598-949d-fad78c660ea4"
      },
      "source": [
        "nlu.load('dep').predict('Typed Dependencies represent a grammatical tree structure where every edge has a label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_typed_conllu download started this may take some time.\n",
            "Approximate size to download 257.4 KB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n",
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.6 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>labled_dependency</th>\n",
              "      <th>dependency</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNP</td>\n",
              "      <td>root</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>Typed</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>represent</td>\n",
              "      <td>Dependencies</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBD</td>\n",
              "      <td>parataxis</td>\n",
              "      <td>Typed</td>\n",
              "      <td>represent</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>structure</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JJ</td>\n",
              "      <td>amod</td>\n",
              "      <td>structure</td>\n",
              "      <td>grammatical</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NN</td>\n",
              "      <td>flat</td>\n",
              "      <td>structure</td>\n",
              "      <td>tree</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>represent</td>\n",
              "      <td>structure</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WRB</td>\n",
              "      <td>mark</td>\n",
              "      <td>structure</td>\n",
              "      <td>where</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>edge</td>\n",
              "      <td>every</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>where</td>\n",
              "      <td>edge</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>VBZ</td>\n",
              "      <td>root</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>has</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DT</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>label</td>\n",
              "      <td>a</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>has</td>\n",
              "      <td>label</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pos labled_dependency dependency         token          id\n",
              "0   NNP              root       ROOT         Typed  8589934592\n",
              "1   NNP             nsubj  represent  Dependencies  8589934592\n",
              "2   VBD         parataxis      Typed     represent  8589934592\n",
              "3    DT             nsubj  structure             a  8589934592\n",
              "4    JJ              amod  structure   grammatical  8589934592\n",
              "5    NN              flat  structure          tree  8589934592\n",
              "6    NN             nsubj  represent     structure  8589934592\n",
              "7   WRB              mark  structure         where  8589934592\n",
              "8    DT             nsubj       edge         every  8589934592\n",
              "9    NN             nsubj      where          edge  8589934592\n",
              "10  VBZ              root       ROOT           has  8589934592\n",
              "11   DT             nsubj      label             a  8589934592\n",
              "12   NN             nsubj        has         label  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK3iVVxLFANO",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvq5WpEnFBia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "2c0f436a-8435-4759-f682-61af60a28d3b"
      },
      "source": [
        "nlu.load('tokenize').predict('Each word and Symbol in a sentence will generate token.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Each word and Symbol in a sentence will genera...</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence          id\n",
              "0  Each word and Symbol in a sentence will genera...  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6eOMTMFUsW",
        "colab_type": "text"
      },
      "source": [
        "# Remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnxbz-a3FSku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ed75daf8-8485-4146-c8ac-a8e69843c54d"
      },
      "source": [
        "nlu.load('stopwords').predict('I want you to remove stopewords from this sentence please')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stopwords_en download started this may take some time.\n",
            "Approximate size to download 2.9 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>cleanTokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I want you to remove stopewords from this sent...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[remove, stopewords, sentence]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                     cleanTokens\n",
              "0  I want you to remove stopewords from this sent...  ...  [remove, stopewords, sentence]\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4tndNMFp68",
        "colab_type": "text"
      },
      "source": [
        "# Normalize as sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2MI8lgvFO9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "5e5159c4-6700-4467-d8d1-c4d71006bf52"
      },
      "source": [
        "nlu.load('norm').predict('I want the stopwords removed from this sentence')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I want the stopwords removed from this sentence</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[I, want, the, stopwords, removed, from, this,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          sentence  ...                                         normalized\n",
              "0  I want the stopwords removed from this sentence  ...  [I, want, the, stopwords, removed, from, this,...\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A81OKGHXGXgZ",
        "colab_type": "text"
      },
      "source": [
        "# Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxNEncqGbEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "cacd8376-0472-4b89-d7a5-11aedd1b7913"
      },
      "source": [
        "nlu.load('stem').predict('Stemming is a surprisingly useful method for converting a conjugated words in to their stem')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stemming is a surprisingly useful method for c...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[stem, i, a, surprisingli, us, method, for, co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                               stem\n",
              "0  Stemming is a surprisingly useful method for c...  ...  [stem, i, a, surprisingli, us, method, for, co...\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMq33Df0IpWY",
        "colab_type": "text"
      },
      "source": [
        "# Spell checking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKxJ0tZ_Iva0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "f41be88f-d7b1-4d67-801f-5ad64fb93f5c"
      },
      "source": [
        "nlu.load('spell').predict('I liek peantut buter and jelli')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check_spelling_dl download started this may take some time.\n",
            "Approx size to download 112.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checked</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>like</td>\n",
              "      <td>liek</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>peanut</td>\n",
              "      <td>peantut</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>butter</td>\n",
              "      <td>buter</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Helli</td>\n",
              "      <td>jelli</td>\n",
              "      <td>8589934592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  checked    token          id\n",
              "0       I        I  8589934592\n",
              "1    like     liek  8589934592\n",
              "2  peanut  peantut  8589934592\n",
              "3  butter    buter  8589934592\n",
              "4     and      and  8589934592\n",
              "5   Helli    jelli  8589934592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WNVlCAVIpD2",
        "colab_type": "text"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vrfEGjILdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6344f7e8-dff2-4f01-d9e3-d7234b6dbb00"
      },
      "source": [
        "nlu.load('lemma').predict('Lemmatizing generates a less noisy version of the inputted tokens')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lemmatizing generates a less noisy version of ...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[Lemmatizing, generate, a, less, noisy, versio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                              lemma\n",
              "0  Lemmatizing generates a less noisy version of ...  ...  [Lemmatizing, generate, a, less, noisy, versio...\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ72zsvuI8Jg",
        "colab_type": "text"
      },
      "source": [
        "# Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8DyiMFPILWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "64ae5ea3-117e-46db-af89-3bc6e2f81f38"
      },
      "source": [
        "nlu.load('norm').predict('@CKL_IT says that #normalizers are pretty useful to clean #structured_strings in #NLU like tweets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@CKL_IT says that #normalizers are pretty usef...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[CKLIT, says, that, normalizers, are, pretty, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                         normalized\n",
              "0  @CKL_IT says that #normalizers are pretty usef...  ...  [CKLIT, says, that, normalizers, are, pretty, ...\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE375gf4JlMS",
        "colab_type": "text"
      },
      "source": [
        "# Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqFz6sgdJhej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a7cd18e3-de36-42db-f7e3-931df6689fc0"
      },
      "source": [
        "nlu.load('ngram').predict('To be or not to be ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>ngrams</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To be or not to be</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[To be, be or, or not, not to, to be]</td>\n",
              "      <td>[TO, VB, CC, RB, TO, VB]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             document  ...                       pos\n",
              "0  To be or not to be  ...  [TO, VB, CC, RB, TO, VB]\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvcmsI-TKbPC",
        "colab_type": "text"
      },
      "source": [
        "# Date Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBukfPeHKawP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "4b823603-cd31-4fe3-be83-16e124b498c0"
      },
      "source": [
        "nlu.load('match.datetime').predict(' In the years 2000/01/01 to 2010/01/01 a lot of things happend') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_datetime download started this may take some time.\n",
            "Approx size to download 12.9 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the years 2000/01/01 to 2010/01/01 a lot o...</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[2000/01/01, 2001/01/01]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  ...                      date\n",
              "0   In the years 2000/01/01 to 2010/01/01 a lot o...  ...  [2000/01/01, 2001/01/01]\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4d3N3VsKpqA",
        "colab_type": "text"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Ww77_vKOiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "6c287029-3dd2-47fa-900d-3b57901b1044"
      },
      "source": [
        "# Checkout https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html for all possible POS labels\n",
        "# First we load the pipeline\n",
        "pipe = nlu.load('match.chunks')\n",
        "# Now we print the info to see at which index which com,ponent is and what parameters we can configure on them \n",
        "pipe.print_info()\n",
        "# Lets set our Chunker to only match NN\n",
        "pipe.pipe_components[4].model.setRegexParsers(['<NN>'])\n",
        "# Now we can predict with the configured pipeline\n",
        "pipe.predict(\"Jim and Joe went to the market next to the town hall in America\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_chunks download started this may take some time.\n",
            "Approx size to download 4.3 MB\n",
            "[OK!]\n",
            "-------------------------------------At pipe.pipe_components[0].model  : document_assembler with configurable parameters: --------------------------------------\n",
            "Param Name [ cleanupMode ] :  Param Info : possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full  currenlty Configured as :  disabled\n",
            "--------------------------------------At pipe.pipe_components[1].model  : sentence_detector with configurable parameters: --------------------------------------\n",
            "Param Name [ customBounds ] :  Param Info : characters used to explicitly mark sentence bounds  currenlty Configured as :  []\n",
            "Param Name [ explodeSentences ] :  Param Info : whether to explode each sentence into a different row, for better parallelization. Defaults to false.  currenlty Configured as :  False\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ maxLength ] :  Param Info : Set the maximum allowed length for each sentence  currenlty Configured as :  99999\n",
            "Param Name [ minLength ] :  Param Info : Set the minimum allowed length for each sentence.  currenlty Configured as :  0\n",
            "Param Name [ useAbbreviations ] :  Param Info : whether to apply abbreviations at sentence detection  currenlty Configured as :  True\n",
            "Param Name [ useCustomBoundsOnly ] :  Param Info : Only utilize custom bounds in sentence detection  currenlty Configured as :  False\n",
            "----------------------------------------At pipe.pipe_components[2].model  : regex_matcher with configurable parameters: ----------------------------------------\n",
            "Param Name [ caseSensitiveExceptions ] :  Param Info : Whether to care for case sensitiveness in exceptions  currenlty Configured as :  True\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ targetPattern ] :  Param Info : pattern to grab from text as token candidates. Defaults \\S+  currenlty Configured as :  \\S+\n",
            "Param Name [ maxLength ] :  Param Info : Set the maximum allowed length for each token  currenlty Configured as :  99999\n",
            "Param Name [ minLength ] :  Param Info : Set the minimum allowed length for each token  currenlty Configured as :  0\n",
            "----------------------------------------At pipe.pipe_components[3].model  : sentiment_dl  with configurable parameters: ----------------------------------------\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "---------------------------------------At pipe.pipe_components[4].model  : default_chunker with configurable parameters: ---------------------------------------\n",
            "Param Name [ lazyAnnotator ] :  Param Info : Whether this AnnotatorModel acts as lazy in RecursivePipelines  currenlty Configured as :  False\n",
            "Param Name [ regexParsers ] :  Param Info : an array of grammar based chunk parsers  currenlty Configured as :  ['<NN>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>id</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>market</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>town</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hall</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    chunk          id                                                pos\n",
              "0  market  8589934592  [NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...\n",
              "1    town  8589934592  [NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...\n",
              "2    hall  8589934592  [NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kusWLGiiK7y4",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Detector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlILHSIoK0dL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "c3917e31-7ba1-4913-ae74-5a1f5946be67"
      },
      "source": [
        "nlu.load('sentence_detector').predict('NLU can detect things. Like beginning and endings of sentences. It can also do much more!', output_level ='sentence')   # todo buggy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_dl_sentence download started this may take some time.\n",
            "Approximate size to download 13.3 MB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>id</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>ner</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLU can detect things.</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Like beginning and endings of sentences.</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It can also do much more!</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[[0.4970400035381317, -0.013454999774694443, 0...</td>\n",
              "      <td>[O, O, O, O, O, B-sent, O, O, O, O, O, O, B-se...</td>\n",
              "      <td>[NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   sentence  ...                                                pos\n",
              "0                    NLU can detect things.  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "1  Like beginning and endings of sentences.  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "2                 It can also do much more!  ...  [NNP, MD, VB, NNS, ., IN, VBG, CC, NNS, IN, NN...\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJDMpFLf9Pq",
        "colab_type": "text"
      },
      "source": [
        "# Output Level demonstration\n",
        "\n",
        "## Token level output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2tHz8XwgRCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "d1db04d1-ec50-4d17-a44c-8f5f79c74514"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun!', 'I love the city New-York'], output_level='token', positions=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checked</th>\n",
              "      <th>checked_begin</th>\n",
              "      <th>checked_end</th>\n",
              "      <th>token</th>\n",
              "      <th>id</th>\n",
              "      <th>document_begin</th>\n",
              "      <th>document_end</th>\n",
              "      <th>sentence_begin</th>\n",
              "      <th>sentence_end</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_begin</th>\n",
              "      <th>sentiment_end</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>love</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>data</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>science</td>\n",
              "      <td>12</td>\n",
              "      <td>18</td>\n",
              "      <td>science</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>It</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>is</td>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>so</td>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>much</td>\n",
              "      <td>30</td>\n",
              "      <td>33</td>\n",
              "      <td>much</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>fun</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>fun</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>!</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>[19, 38]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>love</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>love</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>the</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>the</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>city</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>city</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>New-York</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>New-York</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     checked  checked_begin  ...  sentiment_end sentiment\n",
              "0          I              0  ...       [19, 38]  positive\n",
              "1       love              2  ...       [19, 38]  positive\n",
              "2       data              7  ...       [19, 38]  positive\n",
              "3    science             12  ...       [19, 38]  positive\n",
              "4          !             19  ...       [19, 38]  positive\n",
              "5         It             21  ...       [19, 38]  positive\n",
              "6         is             24  ...       [19, 38]  positive\n",
              "7         so             27  ...       [19, 38]  positive\n",
              "8       much             30  ...       [19, 38]  positive\n",
              "9        fun             35  ...       [19, 38]  positive\n",
              "10         !             38  ...       [19, 38]  positive\n",
              "11         I              0  ...           [23]  positive\n",
              "12      love              2  ...           [23]  positive\n",
              "13       the              7  ...           [23]  positive\n",
              "14      city             11  ...           [23]  positive\n",
              "15  New-York             16  ...           [23]  positive\n",
              "\n",
              "[16 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ASEw1GmevS5",
        "colab_type": "text"
      },
      "source": [
        "## Sentence level output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJdMRydRgYYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6d03706e-d8ba-4b3f-8348-39e9ac71aedf"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun!', 'I love the city New-York'], output_level='sentence', positions=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_begin</th>\n",
              "      <th>sentiment_end</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>document_begin</th>\n",
              "      <th>document_end</th>\n",
              "      <th>token_begin</th>\n",
              "      <th>token_end</th>\n",
              "      <th>checked_begin</th>\n",
              "      <th>checked_end</th>\n",
              "      <th>checked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.754000</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>I love data science!</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38]</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38]</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38]</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.612100</td>\n",
              "      <td>21</td>\n",
              "      <td>38</td>\n",
              "      <td>It is so much fun!</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[38]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38]</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38]</td>\n",
              "      <td>[0, 2, 7, 12, 19, 21, 24, 27, 30, 35, 38]</td>\n",
              "      <td>[0, 5, 10, 18, 19, 22, 25, 28, 33, 37, 38]</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.734200</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>I love the city New-York</td>\n",
              "      <td>positive</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>[0, 2, 7, 11, 16]</td>\n",
              "      <td>[0, 5, 9, 14, 23]</td>\n",
              "      <td>[0, 2, 7, 11, 16]</td>\n",
              "      <td>[0, 5, 9, 14, 23]</td>\n",
              "      <td>[I, love, the, city, New-York]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment_confidence  ...                                            checked\n",
              "0             0.754000  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "1             0.612100  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "2             0.734200  ...                     [I, love, the, city, New-York]\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsg-tkBHexvV",
        "colab_type": "text"
      },
      "source": [
        "## Document level output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbFG5qD1gZSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "901de95a-ee39-4c49-e7d9-ad4028e4fa1a"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun!', 'I love the city New-York'], output_level='document',)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>checked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love data science! It is so much fun!</td>\n",
              "      <td>0</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>positive</td>\n",
              "      <td>[I, love, data, science, !, It, is, so, much, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love the city New-York</td>\n",
              "      <td>8589934592</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>positive</td>\n",
              "      <td>[I, love, the, city, New-York]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  document  ...                                            checked\n",
              "0  I love data science! It is so much fun!  ...  [I, love, data, science, !, It, is, so, much, ...\n",
              "1                 I love the city New-York  ...                     [I, love, the, city, New-York]\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzKLswYJezjp",
        "colab_type": "text"
      },
      "source": [
        "## Chunk level output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPbzhIdYcISU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "54b78b98-8d47-440d-d152-e1fc8f723edc"
      },
      "source": [
        "nlu.load('sentiment').predict(['I love data science! It is so much fun!', 'I love the city New-York'], output_level='chunk',)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>id</th>\n",
              "      <th>checked</th>\n",
              "      <th>pos</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [chunk, id, checked, pos, sentiment]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}