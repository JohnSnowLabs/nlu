{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU - Chunking  Demo .ipynb","provenance":[{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[],"authorship_tag":"ABX9TyOPSdOC70aQ06Ts/BiTt4o+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FgtBtiBmV1fD","colab_type":"text"},"source":["# Grammatical Chunk Matching with NLU\n","With the chunker you can filter a data set based on Part of Speech Tags with Regex patterns.    \n"," \n","I.e. You could get all nouns or adjectives in your datset with the following parameterization.\n","```\n","pipe['default_chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","```\n","\n","See [here](https://www.rexegg.com/regex-quickstart.html)  for a great reference of Regex operators\n","\n","## Overview of all Part of Speech Tags : \n","\n","|Tag |Description |\n","|------|------------\n","|CC| \tCoordinating conjunction |\n","|CD| \tCardinal number |\n","|DT| \tDeterminer |\n","|EX| \tExistential there |\n","|FW| \tForeign word |\n","|IN| \tPreposition or subordinating conjunction |\n","|JJ| \tAdjective |\n","|JJR| \tAdjective, comparative |\n","|JJS| \tAdjective, superlative |\n","|LS| \tList item marker |\n","|MD| \tModal |\n","|NN| \tNoun, singular or mass |\n","|NNS| \tNoun, plural |\n","|NNP| \tProper noun, singular |\n","|NNPS| \tProper noun, plural |\n","|PDT| \tPredeterminer |\n","|POS| \tPossessive ending |\n","|PRP| \tPersonal pronoun |\n","|PRP$| \tPossessive pronoun |\n","|RB| \tAdverb |\n","|RBR| \tAdverb, comparative |\n","|RBS| \tAdverb, superlative |\n","|RP| \tParticle |\n","|SYM| \tSymbol |\n","|TO| \tto |\n","|UH| \tInterjection |\n","|VB| \tVerb, base form |\n","|VBD| \tVerb, past tense |\n","|VBG| \tVerb, gerund or present participle |\n","|VBN| \tVerb, past participle |\n","|VBP| \tVerb, non-3rd person singular present |\n","|VBZ| \tVerb, 3rd person singular present |\n","|WDT| \tWh-determiner |\n","|WP| \tWh-pronoun |\n","|WP\\$| \tPossessive wh-pronoun |\n","|WRB| \tWh-adverb |\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Chunks are Named \n"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600190044626,"user_tz":-120,"elapsed":59759,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu  > /dev/null    \n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYQRU3pRO146","colab_type":"text"},"source":["# 2. Load the Chunker and print parameters"]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"ok","timestamp":1600190082391,"user_tz":-120,"elapsed":97438,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"ee8861f0-8ea5-406d-d438-829a30cbdc8f"},"source":["import nlu \n","\n","pipe = nlu.load('match.chunks')\n","# Now we print the info to see at which index which com,ponent is and what parameters we can configure on them \n","pipe.print_info()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["match_chunks download started this may take some time.\n","Approx size to download 4.3 MB\n","[OK!]\n","The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('disabled')         | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : disabled\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])                 | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)                | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)          | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)                 | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)                     | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)           | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)       | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['regex_matcher'] has settable params:\n","pipe['regex_matcher'].setCaseSensitiveExceptions(True)        | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_matcher'].setTargetPattern('\\S+')                 | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_matcher'].setMaxLength(99999)                     | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['regex_matcher'].setMinLength(0)                         | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['sentiment_dl'] has settable params:\n",">>> pipe['default_chunker'] has settable params:\n","pipe['default_chunker'].setRegexParsers(['<DT>?<JJ>*<NN>+'])  | Info: an array of grammar based chunk parsers | Currently set to : ['<DT>?<JJ>*<NN>+']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9RRmIv9ZbaX3","colab_type":"text"},"source":["# 3. Configure pipe to only match nounds and adjvectives and predict on data"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1600190089720,"user_tz":-120,"elapsed":104728,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"4928cd89-137e-43ed-eafb-bddcd14e22cb"},"source":["# Lets set our Chunker to only match NN\n","pipe['default_chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"Jim and Joe went to the big blue market next to the town hall\")"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>pos</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>market</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>town hall</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>big blue</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>next</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  chunk                                                pos\n","origin_index                                                              \n","0                market  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...\n","0             town hall  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...\n","0              big blue  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...\n","0                  next  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"do7ECtyfX-UB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600190089723,"user_tz":-120,"elapsed":104719,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":[""],"execution_count":3,"outputs":[]}]}