{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU - Stemmer.ipynb","provenance":[{"file_id":"1pgqoRJ6yGWbTLWdLnRvwG5DLSU3rxuMq","timestamp":1599401652794},{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[],"authorship_tag":"ABX9TyP3mUzsIDtSDwuP3lsAMmY/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rBXrqlGEYA8G","colab_type":"text"},"source":["# Stemming with NLU \n","\n","Stemming returns the base form, the so called stem / root or base word of every token in the input data.    \n","\n","I. e. 'He was hungry' becomes 'He wa hungri'\n","\n","\n","Stemming works by applying a heuristic process that strips and mutates suffixes on  words.\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600189743492,"user_tz":-120,"elapsed":56309,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}}},"source":["\n","import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu  > /dev/null    "],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_CL8HZ8Ydry","colab_type":"text"},"source":["## 2. Load Model and stemm sample string"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1600189776163,"user_tz":-120,"elapsed":88940,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"233590c4-acc0-49ad-b4db-db9408a43ff6"},"source":["import nlu\n","pipe = nlu.load('en.stem')\n","pipe.predict('He was suprised by the diversity of NLU')"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>stem</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>He was suprised by the diversity of NLU</td>\n","      <td>[he, wa, supris, by, the, divers, of, nlu]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             sentence                                        stem\n","origin_index                                                                                     \n","0             He was suprised by the diversity of NLU  [he, wa, supris, by, the, divers, of, nlu]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"IRSEzc-RCceu","colab_type":"text"},"source":["# 3. Get one row per stemmed token by setting outputlevel to token.    \n","This lets us compare what the original token was and what it was stemmed to to. "]},{"cell_type":"code","metadata":{"id":"9bujAZtOCfRW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"ok","timestamp":1600189776167,"user_tz":-120,"elapsed":88921,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"f243fd94-3fd0-417b-844e-6d20ca6f9444"},"source":["pipe.predict('He was suprised by the diversity of NLU', output_level='token')"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>stem</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>He</td>\n","      <td>he</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>was</td>\n","      <td>wa</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>suprised</td>\n","      <td>supris</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>by</td>\n","      <td>by</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>diversity</td>\n","      <td>divers</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>of</td>\n","      <td>of</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NLU</td>\n","      <td>nlu</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  token    stem\n","origin_index                   \n","0                    He      he\n","0                   was      wa\n","0              suprised  supris\n","0                    by      by\n","0                   the     the\n","0             diversity  divers\n","0                    of      of\n","0                   NLU     nlu"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"av7EiK4adb24","colab_type":"text"},"source":["# 4. Checkout the Stemm models NLU has to offer for other languages than English!"]},{"cell_type":"code","metadata":{"id":"hZ8xLHY7dgJ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600189776170,"user_tz":-120,"elapsed":88901,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"cf5da9a5-f09a-43ad-fa02-0e2dd45efe43"},"source":["nlu.print_all_model_kinds_for_action('stem')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.stem') returns Spark NLP model stemmer\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TsRxB950elTp","colab_type":"text"},"source":["## 4.1 Let's try German stemming!"]},{"cell_type":"code","metadata":{"id":"5d_J7-20dvCw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"status":"ok","timestamp":1600189778148,"user_tz":-120,"elapsed":90858,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"6501096e-dce5-4a6d-fa55-d83f4eaf52eb"},"source":["nlu.load('de.stem').predict(\"Er war von der Vielfältigkeit des NLU Packets begeistert\",output_level='token')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>stem</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Er</td>\n","      <td>er</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>war</td>\n","      <td>war</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>von</td>\n","      <td>von</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>der</td>\n","      <td>der</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Vielfältigkeit</td>\n","      <td>vielfältigkeit</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>des</td>\n","      <td>de</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NLU</td>\n","      <td>nlu</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Packets</td>\n","      <td>packet</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>begeistert</td>\n","      <td>begeistert</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       token            stem\n","origin_index                                \n","0                         Er              er\n","0                        war             war\n","0                        von             von\n","0                        der             der\n","0             Vielfältigkeit  vielfältigkeit\n","0                        des              de\n","0                        NLU             nlu\n","0                    Packets          packet\n","0                 begeistert      begeistert"]},"metadata":{"tags":[]},"execution_count":5}]}]}