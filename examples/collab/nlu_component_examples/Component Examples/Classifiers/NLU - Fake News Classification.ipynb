{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU - Fake News Classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTaDWcC/l7aqVLe4hcmsE1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5f5nvmE7zi34","colab_type":"code","colab":{}},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu > /dev/null    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZE4c3HMSkGGu","colab_type":"text"},"source":["# Fake News Classification with NLU\n","\n","Fake news is a problem of increasing size and occurence.        \n","Fortunately we can leverage the structure of natural language with the latest deep learning algorithms with NLU in just one line.\n","\n","\n","The fake news classifiers model uses universal sentence embeddings and is trained with the classifierdl algorithm provided by Spark NLP."]},{"cell_type":"code","metadata":{"id":"7GJX5d6mjk5j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1599971485772,"user_tz":-120,"elapsed":8558,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"0aa8dc86-f458-490c-a2a8-516ba6c2d94e"},"source":["import nlu\n","news_pipe  = nlu.load('en.classify.fakenews')\n","news_pipe.predict(['Unicorns have been sighted on Mars!', '5G and Bill Gates cause COVID', 'Trump to Visit California After Criticism Over Silence on Wildfires' ])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["classifierdl_use_fakenews download started this may take some time.\n","Approximate size to download 21.4 MB\n","[OK!]\n","tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_embeddings</th>\n","      <th>category_confidence</th>\n","      <th>sentence</th>\n","      <th>category</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[-0.01756167598068714, 0.015006818808615208, -...</td>\n","      <td>1.000000</td>\n","      <td>Unicorns have been sighted on Mars!</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[0.08615710586309433, 0.034103237092494965, -0...</td>\n","      <td>1.000000</td>\n","      <td>5G and Bill Gates cause COVID</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[0.03416172042489052, 0.006567075382918119, -0...</td>\n","      <td>0.514414</td>\n","      <td>Trump to Visit California After Criticism Over...</td>\n","      <td>REAL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence_embeddings  ... category\n","origin_index                                                     ...         \n","0             [-0.01756167598068714, 0.015006818808615208, -...  ...     FAKE\n","1             [0.08615710586309433, 0.034103237092494965, -0...  ...     FAKE\n","2             [0.03416172042489052, 0.006567075382918119, -0...  ...     REAL\n","\n","[3 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"AdlYjZJpkO_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1599969601121,"user_tz":-120,"elapsed":1148,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"eea511a1-649f-41e4-e8a0-b49849a6e513"},"source":["news_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['classifier_dl'] has settable params:\n","pipe['classifier_dl'].setLazyAnnotator(False)        | Info: Whether this AnnotatorModel acts as lazy in RecursivePipelines | Currently set to : False\n","pipe['classifier_dl'].setClasses(['FAKE', 'REAL'])   | Info: get the tags used to trained this NerDLModel | Currently set to : ['FAKE', 'REAL']\n","pipe['classifier_dl'].setStorageRef('tfhub_use')     | Info: unique reference name for identification | Currently set to : tfhub_use\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setLazyAnnotator(False)    | Info: Whether this AnnotatorModel acts as lazy in RecursivePipelines | Currently set to : False\n","pipe['default_tokenizer'].setTargetPattern('\\S+')    | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)            | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)        | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['use'] has settable params:\n","pipe['use'].setDimension(512)                        | Info: Number of embedding dimensions | Currently set to : 512\n","pipe['use'].setLazyAnnotator(False)                  | Info: Whether this AnnotatorModel acts as lazy in RecursivePipelines | Currently set to : False\n","pipe['use'].setStorageRef('tfhub_use')               | Info: unique reference name for identification | Currently set to : tfhub_use\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setLazyAnnotator(False)    | Info: Whether this AnnotatorModel acts as lazy in RecursivePipelines | Currently set to : False\n","pipe['sentence_detector'].setUseAbbreviations(True)  | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setDetectLists(True)       | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n","pipe['sentence_detector'].setCustomBounds([])        | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMinLength(0)            | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setMaxLength(99999)        | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yzmZCOypnpeX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}